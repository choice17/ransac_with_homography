{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geometric distance L2norm Regression for Homography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data init \n",
    "M = 4  # number of data\n",
    "N = 3  # features number\n",
    "data_x = np.random.rand(N,M)*1000\n",
    "data_x = data_x\n",
    "data_x[2,:] = 1\n",
    "t_m = np.random.rand(N,N)\n",
    "t_m[2,2] = 1\n",
    "data_y = np.dot(t_m,data_x.copy().astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[825.11177597,  76.97138931, 188.78344314,  96.54784501],\n",
       "       [939.91483525, 198.49137721, 390.13632387, 500.63299411],\n",
       "       [  1.        ,   1.        ,   1.        ,   1.        ]])"
      ]
     },
     "execution_count": 1213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1214,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_m = np.array([[ 7.92272362e-01,  8.22117652e-02,  4.34671442e+02],\n",
    "       [-1.33501977e-01,  9.58413827e-01,  6.55962640e+01],\n",
    "       [-2.09248580e-04,  1.60937718e-05,  1.00000000e+00]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.16565676e+03 5.11972073e+02 6.16313142e+02 5.52321553e+02]\n",
      " [8.56269585e+02 2.45557312e+02 4.14305348e+02 5.32520520e+02]\n",
      " [8.42473307e-01 9.87088321e-01 9.66776098e-01 9.87854574e-01]]\n"
     ]
    }
   ],
   "source": [
    "data_y = np.dot(t_m, data_x.copy().astype(float))\n",
    "print(data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1302,
   "metadata": {},
   "outputs": [],
   "source": [
    "DBG = 0\n",
    "DBG_LOSS = 0\n",
    "def DEBUG(*args):\n",
    "    if DBG: print(*args)\n",
    "\n",
    "def DEBUGLOSS(*args):\n",
    "    if DBG_LOSS: print(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1303,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variables(object):\n",
    "    \"\"\"\n",
    "    Variable that can used to train with mask\n",
    "    \"\"\"\n",
    "    __slots__ = ('train_mask','dim','trainable','val')\n",
    "    def __init__(self, val=None, dim=None, trainable=1):\n",
    "        self.dim = dim\n",
    "        if val is None:\n",
    "            self.val = np.random.rand(dim[0],dim[1])\n",
    "            #self.val = np.zeros((dim[0],dim[1]))\n",
    "        else:\n",
    "            assert val.shape == dim, \"input val not consistent to dim input\"\n",
    "            self.val = val\n",
    "        self.train_mask = np.ones_like(self.val)\n",
    "        self.trainable = trainable\n",
    "    def set_trainMask(self, mask):\n",
    "        assert mask.shape == self.dim\n",
    "        self.train_mask = mask\n",
    "    def update(self, dv):\n",
    "        if self.trainable:\n",
    "            self.val -= (dv * self.train_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1304,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression(object):\n",
    "    \"\"\"\n",
    "    Note. Training gradient may explode if lr is too large.\n",
    "    \"\"\"\n",
    "    __slots__ = ('var', 'iter', 'lr', 'err_th', 'j_hist',\n",
    "                'x_mean', 'y_mean', 'x_std', 'y_std',\n",
    "                'decay_rate', 'loss_weights','norm', 'update_steps', 'dim','data_m',\n",
    "                'solver', 'loss_func')\n",
    "    LOSSFUNCS = {'L2norm':0, 'L1norm':1}\n",
    "    SOLVERS = {'Grad':0,'Adam':1}\n",
    "\n",
    "    def __init__(self, var=None, dim=None, iter=500, decay_rate=0.9, norm=1, update_steps=100, data_m=0,\n",
    "                solver='Grad', loss_func='L2norm'):\n",
    "        self.var = var\n",
    "        self.iter = iter\n",
    "        self.lr = 0.001\n",
    "        self.err_th = 1e-4\n",
    "        self.j_hist = np.zeros((self.iter,1))\n",
    "        self.decay_rate=0.9\n",
    "        self.loss_weights=None\n",
    "        self.update_steps=update_steps\n",
    "        self.norm = norm\n",
    "        self.data_m = data_m\n",
    "        self.solver = solver\n",
    "        self.loss_func = loss_func\n",
    "\n",
    "    def process(self, x, y, method=0):\n",
    "        if method == 0:\n",
    "            return x.copy(), y.copy()\n",
    "        elif method == 1:\n",
    "            n, m = x.shape\n",
    "            self.x_std = np.std(x,axis=1, keepdims=1)\n",
    "            _x = x.copy() / self.x_std\n",
    "            _y = y.copy() / self.x_std\n",
    "            _x[-1,:] = x[-1,:]\n",
    "            _y[-1,:] = y[-1,:]\n",
    "            return _x, _y\n",
    "        elif method == 2:\n",
    "            self.x_mean = np.mean(x,axis=1, keepdims=1)\n",
    "            self.x_std = np.std(x,axis=1, keepdims=1)\n",
    "            _x = (x.copy() - self.x_mean)/self.x_std\n",
    "            _y = (y.copy() - self.x_mean)/self.x_std\n",
    "            return _x, _y\n",
    "        return x.copy(), y.copy()\n",
    "    \n",
    "    def pred(W,x):\n",
    "        return np.dot(W, x)\n",
    "    \n",
    "    def computeLoss(self, res):\n",
    "        n, m = self.dim\n",
    "        p, q = self.var.val.shape\n",
    "        if self.loss_func == 'L2norm':\n",
    "            loss = np.sum(res * res) / (2 * m * n)\n",
    "        if self.norm > 0:\n",
    "            loss += (np.sum(self.var.val ** 2) - 1) / (2 * p * q) * self.norm \n",
    "        return loss\n",
    "    \n",
    "    def lrPolicy(self, iteration, lr):\n",
    "        if (iteration+1) % self.update_steps == 0:\n",
    "            lr *= self.decay_rate\n",
    "        return lr\n",
    "\n",
    "    def lossDiff(self, pred, y):\n",
    "        res = (pred - y)\n",
    "        if self.loss_weights is not None:\n",
    "            res *= self.loss_weights\n",
    "        return res\n",
    "\n",
    "    def computeGradient(self, res, _x, *args):\n",
    "        n, m = self.dim\n",
    "        if self.loss_func == 'L2norm':\n",
    "            tV = np.dot(res, _x.T) / m\n",
    "        if self.norm > 0:\n",
    "            tV += self.var.val * self.norm \n",
    "        return tV\n",
    "    \n",
    "    def computeSolver(self, lr, tV):\n",
    "        if self.solver == 'Grad':\n",
    "            dV = lr * tV\n",
    "        return dV\n",
    "\n",
    "    def updateWeight(self, dV_masked):\n",
    "        if self.var.trainable:\n",
    "            self.var.val -= dV_masked\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        assert len(x.shape) == 2, 'shape of x is not support'\n",
    "        assert self.var.dim[1] == x.shape[0], 'shape of x %s is not consistent to var dim %s' % (self.var.dim, x.shape)\n",
    "        assert self.var.dim[0] == y.shape[0], 'shape of y %s is not consistent to var dim %s' % (self.var.dim, y.shape)\n",
    "        n, m = self.dim = x.shape\n",
    "        p, q = self.var.val.shape\n",
    "        print('processing data')\n",
    "        _x, _y = self.process(x, y, self.data_m)\n",
    "        print('output:', _x[:,0], _y[:,0])\n",
    "        dv_hist = []\n",
    "        lr= self.lr\n",
    "\n",
    "        for i in range(self.iter):\n",
    "            # update policy\n",
    "            lr = self.lrPolicy(i, lr)\n",
    "\n",
    "            pred = Regression.pred(self.var.val, _x)\n",
    "            DEBUG('pred',pred[:,0])\n",
    "\n",
    "            res = self.lossDiff(pred,y)\n",
    "            DEBUG('res',res[0,:], '\\ntv', np.dot(res, _x.T)[0,:])\n",
    "\n",
    "            loss = self.computeLoss(res)\n",
    "            self.j_hist[i,0] = loss\n",
    "\n",
    "            if (loss < self.err_th):\n",
    "                return self.var.val\n",
    "            \n",
    "            # Gradient\n",
    "            tV = self.computeGradient(res, _x, m)\n",
    "            dV = self.computeSolver(lr, tV)\n",
    "\n",
    "            dv_hist.append(dV[0,0])\n",
    "\n",
    "            # Update variable\n",
    "            self.updateWeight(dV * self.var.train_mask)\n",
    "            DEBUG('dv',dV[0,:],'val',self.var.val[0,:])\n",
    "            DEBUGLOSS('==============',i,'============', 'loss',loss, lr)\n",
    "        print(\"iter Done! Final loss:\", loss)\n",
    "        return dv_hist\n",
    "            #self.var.update(dV, self.ir, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1305,
   "metadata": {},
   "outputs": [],
   "source": [
    "var  = Variables(dim=(N,N))\n",
    "# var.val[:,-1] = 200\n",
    "var.val[-1,-1] = 1.0\n",
    "mask = np.ones_like(t_m)\n",
    "mask[-1,:] = 1\n",
    "mask[N-1,N-1] = 0\n",
    "var.set_trainMask(mask)\n",
    "reg = Regression(var=var,iter=1000, update_steps=400, data_m=0)\n",
    "reg.err_th = 1e-10\n",
    "#reg.lr = 0.000002\n",
    "reg.lr = np.ones((N,N)) * 0.0000035\n",
    "reg.lr[:,2] = 0.275\n",
    "reg.norm = 0\n",
    "reg.loss_weights = np.array([[1,1,1]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1306,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean = np.mean(data_x, axis=1, keepdims=1)\n",
    "x_mean[2,0] = 0\n",
    "_data_x = (data_x) / np.array([[100,100,100]]).T\n",
    "_data_y = (data_y) / np.array([[100,100,100]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data\n",
      "output: [825.11177597 939.91483525   1.        ] [1.16565676e+03 8.56269585e+02 8.42473307e-01]\n",
      "iter Done! Final loss: 6.235712530023916e-09\n"
     ]
    }
   ],
   "source": [
    "dv_hist = reg.fit(data_x, data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.92272362e-01,  8.22117652e-02,  4.34671442e+02],\n",
       "       [-1.33501977e-01,  9.58413827e-01,  6.55962640e+01],\n",
       "       [-2.09248580e-04,  1.60937718e-05,  1.00000000e+00]])"
      ]
     },
     "execution_count": 1308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.92270906e-01,  8.22135679e-02,  4.34670934e+02],\n",
       "       [-1.33501667e-01,  9.58413443e-01,  6.55963723e+01],\n",
       "       [-2.09248580e-04,  1.60937718e-05,  1.00000000e+00]])"
      ]
     },
     "execution_count": 1309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.var.val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.16565676e+03 8.56269585e+02 8.42473307e-01] [[1.16565674e+03]\n",
      " [8.56269588e+02]\n",
      " [8.42473307e-01]]\n",
      "[511.97207291 245.55731181   0.98708832] [[511.97181023]\n",
      " [245.55736778]\n",
      " [  0.98708832]]\n",
      "[616.31314226 414.30534833   0.9667761 ] [[616.31306226]\n",
      " [414.30536537]\n",
      " [  0.9667761 ]]\n",
      "[552.32155338 532.52051963   0.98785457] [[552.32180688]\n",
      " [532.52046562]\n",
      " [  0.98785457]]\n"
     ]
    }
   ],
   "source": [
    "for idx in range(4):\n",
    "    test_x = data_x[:,idx]\n",
    "    test_y = data_y[:,idx]\n",
    "    pred_y = np.dot(reg.var.val,data_x[:,idx:idx+1])\n",
    "    print (test_y, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.24692776e+03, 8.72551513e+02, 8.22174312e-01]),\n",
       " array([[836.42794144],\n",
       "        [270.5636486 ],\n",
       "        [  0.89838256]]))"
      ]
     },
     "execution_count": 1176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y, pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1da2de576d8>]"
      ]
     },
     "execution_count": 1311,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.loglog(reg.j_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1da2dd33438>]"
      ]
     },
     "execution_count": 1314,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.array(dv_hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, params):\n",
    "    n_samples = len(y)\n",
    "    h = X @ params\n",
    "    return (1/(2*n_samples))*np.sum((h-y)**2)\n",
    "def gradient_descent(X, y, params, learning_rate, n_iters):\n",
    "    n_samples = len(y)\n",
    "    J_history = np.zeros((n_iters,1))\n",
    "\n",
    "    for i in range(n_iters):\n",
    "        params = params - (learning_rate/n_samples) * X.T @ (X @ params - y) \n",
    "        J_history[i] = compute_cost(X, y, params)\n",
    "\n",
    "    return (J_history, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples in our dataset is: 506\n"
     ]
    }
   ],
   "source": [
    "dataset = load_boston()\n",
    "\n",
    "X = dataset.data\n",
    "y = dataset.target[:,np.newaxis]\n",
    "\n",
    "print(\"Total samples in our dataset is: {}\".format(X.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = len(y)\n",
    "\n",
    "mu = np.mean(X, 0)\n",
    "sigma = np.std(X, 0)\n",
    "\n",
    "X = (X-mu) / sigma\n",
    "\n",
    "X = np.hstack((np.ones((n_samples,1)),X))\n",
    "n_features = np.size(X,1)\n",
    "params = np.zeros((n_features,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial cost is:  296.0734584980237 \n",
      "\n",
      "Optimal parameters are: \n",
      " [[22.53279993]\n",
      " [-0.83980839]\n",
      " [ 0.92612237]\n",
      " [-0.17541988]\n",
      " [ 0.72676226]\n",
      " [-1.82369448]\n",
      " [ 2.78447498]\n",
      " [-0.05650494]\n",
      " [-2.96695543]\n",
      " [ 1.80785186]\n",
      " [-1.1802415 ]\n",
      " [-1.99990382]\n",
      " [ 0.85595908]\n",
      " [-3.69524414]] \n",
      "\n",
      "Final cost is:  [11.00713381]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_iters = 1500\n",
    "learning_rate = 0.01\n",
    "\n",
    "initial_cost = compute_cost(X, y, params)\n",
    "\n",
    "print(\"Initial cost is: \", initial_cost, \"\\n\")\n",
    "\n",
    "(J_history, optimal_params) = gradient_descent(X, y, params, learning_rate, n_iters)\n",
    "\n",
    "print(\"Optimal parameters are: \\n\", optimal_params, \"\\n\")\n",
    "\n",
    "print(\"Final cost is: \", J_history[-1])\n",
    "\n",
    "plt.plot(range(len(J_history)), J_history, 'r')\n",
    "\n",
    "plt.title(\"Convergence Graph of Cost Function\")\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression():\n",
    "    def __init__(self, X, y, alpha=0.03, n_iter=1500):\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.n_iter = n_iter\n",
    "        self.n_samples = len(y)\n",
    "        self.n_features = np.size(X, 1)\n",
    "        self.X = np.hstack((np.ones(\n",
    "            (self.n_samples, 1)), (X - np.mean(X, 0)) / np.std(X, 0)))\n",
    "        self.y = y[:, np.newaxis]\n",
    "        self.params = np.zeros((self.n_features + 1, 1))\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "    def fit(self):\n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "            self.params = self.params - (self.alpha/self.n_samples) * \\\n",
    "            self.X.T @ (self.X @ self.params - self.y)\n",
    "\n",
    "        self.intercept_ = self.params[0]\n",
    "        self.coef_ = self.params[1:]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def score(self, X=None, y=None):\n",
    "\n",
    "        if X is None:\n",
    "            X = self.X\n",
    "        else:\n",
    "            n_samples = np.size(X, 0)\n",
    "            X = np.hstack((np.ones(\n",
    "                (n_samples, 1)), (X - np.mean(X, 0)) / np.std(X, 0)))\n",
    "\n",
    "        if y is None:\n",
    "            y = self.y\n",
    "        else:\n",
    "            y = y[:, np.newaxis]\n",
    "\n",
    "        y_pred = X @ self.params\n",
    "        score = 1 - (((y - y_pred)**2).sum() / ((y - y.mean())**2).sum())\n",
    "\n",
    "        return score\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_samples = np.size(X, 0)\n",
    "        y = np.hstack((np.ones((n_samples, 1)), (X-np.mean(X, 0)) \\\n",
    "                            / np.std(X, 0))) @ self.params\n",
    "        return y\n",
    "\n",
    "    def get_params(self):\n",
    "\n",
    "        return self.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 2.71\n",
    "b = -2\n",
    "N = 100\n",
    "x = np.linspace(0, 10, N)\n",
    "x = np.expand_dims(x,1)\n",
    "y = a *x + b + np.random.rand(N,1) / 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.LinearRegression at 0x191579044e0>"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression(x,y)\n",
    "lr.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variables(object):\n",
    "    \"\"\"\n",
    "    Variable that can used to train with mask\n",
    "    \"\"\"\n",
    "    __slots__ = ('train_mask','dim','trainable','val')\n",
    "    def __init__(self, val=None, dim=None, trainable=1):\n",
    "        self.dim = dim\n",
    "        if val is None:\n",
    "            self.val = np.random.rand(dim[0],dim[1])\n",
    "            #self.val = np.zeros((dim[0],dim[1]))\n",
    "        else:\n",
    "            assert val.shape == dim, \"input val not consistent to dim input\"\n",
    "            self.val = val\n",
    "        self.train_mask = np.ones_like(self.val)\n",
    "        self.trainable = trainable\n",
    "    def set_trainMask(self, mask):\n",
    "        assert mask.shape == self.dim\n",
    "        self.train_mask = mask\n",
    "    def update(self, dv):\n",
    "        if self.trainable:\n",
    "            self.val -= (dv * self.train_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression(object):\n",
    "    \"\"\"\n",
    "    Note. Training gradient may explode if lr is too large. reprojection loss\n",
    "    \"\"\"\n",
    "    __slots__ = ('var', 'iter', 'lr', 'err_th', 'j_hist',\n",
    "                'x_mean', 'y_mean', 'x_std', 'y_std',\n",
    "                'decay_rate', 'loss_weights','norm')\n",
    "    def __init__(self, var=None, iter=500, decay_rate=0.9, norm=1):\n",
    "        self.var = var\n",
    "        self.iter = iter\n",
    "        self.lr = 0.001\n",
    "        self.err_th = 1e-4\n",
    "        self.j_hist = np.zeros((self.iter,1))\n",
    "        self.decay_rate=0.9\n",
    "        self.loss_weights=None\n",
    "        self.norm = norm\n",
    "\n",
    "    def process(self, x, y):\n",
    "        #self.x_mean = np.mean(x)\n",
    "        self.x_std = np.std(x)\n",
    "        #self.y_mean = np.mean(y)\n",
    "        #self.y_std = np.std(y)\n",
    "        #_x = (x.copy() - self.x_mean)/self.x_std\n",
    "        #_y = (y.copy() - self.y_mean)/self.y_std\n",
    "        _x = x.copy()/self.x_std\n",
    "        _y = y.copy()/self.x_std\n",
    "        return _x, _y\n",
    "        #return x.copy(), y.copy()\n",
    "    \n",
    "    def pred(x,val):\n",
    "        return np.dot(x, val)\n",
    "    \n",
    "    def compute_cost(x, val, y):\n",
    "        m, n = y.shape\n",
    "        pred = Regression.pred(x, val)\n",
    "        dloss = pred - y\n",
    "        cost = np.sum(dloss * dloss) / (2 * m * n)\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        assert len(x.shape) == 2, 'shape of x is not support'\n",
    "        assert self.var.dim[0] == x.shape[1], 'shape of x %s is not consistent to var dim %s' % (self.var.dim, x.shape)\n",
    "        assert self.var.dim[1] == y.shape[1], 'shape of y %s is not consistent to var dim %s' % (self.var.dim, y.shape)\n",
    "        m, n = x.shape\n",
    "        print('processing data')\n",
    "        _x, _y = self.process(x, y)\n",
    "        dv_hist = []\n",
    "        lr= self.lr\n",
    "        method = 'L2norm'\n",
    "        for i in range(self.iter):\n",
    "            if i % 200 == 0:\n",
    "                lr *= self.decay_rate\n",
    "            pred = np.dot(_x, self.var.val)\n",
    "            # Loss L2norm\n",
    "            if self.loss_weights is not None:\n",
    "                res = (pred - _y) * self.loss_weights\n",
    "            else:\n",
    "                res = (pred - _y)\n",
    "            if method == 'L2norm':\n",
    "                invH = np.linalg.inv(self.var.val)\n",
    "                reproj = np.dot(pred, invH)\n",
    "                res_re = (reproj - _x) * self.loss_weights\n",
    "                loss = np.sum(res * res) / (2 * m * n) + np.sum(res_re * res_re) / (2 * m * n)\n",
    "            elif method == 'LMS':\n",
    "                loss = np.sum(np.median(res * res, axis=0)) / (2 * n)\n",
    "            if self.norm > 0:\n",
    "                loss += (np.sum(self.var.val ** 2) - 1) / 2 * self.norm\n",
    "            self.j_hist[i,0] = loss\n",
    "            if (loss < self.err_th):\n",
    "                return self.var.val\n",
    "            # Gradient\n",
    "            if method == 'L2norm':\n",
    "                # https://math.stackexchange.com/questions/190424/how-to-evaluate-the-derivatives-of-matrix-inverse\n",
    "                tV = np.dot(_x.T, res) / m\n",
    "                dH = -np.dot(np.dot(invH, self.var.val.T), invH)\n",
    "                dR = np.dot(pred, dH)\n",
    "                tV += (np.dot(dR.T, res_re) / m)\n",
    "                if self.norm > 0:\n",
    "                    tV += self.var.val * self.norm \n",
    "                dV = lr * tV\n",
    "                dv_hist.append(dV[0,0])\n",
    "            elif method == 'LMS':\n",
    "                dV = (lr) # working on\n",
    "            DEBUG('==============',i,'============', 'loss',loss, lr)\n",
    "            # Update variable\n",
    "            if self.var.trainable:\n",
    "                self.var.val -= (dV * self.var.train_mask)\n",
    "        print(\"iter Done! Final loss:\", loss)\n",
    "        return dv_hist\n",
    "            #self.var.update(dV, self.ir, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = np.random.rand(N,N)\n",
    "var  = Variables(val=rand,dim=(N,N))\n",
    "#var  = Variables(dim=(N,N))\n",
    "var.val[N-1,N-1] = 1.0\n",
    "mask = np.ones_like(t_m)\n",
    "mask[-1,:] = 1\n",
    "mask[N-1,N-1] = 0\n",
    "var.set_trainMask(mask)\n",
    "reg = Regression(var=var,iter=1000)\n",
    "reg.err_th = 1e-10\n",
    "#reg.lr = 0.000001\n",
    "#reg.lr = np.ones((N,N)) * 0.01\n",
    "#reg.lr[-1,:] = 0.0001\n",
    "reg.lr = 0.1\n",
    "reg.norm = 1e-8\n",
    "reg.loss_weights = np.array([[1,1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data\n",
      "============== 0 ============ loss 116014.94453889747 0.09000000000000001\n",
      "============== 1 ============ loss 22144.163619434694 0.09000000000000001\n",
      "============== 2 ============ loss 6089.0714430236585 0.09000000000000001\n",
      "============== 3 ============ loss 3273.3673188490893 0.09000000000000001\n",
      "============== 4 ============ loss 2712.7321334867115 0.09000000000000001\n",
      "============== 5 ============ loss 2538.658798665922 0.09000000000000001\n",
      "============== 6 ============ loss 2433.211939650282 0.09000000000000001\n",
      "============== 7 ============ loss 2342.2069540665484 0.09000000000000001\n",
      "============== 8 ============ loss 2256.326148113136 0.09000000000000001\n",
      "============== 9 ============ loss 2173.887221394699 0.09000000000000001\n",
      "============== 10 ============ loss 2094.5101826155715 0.09000000000000001\n",
      "============== 11 ============ loss 2018.0399845150655 0.09000000000000001\n",
      "============== 12 ============ loss 1944.3631428361227 0.09000000000000001\n",
      "============== 13 ============ loss 1873.3764233692837 0.09000000000000001\n",
      "============== 14 ============ loss 1804.9813996276673 0.09000000000000001\n",
      "============== 15 ============ loss 1739.0834152587179 0.09000000000000001\n",
      "============== 16 ============ loss 1675.5912997681091 0.09000000000000001\n",
      "============== 17 ============ loss 1614.4172163136573 0.09000000000000001\n",
      "============== 18 ============ loss 1555.4765357550073 0.09000000000000001\n",
      "============== 19 ============ loss 1498.6877188222875 0.09000000000000001\n",
      "============== 20 ============ loss 1443.9722031854537 0.09000000000000001\n",
      "============== 21 ============ loss 1391.2542947484305 0.09000000000000001\n",
      "============== 22 ============ loss 1340.4610629293138 0.09000000000000001\n",
      "============== 23 ============ loss 1291.5222397667142 0.09000000000000001\n",
      "============== 24 ============ loss 1244.3701227101292 0.09000000000000001\n",
      "============== 25 ============ loss 1198.9394809594166 0.09000000000000001\n",
      "============== 26 ============ loss 1155.1674652237355 0.09000000000000001\n",
      "============== 27 ============ loss 1112.9935207750968 0.09000000000000001\n",
      "============== 28 ============ loss 1072.3593036762354 0.09000000000000001\n",
      "============== 29 ============ loss 1033.2086000669217 0.09000000000000001\n",
      "============== 30 ============ loss 995.487248397045 0.09000000000000001\n",
      "============== 31 ============ loss 959.1430644988874 0.09000000000000001\n",
      "============== 32 ============ loss 924.125769394926 0.09000000000000001\n",
      "============== 33 ============ loss 890.3869197413065 0.09000000000000001\n",
      "============== 34 ============ loss 857.8798408107499 0.09000000000000001\n",
      "============== 35 ============ loss 826.5595619221809 0.09000000000000001\n",
      "============== 36 ============ loss 796.3827542277628 0.09000000000000001\n",
      "============== 37 ============ loss 767.3076707712511 0.09000000000000001\n",
      "============== 38 ============ loss 739.2940887347701 0.09000000000000001\n",
      "============== 39 ============ loss 712.3032537940833 0.09000000000000001\n",
      "============== 40 ============ loss 686.2978265054107 0.09000000000000001\n",
      "============== 41 ============ loss 661.2418306495974 0.09000000000000001\n",
      "============== 42 ============ loss 637.1006034621923 0.09000000000000001\n",
      "============== 43 ============ loss 613.8407476805628 0.09000000000000001\n",
      "============== 44 ============ loss 591.4300853417348 0.09000000000000001\n",
      "============== 45 ============ loss 569.8376132670159 0.09000000000000001\n",
      "============== 46 ============ loss 549.0334601718328 0.09000000000000001\n",
      "============== 47 ============ loss 528.9888453414416 0.09000000000000001\n",
      "============== 48 ============ loss 509.6760388153498 0.09000000000000001\n",
      "============== 49 ============ loss 491.06832302535815 0.09000000000000001\n",
      "============== 50 ============ loss 473.1399558341659 0.09000000000000001\n",
      "============== 51 ============ loss 455.8661349233893 0.09000000000000001\n",
      "============== 52 ============ loss 439.22296348174433 0.09000000000000001\n",
      "============== 53 ============ loss 423.187417145916 0.09000000000000001\n",
      "============== 54 ============ loss 407.73731214837875 0.09000000000000001\n",
      "============== 55 ============ loss 392.85127462811346 0.09000000000000001\n",
      "============== 56 ============ loss 378.50871106175356 0.09000000000000001\n",
      "============== 57 ============ loss 364.6897797742647 0.09000000000000001\n",
      "============== 58 ============ loss 351.3753634897319 0.09000000000000001\n",
      "============== 59 ============ loss 338.5470428842992 0.09000000000000001\n",
      "============== 60 ============ loss 326.1870711046544 0.09000000000000001\n",
      "============== 61 ============ loss 314.27834921682563 0.09000000000000001\n",
      "============== 62 ============ loss 302.80440255131595 0.09000000000000001\n",
      "============== 63 ============ loss 291.7493579118429 0.09000000000000001\n",
      "============== 64 ============ loss 281.09792161617906 0.09000000000000001\n",
      "============== 65 ============ loss 270.835358338688 0.09000000000000001\n",
      "============== 66 ============ loss 260.9474707252987 0.09000000000000001\n",
      "============== 67 ============ loss 251.42057975272087 0.09000000000000001\n",
      "============== 68 ============ loss 242.24150580472082 0.09000000000000001\n",
      "============== 69 ============ loss 233.39755043928437 0.09000000000000001\n",
      "============== 70 ============ loss 224.87647882144307 0.09000000000000001\n",
      "============== 71 ============ loss 216.66650279745534 0.09000000000000001\n",
      "============== 72 ============ loss 208.75626458693972 0.09000000000000001\n",
      "============== 73 ============ loss 201.13482107038269 0.09000000000000001\n",
      "============== 74 ============ loss 193.79162865030077 0.09000000000000001\n",
      "============== 75 ============ loss 186.71652866510053 0.09000000000000001\n",
      "============== 76 ============ loss 179.89973333546664 0.09000000000000001\n",
      "============== 77 ============ loss 173.3318122238327 0.09000000000000001\n",
      "============== 78 ============ loss 167.00367918820064 0.09000000000000001\n",
      "============== 79 ============ loss 160.90657981226545 0.09000000000000001\n",
      "============== 80 ============ loss 155.03207929445145 0.09000000000000001\n",
      "============== 81 ============ loss 149.37205077910747 0.09000000000000001\n",
      "============== 82 ============ loss 143.91866411371862 0.09000000000000001\n",
      "============== 83 ============ loss 138.6643750165803 0.09000000000000001\n",
      "============== 84 ============ loss 133.60191463994994 0.09000000000000001\n",
      "============== 85 ============ loss 128.72427951423577 0.09000000000000001\n",
      "============== 86 ============ loss 124.02472185931485 0.09000000000000001\n",
      "============== 87 ============ loss 119.49674024957316 0.09000000000000001\n",
      "============== 88 ============ loss 115.13407061975626 0.09000000000000001\n",
      "============== 89 ============ loss 110.93067759918829 0.09000000000000001\n",
      "============== 90 ============ loss 106.88074616236626 0.09000000000000001\n",
      "============== 91 ============ loss 102.97867358438678 0.09000000000000001\n",
      "============== 92 ============ loss 99.21906169006853 0.09000000000000001\n",
      "============== 93 ============ loss 95.59670938605385 0.09000000000000001\n",
      "============== 94 ============ loss 92.10660546555252 0.09000000000000001\n",
      "============== 95 ============ loss 88.74392167577942 0.09000000000000001\n",
      "============== 96 ============ loss 85.50400603849478 0.09000000000000001\n",
      "============== 97 ============ loss 82.38237641439676 0.09000000000000001\n",
      "============== 98 ============ loss 79.37471430248189 0.09000000000000001\n",
      "============== 99 ============ loss 76.47685886577638 0.09000000000000001\n",
      "============== 100 ============ loss 73.68480117518523 0.09000000000000001\n",
      "============== 101 ============ loss 70.99467866349327 0.09000000000000001\n",
      "============== 102 ============ loss 68.40276978184176 0.09000000000000001\n",
      "============== 103 ============ loss 65.90548885129466 0.09000000000000001\n",
      "============== 104 ============ loss 63.49938110236697 0.09000000000000001\n",
      "============== 105 ============ loss 61.18111789565656 0.09000000000000001\n",
      "============== 106 ============ loss 58.94749211696272 0.09000000000000001\n",
      "============== 107 ============ loss 56.795413740528446 0.09000000000000001\n",
      "============== 108 ============ loss 54.72190555426027 0.09000000000000001\n",
      "============== 109 ============ loss 52.72409904101766 0.09000000000000001\n",
      "============== 110 ============ loss 50.79923041027409 0.09000000000000001\n",
      "============== 111 ============ loss 48.94463677465343 0.09000000000000001\n",
      "============== 112 ============ loss 47.15775246606087 0.09000000000000001\n",
      "============== 113 ============ loss 45.43610548630829 0.09000000000000001\n",
      "============== 114 ============ loss 43.77731408732027 0.09000000000000001\n",
      "============== 115 ============ loss 42.17908347619787 0.09000000000000001\n",
      "============== 116 ============ loss 40.639202640571035 0.09000000000000001\n",
      "============== 117 ============ loss 39.1555412898618 0.09000000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== 118 ============ loss 37.726046908211856 0.09000000000000001\n",
      "============== 119 ============ loss 36.34874191500853 0.09000000000000001\n",
      "============== 120 ============ loss 35.021720929076224 0.09000000000000001\n",
      "============== 121 ============ loss 33.74314813274945 0.09000000000000001\n",
      "============== 122 ============ loss 32.511254732179815 0.09000000000000001\n",
      "============== 123 ============ loss 31.32433651036591 0.09000000000000001\n",
      "============== 124 ============ loss 30.180751469519826 0.09000000000000001\n",
      "============== 125 ============ loss 29.078917559506188 0.09000000000000001\n",
      "============== 126 ============ loss 28.01731048921618 0.09000000000000001\n",
      "============== 127 ============ loss 26.99446161784433 0.09000000000000001\n",
      "============== 128 ============ loss 26.008955923153707 0.09000000000000001\n",
      "============== 129 ============ loss 25.05943004391651 0.09000000000000001\n",
      "============== 130 ============ loss 24.144570393824324 0.09000000000000001\n",
      "============== 131 ============ loss 23.26311134425659 0.09000000000000001\n",
      "============== 132 ============ loss 22.41383347339563 0.09000000000000001\n",
      "============== 133 ============ loss 21.595561879262103 0.09000000000000001\n",
      "============== 134 ============ loss 20.807164554343412 0.09000000000000001\n",
      "============== 135 ============ loss 20.047550819559067 0.09000000000000001\n",
      "============== 136 ============ loss 19.315669815403954 0.09000000000000001\n",
      "============== 137 ============ loss 18.61050904817505 0.09000000000000001\n",
      "============== 138 ============ loss 17.931092989276014 0.09000000000000001\n",
      "============== 139 ============ loss 17.276481725659778 0.09000000000000001\n",
      "============== 140 ============ loss 16.64576965954016 0.09000000000000001\n",
      "============== 141 ============ loss 16.038084255578305 0.09000000000000001\n",
      "============== 142 ============ loss 15.452584833805371 0.09000000000000001\n",
      "============== 143 ============ loss 14.888461406617003 0.09000000000000001\n",
      "============== 144 ============ loss 14.344933558226053 0.09000000000000001\n",
      "============== 145 ============ loss 13.821249365025917 0.09000000000000001\n",
      "============== 146 ============ loss 13.316684355370464 0.09000000000000001\n",
      "============== 147 ============ loss 12.83054050733046 0.09000000000000001\n",
      "============== 148 ============ loss 12.362145283043288 0.09000000000000001\n",
      "============== 149 ============ loss 11.910850698313991 0.09000000000000001\n",
      "============== 150 ============ loss 11.47603242618733 0.09000000000000001\n",
      "============== 151 ============ loss 11.057088933245534 0.09000000000000001\n",
      "============== 152 ============ loss 10.653440647440112 0.09000000000000001\n",
      "============== 153 ============ loss 10.264529156304752 0.09000000000000001\n",
      "============== 154 ============ loss 9.889816434440611 0.09000000000000001\n",
      "============== 155 ============ loss 9.528784099205565 0.09000000000000001\n",
      "============== 156 ============ loss 9.180932693576898 0.09000000000000001\n",
      "============== 157 ============ loss 8.845780995197268 0.09000000000000001\n",
      "============== 158 ============ loss 8.52286535064487 0.09000000000000001\n",
      "============== 159 ============ loss 8.211739034010032 0.09000000000000001\n",
      "============== 160 ============ loss 7.911971628889451 0.09000000000000001\n",
      "============== 161 ============ loss 7.623148432942056 0.09000000000000001\n",
      "============== 162 ============ loss 7.344869884185814 0.09000000000000001\n",
      "============== 163 ============ loss 7.076751008237832 0.09000000000000001\n",
      "============== 164 ============ loss 6.818420885736276 0.09000000000000001\n",
      "============== 165 ============ loss 6.569522139206429 0.09000000000000001\n",
      "============== 166 ============ loss 6.329710438659926 0.09000000000000001\n",
      "============== 167 ============ loss 6.098654025244436 0.09000000000000001\n",
      "============== 168 ============ loss 5.876033252284413 0.09000000000000001\n",
      "============== 169 ============ loss 5.661540143078112 0.09000000000000001\n",
      "============== 170 ============ loss 5.454877964838641 0.09000000000000001\n",
      "============== 171 ============ loss 5.255760818190266 0.09000000000000001\n",
      "============== 172 ============ loss 5.063913241651471 0.09000000000000001\n",
      "============== 173 ============ loss 4.879069830558498 0.09000000000000001\n",
      "============== 174 ============ loss 4.700974869900749 0.09000000000000001\n",
      "============== 175 ============ loss 4.529381980562338 0.09000000000000001\n",
      "============== 176 ============ loss 4.364053778477031 0.09000000000000001\n",
      "============== 177 ============ loss 4.2047615462292836 0.09000000000000001\n",
      "============== 178 ============ loss 4.051284916642901 0.09000000000000001\n",
      "============== 179 ============ loss 3.903411567922472 0.09000000000000001\n",
      "============== 180 ============ loss 3.7609369299256605 0.09000000000000001\n",
      "============== 181 ============ loss 3.623663901157061 0.09000000000000001\n",
      "============== 182 ============ loss 3.491402576096139 0.09000000000000001\n",
      "============== 183 ============ loss 3.363969982479753 0.09000000000000001\n",
      "============== 184 ============ loss 3.2411898281754286 0.09000000000000001\n",
      "============== 185 ============ loss 3.1228922572972557 0.09000000000000001\n",
      "============== 186 ============ loss 3.0089136152250417 0.09000000000000001\n",
      "============== 187 ============ loss 2.8990962222017926 0.09000000000000001\n",
      "============== 188 ============ loss 2.7932881551985886 0.09000000000000001\n",
      "============== 189 ============ loss 2.69134303774127 0.09000000000000001\n",
      "============== 190 ============ loss 2.59311983741242 0.09000000000000001\n",
      "============== 191 ============ loss 2.4984826707445023 0.09000000000000001\n",
      "============== 192 ============ loss 2.407300615237534 0.09000000000000001\n",
      "============== 193 ============ loss 2.319447528238905 0.09000000000000001\n",
      "============== 194 ============ loss 2.2348018724360816 0.09000000000000001\n",
      "============== 195 ============ loss 2.1532465477206677 0.09000000000000001\n",
      "============== 196 ============ loss 2.0746687291901478 0.09000000000000001\n",
      "============== 197 ============ loss 1.998959711064848 0.09000000000000001\n",
      "============== 198 ============ loss 1.9260147563027652 0.09000000000000001\n",
      "============== 199 ============ loss 1.8557329517051764 0.09000000000000001\n",
      "============== 200 ============ loss 1.7880170683118983 0.08100000000000002\n",
      "============== 201 ============ loss 1.7292431950559328 0.08100000000000002\n",
      "============== 202 ============ loss 1.672402317437407 0.08100000000000002\n",
      "============== 203 ============ loss 1.617430861827634 0.08100000000000002\n",
      "============== 204 ============ loss 1.5642673454480314 0.08100000000000002\n",
      "============== 205 ============ loss 1.512852307604811 0.08100000000000002\n",
      "============== 206 ============ loss 1.4631282431857369 0.08100000000000002\n",
      "============== 207 ============ loss 1.4150395383434926 0.08100000000000002\n",
      "============== 208 ============ loss 1.3685324082945753 0.08100000000000002\n",
      "============== 209 ============ loss 1.3235548371639219 0.08100000000000002\n",
      "============== 210 ============ loss 1.2800565198079303 0.08100000000000002\n",
      "============== 211 ============ loss 1.2379888055507937 0.08100000000000002\n",
      "============== 212 ============ loss 1.1973046437715078 0.08100000000000002\n",
      "============== 213 ============ loss 1.1579585312798528 0.08100000000000002\n",
      "============== 214 ============ loss 1.1199064614240983 0.08100000000000002\n",
      "============== 215 ============ loss 1.0831058748712725 0.08100000000000002\n",
      "============== 216 ============ loss 1.0475156120072984 0.08100000000000002\n",
      "============== 217 ============ loss 1.013095866901989 0.08100000000000002\n",
      "============== 218 ============ loss 0.9798081427880013 0.08100000000000002\n",
      "============== 219 ============ loss 0.9476152090047713 0.08100000000000002\n",
      "============== 220 ============ loss 0.9164810593575574 0.08100000000000002\n",
      "============== 221 ============ loss 0.8863708718465242 0.08100000000000002\n",
      "============== 222 ============ loss 0.857250969720411 0.08100000000000002\n",
      "============== 223 ============ loss 0.8290887838105048 0.08100000000000002\n",
      "============== 224 ============ loss 0.8018528161041013 0.08100000000000002\n",
      "============== 225 ============ loss 0.7755126045155386 0.08100000000000002\n",
      "============== 226 ============ loss 0.7500386888156998 0.08100000000000002\n",
      "============== 227 ============ loss 0.725402577682796 0.08100000000000002\n",
      "============== 228 ============ loss 0.7015767168360363 0.08100000000000002\n",
      "============== 229 ============ loss 0.6785344582176775 0.08100000000000002\n",
      "============== 230 ============ loss 0.656250030188485 0.08100000000000002\n",
      "============== 231 ============ loss 0.6346985087037554 0.08100000000000002\n",
      "============== 232 ============ loss 0.6138557894368265 0.08100000000000002\n",
      "============== 233 ============ loss 0.5936985608199541 0.08100000000000002\n",
      "============== 234 ============ loss 0.5742042779713031 0.08100000000000002\n",
      "============== 235 ============ loss 0.5553511374798216 0.08100000000000002\n",
      "============== 236 ============ loss 0.5371180530191962 0.08100000000000002\n",
      "============== 237 ============ loss 0.5194846317640327 0.08100000000000002\n",
      "============== 238 ============ loss 0.5024311515813994 0.08100000000000002\n",
      "============== 239 ============ loss 0.4859385389727951 0.08100000000000002\n",
      "============== 240 ============ loss 0.46998834774142023 0.08100000000000002\n",
      "============== 241 ============ loss 0.4545627383610631 0.08100000000000002\n",
      "============== 242 ============ loss 0.4396444580234718 0.08100000000000002\n",
      "============== 243 ============ loss 0.4252168213420772 0.08100000000000002\n",
      "============== 244 ============ loss 0.41126369169028415 0.08100000000000002\n",
      "============== 245 ============ loss 0.3977694631533721 0.08100000000000002\n",
      "============== 246 ============ loss 0.3847190430743051 0.08100000000000002\n",
      "============== 247 ============ loss 0.3720978351731698 0.08100000000000002\n",
      "============== 248 ============ loss 0.359891723222193 0.08100000000000002\n",
      "============== 249 ============ loss 0.3480870552572425 0.08100000000000002\n",
      "============== 250 ============ loss 0.33667062830922634 0.08100000000000002\n",
      "============== 251 ============ loss 0.32562967363681705 0.08100000000000002\n",
      "============== 252 ============ loss 0.31495184244551516 0.08100000000000002\n",
      "============== 253 ============ loss 0.3046251920761873 0.08100000000000002\n",
      "============== 254 ============ loss 0.29463817264776043 0.08100000000000002\n",
      "============== 255 ============ loss 0.28497961413923734 0.08100000000000002\n",
      "============== 256 ============ loss 0.27563871389677574 0.08100000000000002\n",
      "============== 257 ============ loss 0.26660502455131435 0.08100000000000002\n",
      "============== 258 ============ loss 0.25786844233377737 0.08100000000000002\n",
      "============== 259 ============ loss 0.249419195774689 0.08100000000000002\n",
      "============== 260 ============ loss 0.24124783477501105 0.08100000000000002\n",
      "============== 261 ============ loss 0.23334522003697927 0.08100000000000002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== 262 ============ loss 0.22570251284208398 0.08100000000000002\n",
      "============== 263 ============ loss 0.21831116516551327 0.08100000000000002\n",
      "============== 264 ============ loss 0.21116291011560792 0.08100000000000002\n",
      "============== 265 ============ loss 0.20424975268781856 0.08100000000000002\n",
      "============== 266 ============ loss 0.19756396082259015 0.08100000000000002\n",
      "============== 267 ============ loss 0.19109805675757918 0.08100000000000002\n",
      "============== 268 ============ loss 0.18484480866408198 0.08100000000000002\n",
      "============== 269 ============ loss 0.17879722255867905 0.08100000000000002\n",
      "============== 270 ============ loss 0.17294853448075748 0.08100000000000002\n",
      "============== 271 ============ loss 0.16729220292741193 0.08100000000000002\n",
      "============== 272 ============ loss 0.16182190153712836 0.08100000000000002\n",
      "============== 273 ============ loss 0.15653151201398585 0.08100000000000002\n",
      "============== 274 ============ loss 0.1514151172847922 0.08100000000000002\n",
      "============== 275 ============ loss 0.14646699488102122 0.08100000000000002\n",
      "============== 276 ============ loss 0.14168161053856554 0.08100000000000002\n",
      "============== 277 ============ loss 0.13705361200803223 0.08100000000000002\n",
      "============== 278 ============ loss 0.13257782306836918 0.08100000000000002\n",
      "============== 279 ============ loss 0.1282492377376388 0.08100000000000002\n",
      "============== 280 ============ loss 0.12406301467407499 0.08100000000000002\n",
      "============== 281 ============ loss 0.12001447176126205 0.08100000000000002\n",
      "============== 282 ============ loss 0.11609908087141498 0.08100000000000002\n",
      "============== 283 ============ loss 0.11231246280093381 0.08100000000000002\n",
      "============== 284 ============ loss 0.1086503823724518 0.08100000000000002\n",
      "============== 285 ============ loss 0.10510874369809627 0.08100000000000002\n",
      "============== 286 ============ loss 0.10168358559830726 0.08100000000000002\n",
      "============== 287 ============ loss 0.09837107717155247 0.08100000000000002\n",
      "============== 288 ============ loss 0.09516751350963104 0.08100000000000002\n",
      "============== 289 ============ loss 0.0920693115539828 0.08100000000000002\n",
      "============== 290 ============ loss 0.08907300608809021 0.08100000000000002\n",
      "============== 291 ============ loss 0.08617524586191144 0.08100000000000002\n",
      "============== 292 ============ loss 0.08337278984367438 0.08100000000000002\n",
      "============== 293 ============ loss 0.08066250359486667 0.08100000000000002\n",
      "============== 294 ============ loss 0.07804135576464412 0.08100000000000002\n",
      "============== 295 ============ loss 0.07550641469928845 0.08100000000000002\n",
      "============== 296 ============ loss 0.0730548451634103 0.08100000000000002\n",
      "============== 297 ============ loss 0.07068390516879455 0.08100000000000002\n",
      "============== 298 ============ loss 0.06839094290770062 0.08100000000000002\n",
      "============== 299 ============ loss 0.06617339378691076 0.08100000000000002\n",
      "============== 300 ============ loss 0.06402877755931655 0.08100000000000002\n",
      "============== 301 ============ loss 0.061954695549984756 0.08100000000000002\n",
      "============== 302 ============ loss 0.05994882797325431 0.08100000000000002\n",
      "============== 303 ============ loss 0.05800893133828568 0.08100000000000002\n",
      "============== 304 ============ loss 0.05613283593970282 0.08100000000000002\n",
      "============== 305 ============ loss 0.054318443430998294 0.08100000000000002\n",
      "============== 306 ============ loss 0.05256372447755798 0.08100000000000002\n",
      "============== 307 ============ loss 0.05086671648698785 0.08100000000000002\n",
      "============== 308 ============ loss 0.049225521414027015 0.08100000000000002\n",
      "============== 309 ============ loss 0.04763830363772385 0.08100000000000002\n",
      "============== 310 ============ loss 0.04610328790833111 0.08100000000000002\n",
      "============== 311 ============ loss 0.04461875736180273 0.08100000000000002\n",
      "============== 312 ============ loss 0.04318305159958563 0.08100000000000002\n",
      "============== 313 ============ loss 0.04179456483147723 0.08100000000000002\n",
      "============== 314 ============ loss 0.04045174407971483 0.08100000000000002\n",
      "============== 315 ============ loss 0.039153087441977596 0.08100000000000002\n",
      "============== 316 ============ loss 0.03789714241162483 0.08100000000000002\n",
      "============== 317 ============ loss 0.036682504253103834 0.08100000000000002\n",
      "============== 318 ============ loss 0.03550781443085845 0.08100000000000002\n",
      "============== 319 ============ loss 0.03437175908983685 0.08100000000000002\n",
      "============== 320 ============ loss 0.03327306758601787 0.08100000000000002\n",
      "============== 321 ============ loss 0.03221051106528094 0.08100000000000002\n",
      "============== 322 ============ loss 0.031182901088972013 0.08100000000000002\n",
      "============== 323 ============ loss 0.030189088304689526 0.08100000000000002\n",
      "============== 324 ============ loss 0.029227961160790066 0.08100000000000002\n",
      "============== 325 ============ loss 0.02829844466319366 0.08100000000000002\n",
      "============== 326 ============ loss 0.02739949917298645 0.08100000000000002\n",
      "============== 327 ============ loss 0.026530119243743464 0.08100000000000002\n",
      "============== 328 ============ loss 0.025689332496882537 0.08100000000000002\n",
      "============== 329 ============ loss 0.024876198534159025 0.08100000000000002\n",
      "============== 330 ============ loss 0.024089807885863658 0.08100000000000002\n",
      "============== 331 ============ loss 0.023329280993621408 0.08100000000000002\n",
      "============== 332 ============ loss 0.022593767226637165 0.08100000000000002\n",
      "============== 333 ============ loss 0.021882443930333496 0.08100000000000002\n",
      "============== 334 ============ loss 0.021194515506208336 0.08100000000000002\n",
      "============== 335 ============ loss 0.02052921252204072 0.08100000000000002\n",
      "============== 336 ============ loss 0.019885790851266436 0.08100000000000002\n",
      "============== 337 ============ loss 0.019263530840762932 0.08100000000000002\n",
      "============== 338 ============ loss 0.01866173650590285 0.08100000000000002\n",
      "============== 339 ============ loss 0.01807973475216438 0.08100000000000002\n",
      "============== 340 ============ loss 0.01751687462226595 0.08100000000000002\n",
      "============== 341 ============ loss 0.01697252656813764 0.08100000000000002\n",
      "============== 342 ============ loss 0.016446081746784857 0.08100000000000002\n",
      "============== 343 ============ loss 0.015936951339317464 0.08100000000000002\n",
      "============== 344 ============ loss 0.015444565892394525 0.08100000000000002\n",
      "============== 345 ============ loss 0.014968374681317152 0.08100000000000002\n",
      "============== 346 ============ loss 0.014507845094047822 0.08100000000000002\n",
      "============== 347 ============ loss 0.01406246203554939 0.08100000000000002\n",
      "============== 348 ============ loss 0.013631727351618191 0.08100000000000002\n",
      "============== 349 ============ loss 0.01321515927178174 0.08100000000000002\n",
      "============== 350 ============ loss 0.01281229187040185 0.08100000000000002\n",
      "============== 351 ============ loss 0.012422674545596412 0.08100000000000002\n",
      "============== 352 ============ loss 0.012045871515238742 0.08100000000000002\n",
      "============== 353 ============ loss 0.011681461329586557 0.08100000000000002\n",
      "============== 354 ============ loss 0.011329036399869163 0.08100000000000002\n",
      "============== 355 ============ loss 0.01098820254243503 0.08100000000000002\n",
      "============== 356 ============ loss 0.010658578537873905 0.08100000000000002\n",
      "============== 357 ============ loss 0.010339795704653605 0.08100000000000002\n",
      "============== 358 ============ loss 0.010031497486732165 0.08100000000000002\n",
      "============== 359 ============ loss 0.009733339054803447 0.08100000000000002\n",
      "============== 360 ============ loss 0.009444986920588434 0.08100000000000002\n",
      "============== 361 ============ loss 0.009166118563850523 0.08100000000000002\n",
      "============== 362 ============ loss 0.008896422071701502 0.08100000000000002\n",
      "============== 363 ============ loss 0.008635595789679934 0.08100000000000002\n",
      "============== 364 ============ loss 0.008383347984431598 0.08100000000000002\n",
      "============== 365 ============ loss 0.008139396517352277 0.08100000000000002\n",
      "============== 366 ============ loss 0.007903468529079562 0.08100000000000002\n",
      "============== 367 ============ loss 0.007675300134283359 0.08100000000000002\n",
      "============== 368 ============ loss 0.007454636126526589 0.08100000000000002\n",
      "============== 369 ============ loss 0.0072412296928292125 0.08100000000000002\n",
      "============== 370 ============ loss 0.0070348421376179235 0.08100000000000002\n",
      "============== 371 ============ loss 0.006835242615751525 0.08100000000000002\n",
      "============== 372 ============ loss 0.0066422078743396335 0.08100000000000002\n",
      "============== 373 ============ loss 0.006455522003019587 0.08100000000000002\n",
      "============== 374 ============ loss 0.0062749761925029925 0.08100000000000002\n",
      "============== 375 ============ loss 0.006100368501003616 0.08100000000000002\n",
      "============== 376 ============ loss 0.005931503628380111 0.08100000000000002\n",
      "============== 377 ============ loss 0.005768192697712624 0.08100000000000002\n",
      "============== 378 ============ loss 0.005610253044034346 0.08100000000000002\n",
      "============== 379 ============ loss 0.005457508010040191 0.08100000000000002\n",
      "============== 380 ============ loss 0.005309786748486277 0.08100000000000002\n",
      "============== 381 ============ loss 0.005166924031127212 0.08100000000000002\n",
      "============== 382 ============ loss 0.00502876006389863 0.08100000000000002\n",
      "============== 383 ============ loss 0.004895140308191536 0.08100000000000002\n",
      "============== 384 ============ loss 0.00476591530801624 0.08100000000000002\n",
      "============== 385 ============ loss 0.004640940522834693 0.08100000000000002\n",
      "============== 386 ============ loss 0.0045200761658997935 0.08100000000000002\n",
      "============== 387 ============ loss 0.004403187047909675 0.08100000000000002\n",
      "============== 388 ============ loss 0.004290142425795828 0.08100000000000002\n",
      "============== 389 ============ loss 0.004180815856507827 0.08100000000000002\n",
      "============== 390 ============ loss 0.004075085055573547 0.08100000000000002\n",
      "============== 391 ============ loss 0.003972831760337577 0.08100000000000002\n",
      "============== 392 ============ loss 0.003873941597681735 0.08100000000000002\n",
      "============== 393 ============ loss 0.003778303956121077 0.08100000000000002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== 394 ============ loss 0.003685811862059613 0.08100000000000002\n",
      "============== 395 ============ loss 0.0035963618601692797 0.08100000000000002\n",
      "============== 396 ============ loss 0.003509853897663259 0.08100000000000002\n",
      "============== 397 ============ loss 0.0034261912124012367 0.08100000000000002\n",
      "============== 398 ============ loss 0.0033452802246455946 0.08100000000000002\n",
      "============== 399 ============ loss 0.003267030432427864 0.08100000000000002\n",
      "============== 400 ============ loss 0.0031913543102859085 0.07290000000000002\n",
      "============== 401 ============ loss 0.0031254308476047193 0.07290000000000002\n",
      "============== 402 ============ loss 0.003061460446536621 0.07290000000000002\n",
      "============== 403 ============ loss 0.0029993852469360326 0.07290000000000002\n",
      "============== 404 ============ loss 0.002939149102754309 0.07290000000000002\n",
      "============== 405 ============ loss 0.0028806975312610427 0.07290000000000002\n",
      "============== 406 ============ loss 0.0028239776637859373 0.07290000000000002\n",
      "============== 407 ============ loss 0.0027689381978812885 0.07290000000000002\n",
      "============== 408 ============ loss 0.0027155293509343904 0.07290000000000002\n",
      "============== 409 ============ loss 0.0026637028151443426 0.07290000000000002\n",
      "============== 410 ============ loss 0.002613411713824715 0.07290000000000002\n",
      "============== 411 ============ loss 0.0025646105590098884 0.07290000000000002\n",
      "============== 412 ============ loss 0.0025172552103191856 0.07290000000000002\n",
      "============== 413 ============ loss 0.0024713028350218363 0.07290000000000002\n",
      "============== 414 ============ loss 0.0024267118693170834 0.07290000000000002\n",
      "============== 415 ============ loss 0.002383441980724339 0.07290000000000002\n",
      "============== 416 ============ loss 0.0023414540316177854 0.07290000000000002\n",
      "============== 417 ============ loss 0.002300710043822036 0.07290000000000002\n",
      "============== 418 ============ loss 0.002261173164270997 0.07290000000000002\n",
      "============== 419 ============ loss 0.002222807631671469 0.07290000000000002\n",
      "============== 420 ============ loss 0.0021855787441548867 0.07290000000000002\n",
      "============== 421 ============ loss 0.0021494528279024034 0.07290000000000002\n",
      "============== 422 ============ loss 0.0021143972066830457 0.07290000000000002\n",
      "============== 423 ============ loss 0.0020803801723045297 0.07290000000000002\n",
      "============== 424 ============ loss 0.002047370955927541 0.07290000000000002\n",
      "============== 425 ============ loss 0.0020153397002442832 0.07290000000000002\n",
      "============== 426 ============ loss 0.001984257432477027 0.07290000000000002\n",
      "============== 427 ============ loss 0.001954096038165129 0.07290000000000002\n",
      "============== 428 ============ loss 0.00192482823574035 0.07290000000000002\n",
      "============== 429 ============ loss 0.0018964275518620465 0.07290000000000002\n",
      "============== 430 ============ loss 0.001868868297460967 0.07290000000000002\n",
      "============== 431 ============ loss 0.0018421255445108115 0.07290000000000002\n",
      "============== 432 ============ loss 0.0018161751034808811 0.07290000000000002\n",
      "============== 433 ============ loss 0.0017909935014643146 0.07290000000000002\n",
      "============== 434 ============ loss 0.0017665579609364573 0.07290000000000002\n",
      "============== 435 ============ loss 0.0017428463791674606 0.07290000000000002\n",
      "============== 436 ============ loss 0.0017198373082213496 0.07290000000000002\n",
      "============== 437 ============ loss 0.0016975099355628666 0.07290000000000002\n",
      "============== 438 ============ loss 0.0016758440652353147 0.07290000000000002\n",
      "============== 439 ============ loss 0.0016548200995870322 0.07290000000000002\n",
      "============== 440 ============ loss 0.0016344190215547313 0.07290000000000002\n",
      "============== 441 ============ loss 0.0016146223774606968 0.07290000000000002\n",
      "============== 442 ============ loss 0.001595412260320968 0.07290000000000002\n",
      "============== 443 ============ loss 0.0015767712936523362 0.07290000000000002\n",
      "============== 444 ============ loss 0.0015586826157514373 0.07290000000000002\n",
      "============== 445 ============ loss 0.0015411298644538995 0.07290000000000002\n",
      "============== 446 ============ loss 0.0015240971623244285 0.07290000000000002\n",
      "============== 447 ============ loss 0.0015075691023046863 0.07290000000000002\n",
      "============== 448 ============ loss 0.0014915307337743412 0.07290000000000002\n",
      "============== 449 ============ loss 0.0014759675490303506 0.07290000000000002\n",
      "============== 450 ============ loss 0.0014608654701683322 0.07290000000000002\n",
      "============== 451 ============ loss 0.0014462108363430958 0.07290000000000002\n",
      "============== 452 ============ loss 0.0014319903914208245 0.07290000000000002\n",
      "============== 453 ============ loss 0.0014181912719836752 0.07290000000000002\n",
      "============== 454 ============ loss 0.0014048009956979118 0.07290000000000002\n",
      "============== 455 ============ loss 0.001391807450026462 0.07290000000000002\n",
      "============== 456 ============ loss 0.0013791988812692786 0.07290000000000002\n",
      "============== 457 ============ loss 0.0013669638839364172 0.07290000000000002\n",
      "============== 458 ============ loss 0.0013550913904309026 0.07290000000000002\n",
      "============== 459 ============ loss 0.0013435706610368566 0.07290000000000002\n",
      "============== 460 ============ loss 0.0013323912742112549 0.07290000000000002\n",
      "============== 461 ============ loss 0.0013215431171528405 0.07290000000000002\n",
      "============== 462 ============ loss 0.0013110163766561294 0.07290000000000002\n",
      "============== 463 ============ loss 0.0013008015302401273 0.07290000000000002\n",
      "============== 464 ============ loss 0.001290889337528872 0.07290000000000002\n",
      "============== 465 ============ loss 0.0012812708319031958 0.07290000000000002\n",
      "============== 466 ============ loss 0.0012719373123801592 0.07290000000000002\n",
      "============== 467 ============ loss 0.0012628803357531235 0.07290000000000002\n",
      "============== 468 ============ loss 0.0012540917089462521 0.07290000000000002\n",
      "============== 469 ============ loss 0.0012455634816128374 0.07290000000000002\n",
      "============== 470 ============ loss 0.001237287938939238 0.07290000000000002\n",
      "============== 471 ============ loss 0.0012292575946677143 0.07290000000000002\n",
      "============== 472 ============ loss 0.0012214651843267164 0.07290000000000002\n",
      "============== 473 ============ loss 0.0012139036586582615 0.07290000000000002\n",
      "============== 474 ============ loss 0.0012065661772485841 0.07290000000000002\n",
      "============== 475 ============ loss 0.0011994461023305842 0.07290000000000002\n",
      "============== 476 ============ loss 0.0011925369927893313 0.07290000000000002\n",
      "============== 477 ============ loss 0.001185832598332756 0.07290000000000002\n",
      "============== 478 ============ loss 0.001179326853837669 0.07290000000000002\n",
      "============== 479 ============ loss 0.0011730138738665597 0.07290000000000002\n",
      "============== 480 ============ loss 0.0011668879473426723 0.07290000000000002\n",
      "============== 481 ============ loss 0.001160943532383416 0.07290000000000002\n",
      "============== 482 ============ loss 0.0011551752512918793 0.07290000000000002\n",
      "============== 483 ============ loss 0.0011495778856884151 0.07290000000000002\n",
      "============== 484 ============ loss 0.0011441463717941204 0.07290000000000002\n",
      "============== 485 ============ loss 0.0011388757958480829 0.07290000000000002\n",
      "============== 486 ============ loss 0.001133761389665183 0.07290000000000002\n",
      "============== 487 ============ loss 0.001128798526321779 0.07290000000000002\n",
      "============== 488 ============ loss 0.0011239827159735287 0.07290000000000002\n",
      "============== 489 ============ loss 0.001119309601790182 0.07290000000000002\n",
      "============== 490 ============ loss 0.0011147749560187921 0.07290000000000002\n",
      "============== 491 ============ loss 0.0011103746761564456 0.07290000000000002\n",
      "============== 492 ============ loss 0.0011061047812441693 0.07290000000000002\n",
      "============== 493 ============ loss 0.0011019614082607683 0.07290000000000002\n",
      "============== 494 ============ loss 0.001097940808632711 0.07290000000000002\n",
      "============== 495 ============ loss 0.0010940393448438375 0.07290000000000002\n",
      "============== 496 ============ loss 0.001090253487140233 0.07290000000000002\n",
      "============== 497 ============ loss 0.0010865798103453328 0.07290000000000002\n",
      "============== 498 ============ loss 0.0010830149907558083 0.07290000000000002\n",
      "============== 499 ============ loss 0.0010795558031366002 0.07290000000000002\n",
      "============== 500 ============ loss 0.0010761991178060242 0.07290000000000002\n",
      "============== 501 ============ loss 0.0010729418978030548 0.07290000000000002\n",
      "============== 502 ============ loss 0.0010697811961400848 0.07290000000000002\n",
      "============== 503 ============ loss 0.001066714153139256 0.07290000000000002\n",
      "============== 504 ============ loss 0.0010637379938439856 0.07290000000000002\n",
      "============== 505 ============ loss 0.0010608500255105606 0.07290000000000002\n",
      "============== 506 ============ loss 0.0010580476351706113 0.07290000000000002\n",
      "============== 507 ============ loss 0.0010553282872706976 0.07290000000000002\n",
      "============== 508 ============ loss 0.00105268952137512 0.07290000000000002\n",
      "============== 509 ============ loss 0.0010501289499445137 0.07290000000000002\n",
      "============== 510 ============ loss 0.0010476442561727977 0.07290000000000002\n",
      "============== 511 ============ loss 0.0010452331918935666 0.07290000000000002\n",
      "============== 512 ============ loss 0.00104289357554575 0.07290000000000002\n",
      "============== 513 ============ loss 0.00104062329020172 0.07290000000000002\n",
      "============== 514 ============ loss 0.001038420281650568 0.07290000000000002\n",
      "============== 515 ============ loss 0.0010362825565416389 0.07290000000000002\n",
      "============== 516 ============ loss 0.0010342081805806951 0.07290000000000002\n",
      "============== 517 ============ loss 0.0010321952767801982 0.07290000000000002\n",
      "============== 518 ============ loss 0.0010302420237629196 0.07290000000000002\n",
      "============== 519 ============ loss 0.0010283466541125723 0.07290000000000002\n",
      "============== 520 ============ loss 0.0010265074527757625 0.07290000000000002\n",
      "============== 521 ============ loss 0.0010247227555123892 0.07290000000000002\n",
      "============== 522 ============ loss 0.0010229909473875142 0.07290000000000002\n",
      "============== 523 ============ loss 0.0010213104613126527 0.07290000000000002\n",
      "============== 524 ============ loss 0.0010196797766270605 0.07290000000000002\n",
      "============== 525 ============ loss 0.0010180974177233656 0.07290000000000002\n",
      "============== 526 ============ loss 0.0010165619527125979 0.07290000000000002\n",
      "============== 527 ============ loss 0.0010150719921280446 0.07290000000000002\n",
      "============== 528 ============ loss 0.0010136261876691958 0.07290000000000002\n",
      "============== 529 ============ loss 0.001012223230982304 0.07290000000000002\n",
      "============== 530 ============ loss 0.0010108618524763528 0.07290000000000002\n",
      "============== 531 ============ loss 0.001009540820175465 0.07290000000000002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== 532 ============ loss 0.0010082589386043977 0.07290000000000002\n",
      "============== 533 ============ loss 0.0010070150477054201 0.07290000000000002\n",
      "============== 534 ============ loss 0.0010058080217918597 0.07290000000000002\n",
      "============== 535 ============ loss 0.0010046367685281061 0.07290000000000002\n",
      "============== 536 ============ loss 0.0010035002279409756 0.07290000000000002\n",
      "============== 537 ============ loss 0.001002397371463358 0.07290000000000002\n",
      "============== 538 ============ loss 0.0010013272010006076 0.07290000000000002\n",
      "============== 539 ============ loss 0.0010002887480309516 0.07290000000000002\n",
      "============== 540 ============ loss 0.0009992810727265818 0.07290000000000002\n",
      "============== 541 ============ loss 0.0009983032631053898 0.07290000000000002\n",
      "============== 542 ============ loss 0.0009973544342055227 0.07290000000000002\n",
      "============== 543 ============ loss 0.0009964337272843676 0.07290000000000002\n",
      "============== 544 ============ loss 0.000995540309042378 0.07290000000000002\n",
      "============== 545 ============ loss 0.0009946733708689106 0.07290000000000002\n",
      "============== 546 ============ loss 0.0009938321281114965 0.07290000000000002\n",
      "============== 547 ============ loss 0.0009930158193644655 0.07290000000000002\n",
      "============== 548 ============ loss 0.0009922237057824284 0.07290000000000002\n",
      "============== 549 ============ loss 0.0009914550704099353 0.07290000000000002\n",
      "============== 550 ============ loss 0.0009907092175340227 0.07290000000000002\n",
      "============== 551 ============ loss 0.0009899854720548556 0.07290000000000002\n",
      "============== 552 ============ loss 0.000989283178874007 0.07290000000000002\n",
      "============== 553 ============ loss 0.0009886017023038565 0.07290000000000002\n",
      "============== 554 ============ loss 0.0009879404254898052 0.07290000000000002\n",
      "============== 555 ============ loss 0.0009872987498552182 0.07290000000000002\n",
      "============== 556 ============ loss 0.0009866760945582652 0.07290000000000002\n",
      "============== 557 ============ loss 0.0009860718959657977 0.07290000000000002\n",
      "============== 558 ============ loss 0.0009854856071455191 0.07290000000000002\n",
      "============== 559 ============ loss 0.0009849166973695377 0.07290000000000002\n",
      "============== 560 ============ loss 0.000984364651635453 0.07290000000000002\n",
      "============== 561 ============ loss 0.0009838289701994745 0.07290000000000002\n",
      "============== 562 ============ loss 0.0009833091681247573 0.07290000000000002\n",
      "============== 563 ============ loss 0.0009828047748427137 0.07290000000000002\n",
      "============== 564 ============ loss 0.0009823153337280236 0.07290000000000002\n",
      "============== 565 ============ loss 0.000981840401684058 0.07290000000000002\n",
      "============== 566 ============ loss 0.0009813795487433141 0.07290000000000002\n",
      "============== 567 ============ loss 0.0009809323576777768 0.07290000000000002\n",
      "============== 568 ============ loss 0.0009804984236218848 0.07290000000000002\n",
      "============== 569 ============ loss 0.0009800773537061715 0.07290000000000002\n",
      "============== 570 ============ loss 0.0009796687667014416 0.07290000000000002\n",
      "============== 571 ============ loss 0.0009792722926744335 0.07290000000000002\n",
      "============== 572 ============ loss 0.0009788875726534033 0.07290000000000002\n",
      "============== 573 ============ loss 0.0009785142583020475 0.07290000000000002\n",
      "============== 574 ============ loss 0.0009781520116057274 0.07290000000000002\n",
      "============== 575 ============ loss 0.0009778005045655601 0.07290000000000002\n",
      "============== 576 ============ loss 0.0009774594189009378 0.07290000000000002\n",
      "============== 577 ============ loss 0.0009771284457617342 0.07290000000000002\n",
      "============== 578 ============ loss 0.0009768072854500394 0.07290000000000002\n",
      "============== 579 ============ loss 0.0009764956471473306 0.07290000000000002\n",
      "============== 580 ============ loss 0.0009761932486524586 0.07290000000000002\n",
      "============== 581 ============ loss 0.0009758998161262607 0.07290000000000002\n",
      "============== 582 ============ loss 0.0009756150838432982 0.07290000000000002\n",
      "============== 583 ============ loss 0.000975338793952122 0.07290000000000002\n",
      "============== 584 ============ loss 0.0009750706962414992 0.07290000000000002\n",
      "============== 585 ============ loss 0.0009748105479140251 0.07290000000000002\n",
      "============== 586 ============ loss 0.0009745581133672153 0.07290000000000002\n",
      "============== 587 ============ loss 0.000974313163979192 0.07290000000000002\n",
      "============== 588 ============ loss 0.0009740754779029194 0.07290000000000002\n",
      "============== 589 ============ loss 0.0009738448398644096 0.07290000000000002\n",
      "============== 590 ============ loss 0.0009736210409694106 0.07290000000000002\n",
      "============== 591 ============ loss 0.000973403878512691 0.07290000000000002\n",
      "============== 592 ============ loss 0.0009731931557962837 0.07290000000000002\n",
      "============== 593 ============ loss 0.0009729886819501796 0.07290000000000002\n",
      "============== 594 ============ loss 0.0009727902717605343 0.07290000000000002\n",
      "============== 595 ============ loss 0.0009725977455016167 0.07290000000000002\n",
      "============== 596 ============ loss 0.0009724109287735215 0.07290000000000002\n",
      "============== 597 ============ loss 0.0009722296523441458 0.07290000000000002\n",
      "============== 598 ============ loss 0.0009720537519959958 0.07290000000000002\n",
      "============== 599 ============ loss 0.0009718830683781753 0.07290000000000002\n",
      "============== 600 ============ loss 0.000971717446861346 0.06561000000000002\n",
      "============== 601 ============ loss 0.0009715726993531213 0.06561000000000002\n",
      "============== 602 ============ loss 0.0009714318184860747 0.06561000000000002\n",
      "============== 603 ============ loss 0.0009712947010300429 0.06561000000000002\n",
      "============== 604 ============ loss 0.000971161246509975 0.06561000000000002\n",
      "============== 605 ============ loss 0.0009710313571327023 0.06561000000000002\n",
      "============== 606 ============ loss 0.0009709049377149525 0.06561000000000002\n",
      "============== 607 ============ loss 0.0009707818956137627 0.06561000000000002\n",
      "============== 608 ============ loss 0.0009706621406594921 0.06561000000000002\n",
      "============== 609 ============ loss 0.000970545585088384 0.06561000000000002\n",
      "============== 610 ============ loss 0.0009704321434797939 0.06561000000000002\n",
      "============== 611 ============ loss 0.0009703217326925391 0.06561000000000002\n",
      "============== 612 ============ loss 0.0009702142718047905 0.06561000000000002\n",
      "============== 613 ============ loss 0.0009701096820546257 0.06561000000000002\n",
      "============== 614 ============ loss 0.0009700078867822323 0.06561000000000002\n",
      "============== 615 ============ loss 0.0009699088113738544 0.06561000000000002\n",
      "============== 616 ============ loss 0.0009698123832075242 0.06561000000000002\n",
      "============== 617 ============ loss 0.0009697185315996325 0.06561000000000002\n",
      "============== 618 ============ loss 0.0009696271877529476 0.06561000000000002\n",
      "============== 619 ============ loss 0.0009695382847068806 0.06561000000000002\n",
      "============== 620 ============ loss 0.0009694517572879111 0.06561000000000002\n",
      "============== 621 ============ loss 0.0009693675420622543 0.06561000000000002\n",
      "============== 622 ============ loss 0.0009692855772891493 0.06561000000000002\n",
      "============== 623 ============ loss 0.0009692058028761908 0.06561000000000002\n",
      "============== 624 ============ loss 0.000969128160334432 0.06561000000000002\n",
      "============== 625 ============ loss 0.0009690525927366547 0.06561000000000002\n",
      "============== 626 ============ loss 0.0009689790446749498 0.06561000000000002\n",
      "============== 627 ============ loss 0.0009689074622202893 0.06561000000000002\n",
      "============== 628 ============ loss 0.0009688377928834886 0.06561000000000002\n",
      "============== 629 ============ loss 0.0009687699855760599 0.06561000000000002\n",
      "============== 630 ============ loss 0.000968703990573544 0.06561000000000002\n",
      "============== 631 ============ loss 0.0009686397594787567 0.06561000000000002\n",
      "============== 632 ============ loss 0.0009685772451862817 0.06561000000000002\n",
      "============== 633 ============ loss 0.0009685164018480855 0.06561000000000002\n",
      "============== 634 ============ loss 0.0009684571848401709 0.06561000000000002\n",
      "============== 635 ============ loss 0.0009683995507296304 0.06561000000000002\n",
      "============== 636 ============ loss 0.0009683434572430624 0.06561000000000002\n",
      "============== 637 ============ loss 0.0009682888632353215 0.06561000000000002\n",
      "============== 638 ============ loss 0.0009682357286597594 0.06561000000000002\n",
      "============== 639 ============ loss 0.0009681840145387285 0.06561000000000002\n",
      "============== 640 ============ loss 0.0009681336829351846 0.06561000000000002\n",
      "============== 641 ============ loss 0.0009680846969245318 0.06561000000000002\n",
      "============== 642 ============ loss 0.0009680370205681919 0.06561000000000002\n",
      "============== 643 ============ loss 0.0009679906188868059 0.06561000000000002\n",
      "============== 644 ============ loss 0.000967945457834907 0.06561000000000002\n",
      "============== 645 ============ loss 0.0009679015042758348 0.06561000000000002\n",
      "============== 646 ============ loss 0.000967858725957488 0.06561000000000002\n",
      "============== 647 ============ loss 0.0009678170914887938 0.06561000000000002\n",
      "============== 648 ============ loss 0.0009677765703168535 0.06561000000000002\n",
      "============== 649 ============ loss 0.0009677371327042963 0.06561000000000002\n",
      "============== 650 ============ loss 0.0009676987497076157 0.06561000000000002\n",
      "============== 651 ============ loss 0.0009676613931560403 0.06561000000000002\n",
      "============== 652 ============ loss 0.0009676250356309135 0.06561000000000002\n",
      "============== 653 ============ loss 0.0009675896504455402 0.06561000000000002\n",
      "============== 654 ============ loss 0.0009675552116256631 0.06561000000000002\n",
      "============== 655 ============ loss 0.0009675216938906303 0.06561000000000002\n",
      "============== 656 ============ loss 0.00096748907263443 0.06561000000000002\n",
      "============== 657 ============ loss 0.0009674573239083573 0.06561000000000002\n",
      "============== 658 ============ loss 0.0009674264244028693 0.06561000000000002\n",
      "============== 659 ============ loss 0.0009673963514307422 0.06561000000000002\n",
      "============== 660 ============ loss 0.0009673670829105219 0.06561000000000002\n",
      "============== 661 ============ loss 0.0009673385973504365 0.06561000000000002\n",
      "============== 662 ============ loss 0.0009673108738322381 0.06561000000000002\n",
      "============== 663 ============ loss 0.00096728389199649 0.06561000000000002\n",
      "============== 664 ============ loss 0.0009672576320270981 0.06561000000000002\n",
      "============== 665 ============ loss 0.000967232074637253 0.06561000000000002\n",
      "============== 666 ============ loss 0.0009672072010549833 0.06561000000000002\n",
      "============== 667 ============ loss 0.0009671829930096021 0.06561000000000002\n",
      "============== 668 ============ loss 0.0009671594327181975 0.06561000000000002\n",
      "============== 669 ============ loss 0.000967136502872775 0.06561000000000002\n",
      "============== 670 ============ loss 0.0009671141866273593 0.06561000000000002\n",
      "============== 671 ============ loss 0.0009670924675859362 0.06561000000000002\n",
      "============== 672 ============ loss 0.0009670713297901169 0.06561000000000002\n",
      "============== 673 ============ loss 0.0009670507577077851 0.06561000000000002\n",
      "============== 674 ============ loss 0.0009670307362215316 0.06561000000000002\n",
      "============== 675 ============ loss 0.0009670112506175841 0.06561000000000002\n",
      "============== 676 ============ loss 0.0009669922865750412 0.06561000000000002\n",
      "============== 677 ============ loss 0.0009669738301554779 0.06561000000000002\n",
      "============== 678 ============ loss 0.0009669558677926941 0.06561000000000002\n",
      "============== 679 ============ loss 0.0009669383862826807 0.06561000000000002\n",
      "============== 680 ============ loss 0.000966921372774052 0.06561000000000002\n",
      "============== 681 ============ loss 0.00096690481475861 0.06561000000000002\n",
      "============== 682 ============ loss 0.0009668887000622848 0.06561000000000002\n",
      "============== 683 ============ loss 0.0009668730168359048 0.06561000000000002\n",
      "============== 684 ============ loss 0.0009668577535469389 0.06561000000000002\n",
      "============== 685 ============ loss 0.0009668428989706856 0.06561000000000002\n",
      "============== 686 ============ loss 0.0009668284421822918 0.06561000000000002\n",
      "============== 687 ============ loss 0.0009668143725486124 0.06561000000000002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== 688 ============ loss 0.0009668006797206002 0.06561000000000002\n",
      "============== 689 ============ loss 0.0009667873536254029 0.06561000000000002\n",
      "============== 690 ============ loss 0.0009667743844593516 0.06561000000000002\n",
      "============== 691 ============ loss 0.0009667617626805677 0.06561000000000002\n",
      "============== 692 ============ loss 0.0009667494790019137 0.06561000000000002\n",
      "============== 693 ============ loss 0.0009667375243844198 0.06561000000000002\n",
      "============== 694 ============ loss 0.0009667258900304918 0.06561000000000002\n",
      "============== 695 ============ loss 0.0009667145673774964 0.06561000000000002\n",
      "============== 696 ============ loss 0.0009667035480915269 0.06561000000000002\n",
      "============== 697 ============ loss 0.000966692824061232 0.06561000000000002\n",
      "============== 698 ============ loss 0.0009666823873919451 0.06561000000000002\n",
      "============== 699 ============ loss 0.0009666722303998806 0.06561000000000002\n",
      "============== 700 ============ loss 0.0009666623456064753 0.06561000000000002\n",
      "============== 701 ============ loss 0.0009666527257329363 0.06561000000000002\n",
      "============== 702 ============ loss 0.0009666433636948161 0.06561000000000002\n",
      "============== 703 ============ loss 0.0009666342525970872 0.06561000000000002\n",
      "============== 704 ============ loss 0.0009666253857287255 0.06561000000000002\n",
      "============== 705 ============ loss 0.0009666167565581036 0.06561000000000002\n",
      "============== 706 ============ loss 0.0009666083587279857 0.06561000000000002\n",
      "============== 707 ============ loss 0.0009666001860509987 0.06561000000000002\n",
      "============== 708 ============ loss 0.0009665922325050758 0.06561000000000002\n",
      "============== 709 ============ loss 0.000966584492228991 0.06561000000000002\n",
      "============== 710 ============ loss 0.0009665769595180946 0.06561000000000002\n",
      "============== 711 ============ loss 0.0009665696288202209 0.06561000000000002\n",
      "============== 712 ============ loss 0.0009665624947314373 0.06561000000000002\n",
      "============== 713 ============ loss 0.0009665555519922487 0.06561000000000002\n",
      "============== 714 ============ loss 0.0009665487954836157 0.06561000000000002\n",
      "============== 715 ============ loss 0.0009665422202233282 0.06561000000000002\n",
      "============== 716 ============ loss 0.0009665358213622366 0.06561000000000002\n",
      "============== 717 ============ loss 0.0009665295941808019 0.06561000000000002\n",
      "============== 718 ============ loss 0.0009665235340855619 0.06561000000000002\n",
      "============== 719 ============ loss 0.0009665176366057712 0.06561000000000002\n",
      "============== 720 ============ loss 0.0009665118973901802 0.06561000000000002\n",
      "============== 721 ============ loss 0.0009665063122037608 0.06561000000000002\n",
      "============== 722 ============ loss 0.000966500876924675 0.06561000000000002\n",
      "============== 723 ============ loss 0.0009664955875411923 0.06561000000000002\n",
      "============== 724 ============ loss 0.0009664904401488764 0.06561000000000002\n",
      "============== 725 ============ loss 0.0009664854309475058 0.06561000000000002\n",
      "============== 726 ============ loss 0.000966480556238508 0.06561000000000002\n",
      "============== 727 ============ loss 0.0009664758124220673 0.06561000000000002\n",
      "============== 728 ============ loss 0.0009664711959946349 0.06561000000000002\n",
      "============== 729 ============ loss 0.0009664667035462601 0.06561000000000002\n",
      "============== 730 ============ loss 0.0009664623317581448 0.06561000000000002\n",
      "============== 731 ============ loss 0.0009664580774001581 0.06561000000000002\n",
      "============== 732 ============ loss 0.0009664539373285397 0.06561000000000002\n",
      "============== 733 ============ loss 0.0009664499084834882 0.06561000000000002\n",
      "============== 734 ============ loss 0.0009664459878871037 0.06561000000000002\n",
      "============== 735 ============ loss 0.0009664421726408975 0.06561000000000002\n",
      "============== 736 ============ loss 0.0009664384599240381 0.06561000000000002\n",
      "============== 737 ============ loss 0.000966434846990972 0.06561000000000002\n",
      "============== 738 ============ loss 0.0009664313311695952 0.06561000000000002\n",
      "============== 739 ============ loss 0.0009664279098591948 0.06561000000000002\n",
      "============== 740 ============ loss 0.0009664245805286148 0.06561000000000002\n",
      "============== 741 ============ loss 0.0009664213407143234 0.06561000000000002\n",
      "============== 742 ============ loss 0.0009664181880186934 0.06561000000000002\n",
      "============== 743 ============ loss 0.0009664151201081211 0.06561000000000002\n",
      "============== 744 ============ loss 0.0009664121347114786 0.06561000000000002\n",
      "============== 745 ============ loss 0.000966409229618272 0.06561000000000002\n",
      "============== 746 ============ loss 0.0009664064026771834 0.06561000000000002\n",
      "============== 747 ============ loss 0.0009664036517943562 0.06561000000000002\n",
      "============== 748 ============ loss 0.0009664009749319516 0.06561000000000002\n",
      "============== 749 ============ loss 0.0009663983701066162 0.06561000000000002\n",
      "============== 750 ============ loss 0.0009663958353879781 0.06561000000000002\n",
      "============== 751 ============ loss 0.0009663933688973513 0.06561000000000002\n",
      "============== 752 ============ loss 0.0009663909688062402 0.06561000000000002\n",
      "============== 753 ============ loss 0.0009663886333350882 0.06561000000000002\n",
      "============== 754 ============ loss 0.0009663863607518603 0.06561000000000002\n",
      "============== 755 ============ loss 0.0009663841493709123 0.06561000000000002\n",
      "============== 756 ============ loss 0.0009663819975516511 0.06561000000000002\n",
      "============== 757 ============ loss 0.0009663799036973095 0.06561000000000002\n",
      "============== 758 ============ loss 0.0009663778662539475 0.06561000000000002\n",
      "============== 759 ============ loss 0.0009663758837090402 0.06561000000000002\n",
      "============== 760 ============ loss 0.000966373954590635 0.06561000000000002\n",
      "============== 761 ============ loss 0.0009663720774661009 0.06561000000000002\n",
      "============== 762 ============ loss 0.0009663702509411239 0.06561000000000002\n",
      "============== 763 ============ loss 0.0009663684736586993 0.06561000000000002\n",
      "============== 764 ============ loss 0.0009663667442981127 0.06561000000000002\n",
      "============== 765 ============ loss 0.0009663650615740041 0.06561000000000002\n",
      "============== 766 ============ loss 0.0009663634242353678 0.06561000000000002\n",
      "============== 767 ============ loss 0.0009663618310646919 0.06561000000000002\n",
      "============== 768 ============ loss 0.0009663602808770283 0.06561000000000002\n",
      "============== 769 ============ loss 0.0009663587725191195 0.06561000000000002\n",
      "============== 770 ============ loss 0.0009663573048685979 0.06561000000000002\n",
      "============== 771 ============ loss 0.000966355876833089 0.06561000000000002\n",
      "============== 772 ============ loss 0.0009663544873494822 0.06561000000000002\n",
      "============== 773 ============ loss 0.000966353135383103 0.06561000000000002\n",
      "============== 774 ============ loss 0.0009663518199269396 0.06561000000000002\n",
      "============== 775 ============ loss 0.0009663505400009752 0.06561000000000002\n",
      "============== 776 ============ loss 0.0009663492946513834 0.06561000000000002\n",
      "============== 777 ============ loss 0.0009663480829498831 0.06561000000000002\n",
      "============== 778 ============ loss 0.0009663469039930408 0.06561000000000002\n",
      "============== 779 ============ loss 0.0009663457569015905 0.06561000000000002\n",
      "============== 780 ============ loss 0.0009663446408198202 0.06561000000000002\n",
      "============== 781 ============ loss 0.0009663435549148875 0.06561000000000002\n",
      "============== 782 ============ loss 0.0009663424983763092 0.06561000000000002\n",
      "============== 783 ============ loss 0.0009663414704152282 0.06561000000000002\n",
      "============== 784 ============ loss 0.0009663404702639478 0.06561000000000002\n",
      "============== 785 ============ loss 0.0009663394971753318 0.06561000000000002\n",
      "============== 786 ============ loss 0.0009663385504222111 0.06561000000000002\n",
      "============== 787 ============ loss 0.0009663376292969096 0.06561000000000002\n",
      "============== 788 ============ loss 0.0009663367331107119 0.06561000000000002\n",
      "============== 789 ============ loss 0.0009663358611933291 0.06561000000000002\n",
      "============== 790 ============ loss 0.0009663350128924079 0.06561000000000002\n",
      "============== 791 ============ loss 0.000966334187573098 0.06561000000000002\n",
      "============== 792 ============ loss 0.0009663333846175376 0.06561000000000002\n",
      "============== 793 ============ loss 0.0009663326034244097 0.06561000000000002\n",
      "============== 794 ============ loss 0.0009663318434085076 0.06561000000000002\n",
      "============== 795 ============ loss 0.0009663311040002888 0.06561000000000002\n",
      "============== 796 ============ loss 0.0009663303846454922 0.06561000000000002\n",
      "============== 797 ============ loss 0.000966329684804673 0.06561000000000002\n",
      "============== 798 ============ loss 0.0009663290039528777 0.06561000000000002\n",
      "============== 799 ============ loss 0.0009663283415791722 0.06561000000000002\n",
      "============== 800 ============ loss 0.000966327697186353 0.05904900000000002\n",
      "============== 801 ============ loss 0.0009663271325848871 0.05904900000000002\n",
      "============== 802 ============ loss 0.0009663265817935232 0.05904900000000002\n",
      "============== 803 ============ loss 0.000966326044477444 0.05904900000000002\n",
      "============== 804 ============ loss 0.0009663255203099504 0.05904900000000002\n",
      "============== 805 ============ loss 0.0009663250089722084 0.05904900000000002\n",
      "============== 806 ============ loss 0.0009663245101530978 0.05904900000000002\n",
      "============== 807 ============ loss 0.0009663240235489906 0.05904900000000002\n",
      "============== 808 ============ loss 0.0009663235488635997 0.05904900000000002\n",
      "============== 809 ============ loss 0.000966323085807783 0.05904900000000002\n",
      "============== 810 ============ loss 0.0009663226340993974 0.05904900000000002\n",
      "============== 811 ============ loss 0.0009663221934630901 0.05904900000000002\n",
      "============== 812 ============ loss 0.0009663217636301695 0.05904900000000002\n",
      "============== 813 ============ loss 0.0009663213443384331 0.05904900000000002\n",
      "============== 814 ============ loss 0.000966320935331995 0.05904900000000002\n",
      "============== 815 ============ loss 0.0009663205363611757 0.05904900000000002\n",
      "============== 816 ============ loss 0.0009663201471823077 0.05904900000000002\n",
      "============== 817 ============ loss 0.0009663197675576174 0.05904900000000002\n",
      "============== 818 ============ loss 0.0009663193972550635 0.05904900000000002\n",
      "============== 819 ============ loss 0.0009663190360482233 0.05904900000000002\n",
      "============== 820 ============ loss 0.0009663186837161416 0.05904900000000002\n",
      "============== 821 ============ loss 0.000966318340043193 0.05904900000000002\n",
      "============== 822 ============ loss 0.0009663180048189745 0.05904900000000002\n",
      "============== 823 ============ loss 0.0009663176778381533 0.05904900000000002\n",
      "============== 824 ============ loss 0.0009663173589003646 0.05904900000000002\n",
      "============== 825 ============ loss 0.0009663170478100926 0.05904900000000002\n",
      "============== 826 ============ loss 0.0009663167443765472 0.05904900000000002\n",
      "============== 827 ============ loss 0.0009663164484135418 0.05904900000000002\n",
      "============== 828 ============ loss 0.0009663161597393878 0.05904900000000002\n",
      "============== 829 ============ loss 0.000966315878176793 0.05904900000000002\n",
      "============== 830 ============ loss 0.0009663156035527572 0.05904900000000002\n",
      "============== 831 ============ loss 0.0009663153356984531 0.05904900000000002\n",
      "============== 832 ============ loss 0.0009663150744491458 0.05904900000000002\n",
      "============== 833 ============ loss 0.0009663148196440794 0.05904900000000002\n",
      "============== 834 ============ loss 0.0009663145711263787 0.05904900000000002\n",
      "============== 835 ============ loss 0.0009663143287429733 0.05904900000000002\n",
      "============== 836 ============ loss 0.0009663140923444892 0.05904900000000002\n",
      "============== 837 ============ loss 0.0009663138617851674 0.05904900000000002\n",
      "============== 838 ============ loss 0.0009663136369227713 0.05904900000000002\n",
      "============== 839 ============ loss 0.0009663134176185188 0.05904900000000002\n",
      "============== 840 ============ loss 0.0009663132037369668 0.05904900000000002\n",
      "============== 841 ============ loss 0.0009663129951459628 0.05904900000000002\n",
      "============== 842 ============ loss 0.0009663127917165412 0.05904900000000002\n",
      "============== 843 ============ loss 0.0009663125933228688 0.05904900000000002\n",
      "============== 844 ============ loss 0.0009663123998421552 0.05904900000000002\n",
      "============== 845 ============ loss 0.000966312211154576 0.05904900000000002\n",
      "============== 846 ============ loss 0.0009663120271432178 0.05904900000000002\n",
      "============== 847 ============ loss 0.0009663118476939851 0.05904900000000002\n",
      "============== 848 ============ loss 0.0009663116726955531 0.05904900000000002\n",
      "============== 849 ============ loss 0.0009663115020392965 0.05904900000000002\n",
      "============== 850 ============ loss 0.00096631133561921 0.05904900000000002\n",
      "============== 851 ============ loss 0.0009663111733318628 0.05904900000000002\n",
      "============== 852 ============ loss 0.0009663110150763268 0.05904900000000002\n",
      "============== 853 ============ loss 0.0009663108607541201 0.05904900000000002\n",
      "============== 854 ============ loss 0.0009663107102691408 0.05904900000000002\n",
      "============== 855 ============ loss 0.0009663105635276307 0.05904900000000002\n",
      "============== 856 ============ loss 0.0009663104204380844 0.05904900000000002\n",
      "============== 857 ============ loss 0.0009663102809112279 0.05904900000000002\n",
      "============== 858 ============ loss 0.0009663101448599402 0.05904900000000002\n",
      "============== 859 ============ loss 0.000966310012199222 0.05904900000000002\n",
      "============== 860 ============ loss 0.0009663098828461336 0.05904900000000002\n",
      "============== 861 ============ loss 0.0009663097567197361 0.05904900000000002\n",
      "============== 862 ============ loss 0.0009663096337410627 0.05904900000000002\n",
      "============== 863 ============ loss 0.0009663095138330582 0.05904900000000002\n",
      "============== 864 ============ loss 0.0009663093969205338 0.05904900000000002\n",
      "============== 865 ============ loss 0.0009663092829301299 0.05904900000000002\n",
      "============== 866 ============ loss 0.000966309171790262 0.05904900000000002\n",
      "============== 867 ============ loss 0.0009663090634310884 0.05904900000000002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== 868 ============ loss 0.0009663089577844541 0.05904900000000002\n",
      "============== 869 ============ loss 0.0009663088547838579 0.05904900000000002\n",
      "============== 870 ============ loss 0.0009663087543644181 0.05904900000000002\n",
      "============== 871 ============ loss 0.0009663086564628259 0.05904900000000002\n",
      "============== 872 ============ loss 0.0009663085610173104 0.05904900000000002\n",
      "============== 873 ============ loss 0.0009663084679675972 0.05904900000000002\n",
      "============== 874 ============ loss 0.0009663083772548809 0.05904900000000002\n",
      "============== 875 ============ loss 0.0009663082888217737 0.05904900000000002\n",
      "============== 876 ============ loss 0.0009663082026122944 0.05904900000000002\n",
      "============== 877 ============ loss 0.0009663081185718121 0.05904900000000002\n",
      "============== 878 ============ loss 0.0009663080366470291 0.05904900000000002\n",
      "============== 879 ============ loss 0.0009663079567859404 0.05904900000000002\n",
      "============== 880 ============ loss 0.0009663078789378028 0.05904900000000002\n",
      "============== 881 ============ loss 0.00096630780305311 0.05904900000000002\n",
      "============== 882 ============ loss 0.0009663077290835546 0.05904900000000002\n",
      "============== 883 ============ loss 0.0009663076569820122 0.05904900000000002\n",
      "============== 884 ============ loss 0.0009663075867024875 0.05904900000000002\n",
      "============== 885 ============ loss 0.0009663075182001246 0.05904900000000002\n",
      "============== 886 ============ loss 0.000966307451431145 0.05904900000000002\n",
      "============== 887 ============ loss 0.0009663073863528396 0.05904900000000002\n",
      "============== 888 ============ loss 0.0009663073229235369 0.05904900000000002\n",
      "============== 889 ============ loss 0.0009663072611025782 0.05904900000000002\n",
      "============== 890 ============ loss 0.0009663072008502977 0.05904900000000002\n",
      "============== 891 ============ loss 0.0009663071421279945 0.05904900000000002\n",
      "============== 892 ============ loss 0.0009663070848979064 0.05904900000000002\n",
      "============== 893 ============ loss 0.0009663070291231945 0.05904900000000002\n",
      "============== 894 ============ loss 0.0009663069747679151 0.05904900000000002\n",
      "============== 895 ============ loss 0.0009663069217970017 0.05904900000000002\n",
      "============== 896 ============ loss 0.0009663068701762373 0.05904900000000002\n",
      "============== 897 ============ loss 0.0009663068198722454 0.05904900000000002\n",
      "============== 898 ============ loss 0.0009663067708524596 0.05904900000000002\n",
      "============== 899 ============ loss 0.0009663067230851043 0.05904900000000002\n",
      "============== 900 ============ loss 0.0009663066765391834 0.05904900000000002\n",
      "============== 901 ============ loss 0.0009663066311844546 0.05904900000000002\n",
      "============== 902 ============ loss 0.0009663065869914098 0.05904900000000002\n",
      "============== 903 ============ loss 0.0009663065439312691 0.05904900000000002\n",
      "============== 904 ============ loss 0.0009663065019759452 0.05904900000000002\n",
      "============== 905 ============ loss 0.0009663064610980414 0.05904900000000002\n",
      "============== 906 ============ loss 0.0009663064212708297 0.05904900000000002\n",
      "============== 907 ============ loss 0.0009663063824682316 0.05904900000000002\n",
      "============== 908 ============ loss 0.0009663063446648099 0.05904900000000002\n",
      "============== 909 ============ loss 0.0009663063078357415 0.05904900000000002\n",
      "============== 910 ============ loss 0.0009663062719568183 0.05904900000000002\n",
      "============== 911 ============ loss 0.0009663062370044167 0.05904900000000002\n",
      "============== 912 ============ loss 0.0009663062029554919 0.05904900000000002\n",
      "============== 913 ============ loss 0.0009663061697875632 0.05904900000000002\n",
      "============== 914 ============ loss 0.0009663061374786987 0.05904900000000002\n",
      "============== 915 ============ loss 0.0009663061060075069 0.05904900000000002\n",
      "============== 916 ============ loss 0.000966306075353115 0.05904900000000002\n",
      "============== 917 ============ loss 0.0009663060454951576 0.05904900000000002\n",
      "============== 918 ============ loss 0.0009663060164137744 0.05904900000000002\n",
      "============== 919 ============ loss 0.000966305988089589 0.05904900000000002\n",
      "============== 920 ============ loss 0.0009663059605036939 0.05904900000000002\n",
      "============== 921 ============ loss 0.0009663059336376573 0.05904900000000002\n",
      "============== 922 ============ loss 0.000966305907473485 0.05904900000000002\n",
      "============== 923 ============ loss 0.0009663058819936277 0.05904900000000002\n",
      "============== 924 ============ loss 0.0009663058571809716 0.05904900000000002\n",
      "============== 925 ============ loss 0.0009663058330188167 0.05904900000000002\n",
      "============== 926 ============ loss 0.0009663058094908734 0.05904900000000002\n",
      "============== 927 ============ loss 0.0009663057865812531 0.05904900000000002\n",
      "============== 928 ============ loss 0.0009663057642744507 0.05904900000000002\n",
      "============== 929 ============ loss 0.0009663057425553531 0.05904900000000002\n",
      "============== 930 ============ loss 0.0009663057214092074 0.05904900000000002\n",
      "============== 931 ============ loss 0.0009663057008216299 0.05904900000000002\n",
      "============== 932 ============ loss 0.0009663056807785828 0.05904900000000002\n",
      "============== 933 ============ loss 0.0009663056612663792 0.05904900000000002\n",
      "============== 934 ============ loss 0.0009663056422716667 0.05904900000000002\n",
      "============== 935 ============ loss 0.0009663056237814215 0.05904900000000002\n",
      "============== 936 ============ loss 0.0009663056057829357 0.05904900000000002\n",
      "============== 937 ============ loss 0.0009663055882638192 0.05904900000000002\n",
      "============== 938 ============ loss 0.0009663055712119847 0.05904900000000002\n",
      "============== 939 ============ loss 0.0009663055546156406 0.05904900000000002\n",
      "============== 940 ============ loss 0.0009663055384632873 0.05904900000000002\n",
      "============== 941 ============ loss 0.000966305522743708 0.05904900000000002\n",
      "============== 942 ============ loss 0.0009663055074459628 0.05904900000000002\n",
      "============== 943 ============ loss 0.0009663054925593794 0.05904900000000002\n",
      "============== 944 ============ loss 0.0009663054780735482 0.05904900000000002\n",
      "============== 945 ============ loss 0.0009663054639783183 0.05904900000000002\n",
      "============== 946 ============ loss 0.0009663054502637875 0.05904900000000002\n",
      "============== 947 ============ loss 0.0009663054369202994 0.05904900000000002\n",
      "============== 948 ============ loss 0.000966305423938433 0.05904900000000002\n",
      "============== 949 ============ loss 0.0009663054113090016 0.05904900000000002\n",
      "============== 950 ============ loss 0.000966305399023046 0.05904900000000002\n",
      "============== 951 ============ loss 0.0009663053870718266 0.05904900000000002\n",
      "============== 952 ============ loss 0.0009663053754468204 0.05904900000000002\n",
      "============== 953 ============ loss 0.0009663053641397153 0.05904900000000002\n",
      "============== 954 ============ loss 0.0009663053531424038 0.05904900000000002\n",
      "============== 955 ============ loss 0.0009663053424469798 0.05904900000000002\n",
      "============== 956 ============ loss 0.000966305332045731 0.05904900000000002\n",
      "============== 957 ============ loss 0.0009663053219311407 0.05904900000000002\n",
      "============== 958 ============ loss 0.0009663053120958725 0.05904900000000002\n",
      "============== 959 ============ loss 0.0009663053025327725 0.05904900000000002\n",
      "============== 960 ============ loss 0.0009663052932348679 0.05904900000000002\n",
      "============== 961 ============ loss 0.0009663052841953565 0.05904900000000002\n",
      "============== 962 ============ loss 0.0009663052754076046 0.05904900000000002\n",
      "============== 963 ============ loss 0.0009663052668651413 0.05904900000000002\n",
      "============== 964 ============ loss 0.0009663052585616605 0.05904900000000002\n",
      "============== 965 ============ loss 0.0009663052504910095 0.05904900000000002\n",
      "============== 966 ============ loss 0.000966305242647186 0.05904900000000002\n",
      "============== 967 ============ loss 0.0009663052350243461 0.05904900000000002\n",
      "============== 968 ============ loss 0.0009663052276167787 0.05904900000000002\n",
      "============== 969 ============ loss 0.0009663052204189249 0.05904900000000002\n",
      "============== 970 ============ loss 0.0009663052134253592 0.05904900000000002\n",
      "============== 971 ============ loss 0.0009663052066307926 0.05904900000000002\n",
      "============== 972 ============ loss 0.0009663052000300664 0.05904900000000002\n",
      "============== 973 ============ loss 0.0009663051936181518 0.05904900000000002\n",
      "============== 974 ============ loss 0.0009663051873901444 0.05904900000000002\n",
      "============== 975 ============ loss 0.0009663051813412638 0.05904900000000002\n",
      "============== 976 ============ loss 0.0009663051754668455 0.05904900000000002\n",
      "============== 977 ============ loss 0.0009663051697623449 0.05904900000000002\n",
      "============== 978 ============ loss 0.0009663051642233295 0.05904900000000002\n",
      "============== 979 ============ loss 0.0009663051588454782 0.05904900000000002\n",
      "============== 980 ============ loss 0.0009663051536245747 0.05904900000000002\n",
      "============== 981 ============ loss 0.0009663051485565129 0.05904900000000002\n",
      "============== 982 ============ loss 0.0009663051436372869 0.05904900000000002\n",
      "============== 983 ============ loss 0.0009663051388629917 0.05904900000000002\n",
      "============== 984 ============ loss 0.0009663051342298192 0.05904900000000002\n",
      "============== 985 ============ loss 0.0009663051297340568 0.05904900000000002\n",
      "============== 986 ============ loss 0.000966305125372086 0.05904900000000002\n",
      "============== 987 ============ loss 0.0009663051211403791 0.05904900000000002\n",
      "============== 988 ============ loss 0.0009663051170354963 0.05904900000000002\n",
      "============== 989 ============ loss 0.0009663051130540847 0.05904900000000002\n",
      "============== 990 ============ loss 0.0009663051091928752 0.05904900000000002\n",
      "============== 991 ============ loss 0.0009663051054486805 0.05904900000000002\n",
      "============== 992 ============ loss 0.0009663051018183937 0.05904900000000002\n",
      "============== 993 ============ loss 0.0009663050982989871 0.05904900000000002\n",
      "============== 994 ============ loss 0.0009663050948875085 0.05904900000000002\n",
      "============== 995 ============ loss 0.0009663050915810819 0.05904900000000002\n",
      "============== 996 ============ loss 0.0009663050883768993 0.05904900000000002\n",
      "============== 997 ============ loss 0.0009663050852722285 0.05904900000000002\n",
      "============== 998 ============ loss 0.0009663050822644032 0.05904900000000002\n",
      "============== 999 ============ loss 0.000966305079350827 0.05904900000000002\n",
      "iter Done! Final loss: 0.000966305079350827\n"
     ]
    }
   ],
   "source": [
    "dv_hist = reg.fit(data_x, data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.92272362e-01,  8.22117652e-02,  4.34671442e+02],\n",
       "       [-1.33501977e-01,  9.58413827e-01,  6.55962640e+01],\n",
       "       [-2.09248580e-04,  1.60937718e-05,  1.00000000e+00]])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.90900075e-01,  8.06359397e-02,  4.34671372e+02],\n",
       "       [-1.33281047e-01,  9.58667338e-01,  6.55963190e+01],\n",
       "       [ 6.01986564e-01,  6.91658939e-01,  1.00000000e+00]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.var.val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[366 632   1] [   205.59822578    635.80706082 200547.58662   ] [   205.83779247    636.08217029 200547.59589302]\n",
      "[691 937   1] [   422.36864044    954.84210175 361822.66579   ] [   422.22959761    954.68238868 361822.66920104]\n",
      "[386  74   1] [2.95937776e+02 1.02656381e+02 1.72638300e+05] [2.96026618e+02 1.02758515e+02 1.72638277e+05]\n",
      "[570 609   1] [   370.2923331    630.5347429 287711.846716 ] [   370.24687188    630.48255322 287711.84051069]\n"
     ]
    }
   ],
   "source": [
    "for idx in range(4):\n",
    "    test_x = data_x[idx]\n",
    "    test_y = data_y[idx]\n",
    "    pred_y = np.dot(data_x[idx], reg.var.val)\n",
    "    print (data_x[idx], test_y, pred_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
