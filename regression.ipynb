{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geometric distance L2norm Regression for Homography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data init \n",
    "M = 4  # number of data\n",
    "N = 3  # features number\n",
    "data_x = np.random.rand(N,M)*1000\n",
    "data_x = data_x\n",
    "data_x[2,:] = 1\n",
    "t_m = np.random.rand(N,N)\n",
    "t_m[2,2] = 1\n",
    "data_y = np.dot(t_m,data_x.copy().astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[825.11177597,  76.97138931, 188.78344314,  96.54784501],\n",
       "       [939.91483525, 198.49137721, 390.13632387, 500.63299411],\n",
       "       [  1.        ,   1.        ,   1.        ,   1.        ]])"
      ]
     },
     "execution_count": 1213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1214,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_m = np.array([[ 7.92272362e-01,  8.22117652e-02,  4.34671442e+02],\n",
    "       [-1.33501977e-01,  9.58413827e-01,  6.55962640e+01],\n",
    "       [-2.09248580e-04,  1.60937718e-05,  1.00000000e+00]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.16565676e+03 5.11972073e+02 6.16313142e+02 5.52321553e+02]\n",
      " [8.56269585e+02 2.45557312e+02 4.14305348e+02 5.32520520e+02]\n",
      " [8.42473307e-01 9.87088321e-01 9.66776098e-01 9.87854574e-01]]\n"
     ]
    }
   ],
   "source": [
    "data_y = np.dot(t_m, data_x.copy().astype(float))\n",
    "print(data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1302,
   "metadata": {},
   "outputs": [],
   "source": [
    "DBG = 0\n",
    "DBG_LOSS = 0\n",
    "def DEBUG(*args):\n",
    "    if DBG: print(*args)\n",
    "\n",
    "def DEBUGLOSS(*args):\n",
    "    if DBG_LOSS: print(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1303,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variables(object):\n",
    "    \"\"\"\n",
    "    Variable that can used to train with mask\n",
    "    \"\"\"\n",
    "    __slots__ = ('train_mask','dim','trainable','val')\n",
    "    def __init__(self, val=None, dim=None, trainable=1):\n",
    "        self.dim = dim\n",
    "        if val is None:\n",
    "            self.val = np.random.rand(dim[0],dim[1])\n",
    "            #self.val = np.zeros((dim[0],dim[1]))\n",
    "        else:\n",
    "            assert val.shape == dim, \"input val not consistent to dim input\"\n",
    "            self.val = val\n",
    "        self.train_mask = np.ones_like(self.val)\n",
    "        self.trainable = trainable\n",
    "    def set_trainMask(self, mask):\n",
    "        assert mask.shape == self.dim\n",
    "        self.train_mask = mask\n",
    "    def update(self, dv):\n",
    "        if self.trainable:\n",
    "            self.val -= (dv * self.train_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1304,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression(object):\n",
    "    \"\"\"\n",
    "    Note. Training gradient may explode if lr is too large.\n",
    "    \"\"\"\n",
    "    __slots__ = ('var', 'iter', 'lr', 'err_th', 'j_hist',\n",
    "                'x_mean', 'y_mean', 'x_std', 'y_std',\n",
    "                'decay_rate', 'loss_weights','norm', 'update_steps', 'dim','data_m',\n",
    "                'solver', 'loss_func')\n",
    "    LOSSFUNCS = {'L2norm':0, 'L1norm':1}\n",
    "    SOLVERS = {'Grad':0,'Adam':1}\n",
    "\n",
    "    def __init__(self, var=None, dim=None, iter=500, decay_rate=0.9, norm=1, update_steps=100, data_m=0,\n",
    "                solver='Grad', loss_func='L2norm'):\n",
    "        self.var = var\n",
    "        self.iter = iter\n",
    "        self.lr = 0.001\n",
    "        self.err_th = 1e-4\n",
    "        self.j_hist = np.zeros((self.iter,1))\n",
    "        self.decay_rate=0.9\n",
    "        self.loss_weights=None\n",
    "        self.update_steps=update_steps\n",
    "        self.norm = norm\n",
    "        self.data_m = data_m\n",
    "        self.solver = solver\n",
    "        self.loss_func = loss_func\n",
    "\n",
    "    def process(self, x, y, method=0):\n",
    "        if method == 0:\n",
    "            return x.copy(), y.copy()\n",
    "        elif method == 1:\n",
    "            n, m = x.shape\n",
    "            self.x_std = np.std(x,axis=1, keepdims=1)\n",
    "            _x = x.copy() / self.x_std\n",
    "            _y = y.copy() / self.x_std\n",
    "            _x[-1,:] = x[-1,:]\n",
    "            _y[-1,:] = y[-1,:]\n",
    "            return _x, _y\n",
    "        elif method == 2:\n",
    "            self.x_mean = np.mean(x,axis=1, keepdims=1)\n",
    "            self.x_std = np.std(x,axis=1, keepdims=1)\n",
    "            _x = (x.copy() - self.x_mean)/self.x_std\n",
    "            _y = (y.copy() - self.x_mean)/self.x_std\n",
    "            return _x, _y\n",
    "        return x.copy(), y.copy()\n",
    "    \n",
    "    def pred(W,x):\n",
    "        return np.dot(W, x)\n",
    "    \n",
    "    def computeLoss(self, res):\n",
    "        n, m = self.dim\n",
    "        p, q = self.var.val.shape\n",
    "        if self.loss_func == 'L2norm':\n",
    "            loss = np.sum(res * res) / (2 * m * n)\n",
    "        if self.norm > 0:\n",
    "            loss += (np.sum(self.var.val ** 2) - 1) / (2 * p * q) * self.norm \n",
    "        return loss\n",
    "    \n",
    "    def lrPolicy(self, iteration, lr):\n",
    "        if (iteration+1) % self.update_steps == 0:\n",
    "            lr *= self.decay_rate\n",
    "        return lr\n",
    "\n",
    "    def lossDiff(self, pred, y):\n",
    "        res = (pred - y)\n",
    "        if self.loss_weights is not None:\n",
    "            res *= self.loss_weights\n",
    "        return res\n",
    "\n",
    "    def computeGradient(self, res, _x, *args):\n",
    "        n, m = self.dim\n",
    "        if self.loss_func == 'L2norm':\n",
    "            tV = np.dot(res, _x.T) / m\n",
    "        if self.norm > 0:\n",
    "            tV += self.var.val * self.norm \n",
    "        return tV\n",
    "    \n",
    "    def computeSolver(self, lr, tV):\n",
    "        if self.solver == 'Grad':\n",
    "            dV = lr * tV\n",
    "        return dV\n",
    "\n",
    "    def updateWeight(self, dV_masked):\n",
    "        if self.var.trainable:\n",
    "            self.var.val -= dV_masked\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        assert len(x.shape) == 2, 'shape of x is not support'\n",
    "        assert self.var.dim[1] == x.shape[0], 'shape of x %s is not consistent to var dim %s' % (self.var.dim, x.shape)\n",
    "        assert self.var.dim[0] == y.shape[0], 'shape of y %s is not consistent to var dim %s' % (self.var.dim, y.shape)\n",
    "        n, m = self.dim = x.shape\n",
    "        p, q = self.var.val.shape\n",
    "        print('processing data')\n",
    "        _x, _y = self.process(x, y, self.data_m)\n",
    "        print('output:', _x[:,0], _y[:,0])\n",
    "        dv_hist = []\n",
    "        lr= self.lr\n",
    "\n",
    "        for i in range(self.iter):\n",
    "            # update policy\n",
    "            lr = self.lrPolicy(i, lr)\n",
    "\n",
    "            pred = Regression.pred(self.var.val, _x)\n",
    "            DEBUG('pred',pred[:,0])\n",
    "\n",
    "            res = self.lossDiff(pred,y)\n",
    "            DEBUG('res',res[0,:], '\\ntv', np.dot(res, _x.T)[0,:])\n",
    "\n",
    "            loss = self.computeLoss(res)\n",
    "            self.j_hist[i,0] = loss\n",
    "\n",
    "            if (loss < self.err_th):\n",
    "                return self.var.val\n",
    "            \n",
    "            # Gradient\n",
    "            tV = self.computeGradient(res, _x, m)\n",
    "            dV = self.computeSolver(lr, tV)\n",
    "\n",
    "            dv_hist.append(dV[0,0])\n",
    "\n",
    "            # Update variable\n",
    "            self.updateWeight(dV * self.var.train_mask)\n",
    "            DEBUG('dv',dV[0,:],'val',self.var.val[0,:])\n",
    "            DEBUGLOSS('==============',i,'============', 'loss',loss, lr)\n",
    "        print(\"iter Done! Final loss:\", loss)\n",
    "        return dv_hist\n",
    "            #self.var.update(dV, self.ir, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1305,
   "metadata": {},
   "outputs": [],
   "source": [
    "var  = Variables(dim=(N,N))\n",
    "# var.val[:,-1] = 200\n",
    "var.val[-1,-1] = 1.0\n",
    "mask = np.ones_like(t_m)\n",
    "mask[-1,:] = 1\n",
    "mask[N-1,N-1] = 0\n",
    "var.set_trainMask(mask)\n",
    "reg = Regression(var=var,iter=1000, update_steps=400, data_m=0)\n",
    "reg.err_th = 1e-10\n",
    "#reg.lr = 0.000002\n",
    "reg.lr = np.ones((N,N)) * 0.0000035\n",
    "reg.lr[:,2] = 0.275\n",
    "reg.norm = 0\n",
    "reg.loss_weights = np.array([[1,1,1]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1306,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean = np.mean(data_x, axis=1, keepdims=1)\n",
    "x_mean[2,0] = 0\n",
    "_data_x = (data_x) / np.array([[100,100,100]]).T\n",
    "_data_y = (data_y) / np.array([[100,100,100]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data\n",
      "output: [825.11177597 939.91483525   1.        ] [1.16565676e+03 8.56269585e+02 8.42473307e-01]\n",
      "iter Done! Final loss: 6.235712530023916e-09\n"
     ]
    }
   ],
   "source": [
    "dv_hist = reg.fit(data_x, data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.92272362e-01,  8.22117652e-02,  4.34671442e+02],\n",
       "       [-1.33501977e-01,  9.58413827e-01,  6.55962640e+01],\n",
       "       [-2.09248580e-04,  1.60937718e-05,  1.00000000e+00]])"
      ]
     },
     "execution_count": 1308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.92270906e-01,  8.22135679e-02,  4.34670934e+02],\n",
       "       [-1.33501667e-01,  9.58413443e-01,  6.55963723e+01],\n",
       "       [-2.09248580e-04,  1.60937718e-05,  1.00000000e+00]])"
      ]
     },
     "execution_count": 1309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.var.val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.16565676e+03 8.56269585e+02 8.42473307e-01] [[1.16565674e+03]\n",
      " [8.56269588e+02]\n",
      " [8.42473307e-01]]\n",
      "[511.97207291 245.55731181   0.98708832] [[511.97181023]\n",
      " [245.55736778]\n",
      " [  0.98708832]]\n",
      "[616.31314226 414.30534833   0.9667761 ] [[616.31306226]\n",
      " [414.30536537]\n",
      " [  0.9667761 ]]\n",
      "[552.32155338 532.52051963   0.98785457] [[552.32180688]\n",
      " [532.52046562]\n",
      " [  0.98785457]]\n"
     ]
    }
   ],
   "source": [
    "for idx in range(4):\n",
    "    test_x = data_x[:,idx]\n",
    "    test_y = data_y[:,idx]\n",
    "    pred_y = np.dot(reg.var.val,data_x[:,idx:idx+1])\n",
    "    print (test_y, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.24692776e+03, 8.72551513e+02, 8.22174312e-01]),\n",
       " array([[836.42794144],\n",
       "        [270.5636486 ],\n",
       "        [  0.89838256]]))"
      ]
     },
     "execution_count": 1176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y, pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1da2de69710>]"
      ]
     },
     "execution_count": 1177,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEACAYAAAC6d6FnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl01fWd//HnOwk3QICwL9lYZDOyCRjRVovVKlQ2pWOhrd0Yqa22/Z3pdKrz63Tq/KbVds50Wk9tHVopVSvWqtVAUdpqKS6oBBQBWQzIEoKEsO/Z3r8/EtqUBrLc3Pv93ntfj3NyPPeT7/Lmc2Je+XyXz8fcHRERST1pQRcgIiLBUACIiKQoBYCISIpSAIiIpCgFgIhIilIAiIikKAWAiEiKUgCIiKQoBYCISIpSAIiIpKiMoAu4kN69e/ugQYOCLkNEJKGsWbOm0t37NLddqANg0KBBlJSUBF2GiEhCMbOdLdlOl4BERFJUKAPAzKab2YIjR44EXYqISNIKZQC4+xJ3n5+dnR10KSIiSSuUASAiIrGnABARSVFxCwAzm2xmL5nZg2Y2OV7nFRGRpkUVAGa20MwqzGzDOe1TzGyLmZWa2V0NzQ4cBzoCZdGcV0REohftCGARMKVxg5mlAw8AU4FCYK6ZFQIvuftU4BvAPVGeV0REohRVALj7SuDgOc1FQKm7b3f3KuBxYKa71zV8/xCQGc15RUQkerF4EzgX2N3ocxlwuZndDNwAdAd+fL6dzWw+MB+goKAgBuWJiAjEJgCsiTZ396eBp5vb2d0XmNleYHokEpnQ7tWJiAgQm6eAyoD8Rp/zgPIYnEdERKIQiwBYDQwzs8FmFgHmAMUxOI+IiEQh2sdAFwOrgBFmVmZm89y9BrgTWA5sAp5w942tOa6mghARib2o7gG4+9zztC8DlrX1uGY2HZg+dOjQth5CRESaEcqpIDQCEBGJvVAGgKaDFhGJvVAGgEYAIiKxF8oA0AhARCT2QhkAGgGIiMReKANARERiLxZTQUTt7GOgnQcMZfJ//alNx8jr0ZlpYwYwddQAsjt3aN8CRUSSgLl70DWcV78hhT7nu79q9X7u8HbZYXYcOEmHdONDw/swfWwOHynsR+dIKDNPRKTdmNkad5/Y3Hah/m2Y37MzP5pzaZv2dXfW7zlC8VvlLH17L3/cVEGnDulcV9iPGWNzuHp4bzIz0tu5YhGRxBHqEcDEiRO9pKQk6uPU1TmrdxykeF05y9bv5dDJarp1zGDqqAHMGJfDpCG9SE9rahJTEZHE09IRQEoEQGPVtXW8XFrJkrfKWb7xfU5U1dKnayY3jq4Pg0vzu2OmMBCRxJXQAdBoLqDb3n333Zid53R1LS9urqD4rXJe3FJBVU0d+T07MX1MDjPG5TCyf7eYnVtEJFYSOgDOisUI4HyOnq7m9xv3UbyunFdKK6mtc4b368KMsTlMH5vDwF5ZcalDRCRaCoAoVB4/w3Pr91K8rpzVOw4BMDa/OzPG5jBtzAD6desY95pERFoqlAFgZlnASuDf3X1pc9sHFQCN7Tl8iqXryileV87G8qOYweWDezJzXC5TR/Wne+dIoPWJiJwrLgFgZguBaUCFu49q1D4F+BGQDvzc3e9raP8P4ASwMVECoLHSiuMsWVfOknXlbK88QUaacfXwPswYm8P1l+gdAxEJh3gFwNXAceDhswFgZunAVuAj1K8PvBqYC+QAvYGOQGUiBsBZ7s7G8qMUN4TB3iOn6doxg3+YkM+nJhUwpE+XoEsUkRQWt0tAZjYIWNooAK4Avu3uNzR8vrth0y5AFlAInAJucve6Cx07rAHQWF2d88aOg/zq9V08v2Ev1bXOVcN6c+ukgXx4ZF8y0jXdkojEV5BvAucCuxt9LgMud/c7Gwr7LPUjgCZ/+ZvZfGA+QEFBQQzKa19pacakIb2YNKQXFccu5tdv7OaxN3Yx/5E15GR35JOTBnLLxHz6dM0MulQRkb8RiwBo6i2qvwwz3H3RhXZ29wVmtheYHolEJrRzbTHVt2tHvnztML44+SL+uKmCR1/byX8t38IP/7iVqaMG8OkrBjJhYA+9aCYioRCLACgD8ht9zgPKY3Ce0MpIT2PKqP5MGdWfbfuP8+hrO3lyTRnF68q5eEA3bp00kFmX5uimsYgEKhb3ADKovwl8LbCH+pvAn3D3ja09diLcA2ipk1U1PPtWOQ+v2smmvUfpmpnB7Al53HrFQC7STWMRaUfxegpoMTCZ+qd79lH/fP9DZvZR4IfUPwa60N2/08rjxmUqiCC4O2t3HeLhVTtZtr7+pvEHhvbiE0UDufbivnTsoBlKRSQ6oXwRrLWSaQTQlMrjZ/j16t089vou9hw+RZfMDG64pD+zLs3hiiG99ASRiLRJQgdAMo8AmlJb57y2/QDPvrWH59a/z7EzNfTuksn0sQOYOS6XsXnZunEsIi2W0AFwVrKPAJpyurqWFVsqeObNcl7cXEFVbR2DenVmxrhcZo7L0f0CEWlWQgdAqo0AzufIqWqWb3ifZ9ft4dVtB3CH0bnZzByXw7UX92NQr84aGYjI30noADgrFUcA57Pv6GmWNExK93bZEQDye3biQ8P7cPWwPlw5tDddMvVYqYgkeABoBHBhuw6c5M9bK/jz1kpe3VbJyapaMtKMCQN78KER9YFQOKAbaVrmUiQlJXQAnKURQPOqaupYs/MQf966n5Vb9/PO3qMA9O4S+csUFZOG9OKiPlm6XCSSIhQAKari2Gle2lrJS+/uZ9X2A+w7egaAPl0zG8KgJ5OG9GJIbwWCSLJK6ADQJaD24e7sPHCS17Yf4LXtBxQIIikioQPgLI0A2pe7s6NxIGw7QMWx+kDo2xAIV17Ui6uG9yG3e6eAqxWRtgpyOmgJKTNjcO8sBvfOYm5RQZOBULyuft6+i/pkcdWwPnxoeB8uH9JTE9eJJCGNAOQv3J3SiuP8eet+Xnq3ktffO8Dp6joiGWl84KJeXFfYj2tH9qN/dsegSxWRC0joS0C6BxAOp6trKdlxiD9tqeCPm/ax88BJAMbkZXN9YT+mjclhUO+sgKsUkXMldACcpRFAeJwdHfxh0z7+8M4+3tx1GIBRud2YNiaHG0cPIL9n54CrFBFQAEiMlR8+xbL1e1ny9l7W7a4Pg0lDejJ7fB4fHT2ALL2VLBIYBYDEze6DJyleV86Ta8p4r/IEnSPpTBszgE9fMYhRudlBlyeSckIXAGZ2MfBV6hePecHdf9rcPgqAxHJ2sZvflJTx7FvlnKquZeLAHnzmykFMGdWfDlrfQCQu4rUi2EJgGlBxdknIhvYpwI+oXxHs5+5+X6PvpQE/c/d5zR1fAZC4jpyq5jclu3l41U52HTxJv26ZfPLygcwtKqBP18ygyxNJavEKgKuB48DDjdYETqd+TeCPUL9A/Gpgrru/Y2YzgLuAH7v7Y80dXwGQ+OrqnBVbK/jFKzt46d1KIulpzJ6Qy5cmD9VNY5EYicuLYO6+smFR+MaKgFJ3395QyOPATOAddy8Gis3sd0CzASCJLy3N+PDIfnx4ZD+27T/OL155jydWl/GbkjI+NiGPO65REIgEJRaPauQCuxt9LgMuN7PJwM1AJrDsfDub2XxgPkBBQUEMypOgXNSnC/85azR3XDOUB1dsY/Hq3Ty5pozZ4+uDoKCXgkAknmIRAE3NKObuvgJY0dzO7r7AzPYC0yORyIR2rk1CYEB2J+6ZOYovTh7Kg3/exmNv7OKptWV8atJA/s91w+jeORJ0iSIpIRaPZZQB+Y0+5wHlMTiPJLj+2R359oxLeOlfruGWy/J5eNUOPvRfK/jFK+9RXVsXdHkiSS8WAbAaGGZmg80sAswBimNwHkkS/bp15Ls3jWbZV69idG429yx5hxt+uJIXN+8LujSRpBZVAJjZYmAVMMLMysxsnrvXAHcCy4FNwBPuvrE1x3X3Je4+PztbLxGlkpH9u/HIvCIe+sxEcPj8ohK+8EgJ+46eDro0kaQUyjeBNRmcVNXU8dDL7/HDP24lkp7G3R+9mDmX5WudY5EWCN2bwG2h9wBkR+UJ7n56Pau2H6BoUE/umz2aIX26BF2WSKi1NABC+W6+mU03swVHjhwJuhQJ2KDeWTx22+V8f/YYNr9/lBvvf5nFb+wizH+4iCQKjQAkYew7epqvPbGOl0srub6wH/fNHkPPLD0yKnIujQAk6fTr1pGHP1/EN2+8mBVb9jPlhyt5pbQy6LJEElYoA0BPAcn5pKUZ/3jVEH57x5V069SBWx96nZ+sKNUlIZE2CGUAiDTnkpxsnr3jA9w4JofvP7+FLzyyhqOnq4MuSyShhDIAdAlIWiIrM4P754zjW9MKeXFzBTN//AqlFceCLkskYYQyAHQJSFrKzPj8Bwfz2G2TOHa6hpt+8qruC4i0UCgDQKS1igb35Jk7rmRAdkc+s/ANfr16V9AliYSeAkCSRl6Pzjz5xSu54qJefOOp9Xz/+c26OSxyAQoASSrdOnbgF5+9jLlF+fxkxTb+9bfrqa1TCIg0JRbrAUSt0VxAQZciCSgjPY3v3jSanlkRHvjTNo6cquZ/Pj6OzIz0oEsTCZVQjgB0E1iiZWZ8/YaRfPPGi1m2/n1ue3gNp6trgy5LJFRCGQAi7eUfrxrCfTePZuXW/dz+6BrO1CgERM6KWwCY2Swz+5mZPWtm18frvCJzigq49+bRrNiyny89upaqGq02JgLRLwiz0MwqzGzDOe1TzGyLmZWa2V0A7v6Mu98GfBb4eDTnFWmtuUUF/OesUbywuYI7HlMIiED0I4BFwJTGDWaWDjwATAUKgblmVthok282fF8krj41aSD/MfMS/vDOPr6y+E1qtO6wpLioAsDdVwIHz2kuAkrdfbu7VwGPAzOt3veA59x9bTTnFWmrT18xiG9NK+T5je/zreKNek9AUlosHgPNBXY3+lwGXA58GbgOyDazoe7+YFM7m9l8YD5AQUFBDMqTVPf5Dw6m8vgZfrJiGwO6deTL1w4LuiSRQMQiAJpatNXd/X7g/uZ2dvcFZrYXmB6JRCa0e3UiwNdvGMH7R0/z33/YSr/sjtwyMT/okkTiLhZPAZUBjf9vygPKW3MAvQcgsWZmfG/2GK4a1pu7n17PnzZXBF2SSNzFIgBWA8PMbLCZRYA5QHFrDqDpoCUeOqSn8dNPTeDiAV2587G1bN2nqaQltUT7GOhiYBUwwszKzGyeu9cAdwLLgU3AE+6+MfpSRdpfl8wMfv7py+icmcFtD5dw+GRV0CWJxI0WhRcB1uw8xNwFr1E0uCeLPncZGel6SV4SlxaFF2mFCQN78J83jeLl0kq+u2xz0OWIxEUoA0A3gSUIt0zM53MfGMTCV97jmTf3BF2OSMyFMgA0ApCg/N+PXkzRoJ7862/Xs23/8aDLEYmpUAaARgASlIz0NH40dxwdO6Rzx6/WagppSWqhDACNACRIA7I78YNbxrL5/WPcs0QPsEnyCmUAaAQgQZs8oi9fmnwRi9/YzbNv6X6AJKdQBoBIGPzTR4YzYWAP/u2ZDZQfPhV0OSLtLpQBoEtAEgYZ6Wn84Jax1NQ5X39yHXVaXF6STCgDQJeAJCwG9sri36YV8krpARa9uiPockTaVSgDQCRM5lyWz7Uj+/K95zfzruYLkiSiABBphplx3+wxdI6k8y9PvU2tLgVJkghlAOgegIRNn66ZfGt6IW/uOsyjr+0MuhyRdhHKANA9AAmjWeNyuXp4H77//GY9FSRJIZQBIBJGZsZ3Zo2izuHfntmg9YQl4SkARFohv2dnvnb9cF7YXMHv1u8NuhyRqMQtAMxsiJk9ZGZPxuucIrHwuQ8MZkxeNt8ufocjJ6uDLkekzaJdEWyhmVWY2YZz2qeY2RYzKzWzuwDcfbu7z4vmfCJhkJ5m3HvzaA6eOMP//HFr0OWItFm0I4BFwJTGDWaWDjwATAUKgblmVhjleURC5ZKcbD41aSAPr9rBO+VHgy5HpE2iCgB3XwkcPKe5CCht+Iu/CngcmBnNeUTC6GsfGUH3zhH+vVg3hCUxxeIeQC6wu9HnMiDXzHqZ2YPApWZ29/l2NrP5ZlZiZiX79++PQXki7SO7cwe+MWUEq3cc4hnNGCoJKBYBYE20ubsfcPfb3f0id7/3fDu7+wLgHmBtJBKJQXki7ecfJuQzNr873122mWOndUNYEkssAqAMyG/0OQ8oj8F5RAKXlmb8x4xLqDx+hvtfeDfockRaJRYBsBoYZmaDzSwCzAGKY3AekVAYm9+dOZfl84tXdmiyOEko0T4GuhhYBYwwszIzm+fuNcCdwHJgE/CEu7dqXT1NBSGJ5us3jCQrM4N/L96oG8KSMKJ9Cmiuuw9w9w7unufuDzW0L3P34Q3X+7/T2uNqMjhJND2zIvzzDSN4ddsBvSEsCSOUU0FoBCCJ6BNFBRQO6MZ3freJE2dqgi5HpFmhDACNACQRpacZ/2/WJew9cpof/6k06HJEmhXKANAIQBLVhIE9mT0+j5+/tJ3t+48HXY7IBYUyADQCkET2jakj6JiRzreXvBN0KSIXFMoA0AhAElnfrh356nXDWLl1Py+9q7fZJbxCGQAiie7WKwaS270T9z23mTqtISwhpQAQiYHMjHS+dv1wNpYfZakeC5WQCmUA6B6AJIOZ43IZ2b8r//37LVTV1AVdjsjfCWUA6B6AJIP0NOMbU0ay88BJfr16V9DliPydUAaASLKYPKIPRYN78qMXSvVymISOAkAkhsyMu6aOpPL4GRa9uiPockT+RigDQPcAJJmML+jBNSP68POXtmsUIKESygDQPQBJNl+5dhiHTlbz6Gs7gy5F5C9CGQAiyebSgh5cPbwPC1Zu52SVRgESDnELADPLMrNfmtnPzOyT8TqvSFh89dqhHDhRxWOv64kgCYdoF4RZaGYVZrbhnPYpZrbFzErN7K6G5puBJ939NmBGNOcVSUQTBvbkg0N78+Cft3O6ujbockSiHgEsAqY0bjCzdOABYCpQCMw1s0Lq1wbe3bCZfvolJX3l2mFUHj+jUYCEQrQrgq0EDp7TXASUuvt2d68CHgdmUr9YfF57nFckURUN7knR4J489PJ7VNfq7WAJVix+Eefy17/0of4Xfy7wNDDbzH4KLDnfzmY238xKzKxk/37NpCjJ5wtXD2HP4VP87m3NESTByojBMa2JNnf3E8DnmtvZ3ReY2V5geiQSmdDu1YkE7JoRfRnatwv/u3I7M8flYNbU/zIisReLEUAZkN/ocx5Q3poD6D0ASWZpacb8q4awae9RXi6tDLocSWGxCIDVwDAzG2xmEWAOUNyaA+hNYEl2My/NoW/XTBas3B50KZLCon0MdDGwChhhZmVmNs/da4A7geXAJuAJd98YfakiySMzI53PfWAwL71bycZy/aEjwTD38K5WNHHiRC8pKQm6DJGYOHKqmivvfYEbRvXnB7eMC7ocSSJmtsbdJza3XSgfx9QlIEkF2Z06MHtCHkvf3suB42eCLkdSUCgDQDeBJVV8+oqBVNXU8fjq3c1vLNLOQhkAGgFIqhjatysfHNqbR1/bSY1eDJM4C2UAaAQgqeQzVw5i75HT/OGdfUGXIikmlAGgEYCkkg+P7Etej05aMUziLpQBoBGApJL0NOPWSQN5/b2DbH7/aNDlSAoJZQCIpJpbJuaTmZHGL1/VimESP6EMAF0CklTTIyvCjLE5PPvWHo5r3WCJk1AGgC4BSSqaU1TAyapalq5r1dRZIm0WygAQSUXjC7ozvF8XFuudAIkTBYBISJgZH7+sgHW7D7Npr24GS+yFMgB0D0BS1c2X5hJJT+PXGgVIHIQyAHQPQFJVj6wIU0b15+m1ZVo4XmIulAEgksrmXJbP0dM1PL/h/aBLkSSnABAJmUlDejGwV2ceX70r6FIkycUtAMxsiJk9ZGZPxuucIokoLc342Pg8Xtt+kLJDJ4MuR5JYiwLAzBaaWYWZbTinfYqZbTGzUjO760LHcPft7j4vmmJFUsWsS3MBePYtvRMgsdPSEcAiYErjBjNLBx4ApgKFwFwzKzSz0Wa29Jyvvu1atUiSy+/ZmaLBPXlqbRlhXrVPEluLAsDdVwIHz2kuAkob/rKvAh4HZrr7enefds5XRTvXLZL0Zo/PZfv+E6wr0+PQEhvR3APIBRo/rFzW0NYkM+tlZg8Cl5rZ3RfYbr6ZlZhZyf79+6MoTySxTR09gMyMNH67tizoUiRJRRMA1kTbeceq7n7A3W9394vc/d4LbLcAuAdYG4lEoihPJLF169iBjxT2o3hdOVU1Wi1M2l80AVAG5Df6nAfojpVIO5o9Po9DJ6tZsUVXUaX9RRMAq4FhZjbYzCLAHKC4PYrSm8Ai9a4a1pveXSI8vXZP0KVIEmrpY6CLgVXACDMrM7N57l4D3AksBzYBT7j7xvYoSnMBidTLSE9jxthcXtxcwZFT1UGXI0mmpU8BzXX3Ae7ewd3z3P2hhvZl7j684br+d2JbqkhqmjEuh6raOn6/UVNDSPsK5VQQugQk8ldj87LJ79mJJW/vDboUSTKhDABdAhL5KzNj2pgcXimt5OCJqqDLkSQSygDQCEDkb00bM4DaOue5DRoFSPsJZQBoBCDytwoHdGNInyyWrlMASPsJZQBoBCDyt85eBnrtvQNUHD0ddDmSJEIZACLy96aPGYA7LFuvUYC0DwWASIIY1q8rI/t31dNA0m5CGQC6ByDStGljBrBm5yH2HD4VdCmSBEIZALoHINK0aWNyAFi6TtNuSfRCGQAi0rRBvbMYm9+dZ7RSmLQDBYBIgpk1LodNe4+ydd+xoEuRBBfKANA9AJHzmzYmh/Q045k3NUOoRCeUAaB7ACLn16drJh8c2ptn3yqnrk7rBUvbhTIAROTCbro0lz2HT/HGjnOX6hZpubgFgJnNMrOfmdmzZnZ9vM4rkoxuuKQ/WZF0nlqj9YKl7Vq6IMxCM6swsw3ntE8xsy1mVmpmd13oGO7+jLvfBnwW+HibKxYROkXS+ejoASxbv5dTVbVBlyMJqqUjgEXAlMYNZpYOPABMBQqBuWZWaGajzWzpOV99G+36zYb9RCQKsyfkcaKqluVaKEbaKKMlG7n7SjMbdE5zEVDq7tsBzOxxYKa73wtMO/cYZmbAfcBz7r42mqJFBIoG9SS3eyeeWlvGrEtzgy5HElA09wBygd2NPpc1tJ3Pl4HrgI+Z2e3n28jM5ptZiZmV7N+/P4ryRJJbWpoxe3wur5RW8v4RzRAqrRdNAFgTbed9Js3d73f3Ce5+u7s/eIHtFgD3AGsjkUgU5Ykkv5vG51Hn8MxbeidAWi+aACgD8ht9zgPa5f10vQcg0jKDe2cxYWAPnlpThrveCZDWiSYAVgPDzGywmUWAOUBxexSlN4FFWu7m8bm8W3Gc9Xv0/4u0TksfA10MrAJGmFmZmc1z9xrgTmA5sAl4wt03xq5UEWnKtNE5RDLSeHqtLgNJ61iYh40TJ070kpKSoMsQCb07frWWV7dV8vq/XkckQy/4pzozW+PuE5vbLpQ/KboEJNI6syfkcuhkNSu2VARdiiSQUAaAbgKLtM5Vw/rQu0uEp9ZqaghpuVAGgEYAIq3TIT2NmeNyeXFzBYdOVAVdjiSIUAaARgAirTd7fB7Vtc6St7VamLRMKANAIwCR1ivM6cbI/l35TYkuA0nLhDIANAIQaZtPXF7A+j1HeHPXoaBLkQQQygAQkba5eXweXTMzWPTqjqBLkQQQygDQJSCRtumSmcHHJubxu7f3su+oJoiTCwtlAOgSkEjbfeaKQdS686vXdwVdioRcKANARNpuUO8sPjyiL4+s2sGx09VBlyMhpgAQSUJfuXYYh05Ws/DlHUGXIiEWygDQPQCR6IzN7871hf34+UvbOXxSL4ZJ00IZALoHIBK9f7p+OMeravjv328NuhQJqVAGgIhEb2T/bnz2ykE88tpO3njvYNDlSAgpAESS2D9fP4L8np346uNvUnFMj4XK34pbAJjZxWb2oJk9aWZfjNd5RVJZVmYGD35qAodPVvP5Ras5cPxM0CVJiLR0RbCFZlZhZhvOaZ9iZlvMrNTM7rrQMdx9k7vfDtwCNLtQgYi0j0tysvnJJ8fz7r7j3PzTV1mzU5eDpF5LRwCLgCmNG8wsHXgAmAoUAnPNrNDMRpvZ0nO++jbsMwN4GXih3f4FItKsa0b25bHbLqem1pn901Xc+tDr/Hr1LrbtP051bV3Q5UlAWrwkpJkNApa6+6iGz1cA33b3Gxo+3w3g7ve24Fi/c/cbm9tOS0KKtK/jZ2r45as7eGTVTt5vmCoiPc3o360jXTtm0K1jB7Iy00lPM9LM6v+bZqSbkWZgZgH/C+IjDP/Ku6aOpG+3jm3at6VLQma06ej1coHdjT6XAZdfoKDJwM1AJrDsAtvNB+YDFBQURFGeiJyrS2YGd1wzlC9NvojSiuO8tfswOw+cpPzwKY6dqeHY6WoOnKiiptap8/qv2jqnzqG2Lrzrh7cnJxz/zjM1sR+ZRRMATYXkeXvO3VcAK5o7qLsvMLO9wPRIJDKhzdWJyHmZGcP6dWVYv65BlyIBiuYpoDIgv9HnPEBLEYmIJIhoAmA1MMzMBptZBJgDFLdHUXoTWEQk9lr6GOhiYBUwwszKzGyeu9cAdwLLgU3AE+6+sT2K0lxAIiKx16J7AO4+9zzty7jADV0REQmvUE4FoUtAIiKxF8oA0CUgEZHYC2UAaAQgIhJ7oQwAERGJvWheBIsZM5sOTAeOmlkFcO61oOwWtPUGKmNW5N9rqqZY7t/c9m39fkv6tqm2VO/v5rZpTX831a7+bt027d3fEN8+j7a/B7ZoK3cP9RewoC1tQEnQdcZy/+a2b+v31d9t3/5C27Smv8/Tv+rvAPs73n0ebX+39CsRLgEtiaItnqI9f2v3b277tn5f/d327S+0TWv6u6l29Xfrtkn1/m6RFs8GmmjMrMRbMBuetA/1d3ypv+MvGfs8EUYAbbUg6AJSjPo7vtTf8Zd0fZ60IwAREbmwZB4BiIjIBSgARERSlAJARCRFpUwAmFmWmf3SzH5mZp8Mup5kZ2ZDzOwhM3sy6FpSgZnNavjZftbMrg+6nmRGN+NBAAAB80lEQVRnZheb2YNm9qSZfTHoetoqoQPAzBaaWYWZbTinfYqZbTGzUjO7q6H5ZuBJd78NmBH3YpNAa/rb3be7+7xgKk0OrezvZxp+tj8LfDyAchNeK/t7k7vfDtwCJOyjoQkdAMAiYErjBjNLBx4ApgKFwFwzK6R+ycqzi9jXxrHGZLKIlve3RG8Rre/vbzZ8X1pvEa3obzObAbwMvBDfMttPQgeAu68EDp7TXASUNvwFWgU8Dsykfg3jvIZtEvrfHZRW9rdEqTX9bfW+Bzzn7mvjXWsyaO3Pt7sXu/uVQMJeUk7GX4S5/PUvfaj/xZ8LPA3MNrOfEvxr3smkyf42s15m9iBwqZndHUxpSel8P99fBq4DPmZmtwdRWJI638/3ZDO738z+lwReFTGUs4FGyZpoc3c/AXwu3sWkgPP19wFAv4ja3/n6+37g/ngXkwLO198rgBXxLaX9JeMIoAzIb/Q5DygPqJZUoP6OL/V3fCV1fydjAKwGhpnZYDOLAHOA4oBrSmbq7/hSf8dXUvd3QgeAmS0GVgEjzKzMzOa5ew1wJ7Ac2AQ84e4bg6wzWai/40v9HV+p2N+aDE5EJEUl9AhARETaTgEgIpKiFAAiIilKASAikqIUACIiKUoBICKSohQAIiIpSgEgIpKiFAAiIinq/wOhbbW8Aa6ymwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.loglog(reg.j_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1da2c9b8dd8>]"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.loglog(np.array(dv_hist)*-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.27959871e+00,  1.27361709e+00,  1.60651565e-01],\n",
       "       [-5.99621510e-02,  1.13820246e+00,  2.65531557e-02],\n",
       "       [-2.09248580e-04,  1.60937718e-05,  1.00000000e+00]])"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.var.val "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = (data_x[:,0] - reg.x_mean) / reg.x_std\n",
    "test_y = (data_y[:,0] - reg.y_mean) / reg.y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (3,3) and (4,) not aligned: 3 (dim 1) != 4 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-346-54591fca12f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpred_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: shapes (3,3) and (4,) not aligned: 3 (dim 1) != 4 (dim 0)"
     ]
    }
   ],
   "source": [
    "pred_y = np.dot(reg.var.val, test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.57175111,  1.17644567, -0.80010062])"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.57144881,  1.17683568, -0.80574305])"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.82644675, 0.88120926, 0.01278923],\n",
       "       [0.65749945, 0.44678613, 0.23924547],\n",
       "       [0.07954453, 0.92108736, 1.        ]])"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (1,100) and (50,3) not aligned: 100 (dim 1) != 50 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-463-2596fdc4ae0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: shapes (1,100) and (50,3) not aligned: 100 (dim 1) != 50 (dim 0)"
     ]
    }
   ],
   "source": [
    "np.dot(x.T, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-202-ec12f28fd90b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mir\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.40377134, 0.51165326, 0.11446054],\n",
       "       [0.14409324, 0.02794739, 0.74226507],\n",
       "       [0.65643921, 0.66324733, 1.        ]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(self.iter):\n",
    "    pred = np.dot(x, self.var.val)\n",
    "    # Loss L2norm\n",
    "    res = pred - y\n",
    "    loss = 1/(2 * m) *np.sum(res * res) / (n)\n",
    "    self.j_hist[i,0] = loss\n",
    "    if (loss < self.err_th):\n",
    "        return self.var.val\n",
    "    # Gradient\n",
    "    dV = (self.ir / m) * np.dot(x.T, res)\n",
    "    DEBUG('==============',i,'============')\n",
    "    DEBUG('res:',res[0,...], 'pred:',pred[0,...], 'y:',y[0,...])\n",
    "    DEBUG('loss:',loss, 'dV:',dV[0,:], 'x:',x.T.shape, 'res:',res.shape)\n",
    "    # Update variable\n",
    "    if self.var.trainable:\n",
    "        self.var.val -= (dV * self.var.train_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, params):\n",
    "    n_samples = len(y)\n",
    "    h = X @ params\n",
    "    return (1/(2*n_samples))*np.sum((h-y)**2)\n",
    "def gradient_descent(X, y, params, learning_rate, n_iters):\n",
    "    n_samples = len(y)\n",
    "    J_history = np.zeros((n_iters,1))\n",
    "\n",
    "    for i in range(n_iters):\n",
    "        params = params - (learning_rate/n_samples) * X.T @ (X @ params - y) \n",
    "        J_history[i] = compute_cost(X, y, params)\n",
    "\n",
    "    return (J_history, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples in our dataset is: 506\n"
     ]
    }
   ],
   "source": [
    "dataset = load_boston()\n",
    "\n",
    "X = dataset.data\n",
    "y = dataset.target[:,np.newaxis]\n",
    "\n",
    "print(\"Total samples in our dataset is: {}\".format(X.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = len(y)\n",
    "\n",
    "mu = np.mean(X, 0)\n",
    "sigma = np.std(X, 0)\n",
    "\n",
    "X = (X-mu) / sigma\n",
    "\n",
    "X = np.hstack((np.ones((n_samples,1)),X))\n",
    "n_features = np.size(X,1)\n",
    "params = np.zeros((n_features,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial cost is:  296.0734584980237 \n",
      "\n",
      "Optimal parameters are: \n",
      " [[22.53279993]\n",
      " [-0.83980839]\n",
      " [ 0.92612237]\n",
      " [-0.17541988]\n",
      " [ 0.72676226]\n",
      " [-1.82369448]\n",
      " [ 2.78447498]\n",
      " [-0.05650494]\n",
      " [-2.96695543]\n",
      " [ 1.80785186]\n",
      " [-1.1802415 ]\n",
      " [-1.99990382]\n",
      " [ 0.85595908]\n",
      " [-3.69524414]] \n",
      "\n",
      "Final cost is:  [11.00713381]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_iters = 1500\n",
    "learning_rate = 0.01\n",
    "\n",
    "initial_cost = compute_cost(X, y, params)\n",
    "\n",
    "print(\"Initial cost is: \", initial_cost, \"\\n\")\n",
    "\n",
    "(J_history, optimal_params) = gradient_descent(X, y, params, learning_rate, n_iters)\n",
    "\n",
    "print(\"Optimal parameters are: \\n\", optimal_params, \"\\n\")\n",
    "\n",
    "print(\"Final cost is: \", J_history[-1])\n",
    "\n",
    "plt.plot(range(len(J_history)), J_history, 'r')\n",
    "\n",
    "plt.title(\"Convergence Graph of Cost Function\")\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression():\n",
    "    def __init__(self, X, y, alpha=0.03, n_iter=1500):\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.n_iter = n_iter\n",
    "        self.n_samples = len(y)\n",
    "        self.n_features = np.size(X, 1)\n",
    "        self.X = np.hstack((np.ones(\n",
    "            (self.n_samples, 1)), (X - np.mean(X, 0)) / np.std(X, 0)))\n",
    "        self.y = y[:, np.newaxis]\n",
    "        self.params = np.zeros((self.n_features + 1, 1))\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "    def fit(self):\n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "            self.params = self.params - (self.alpha/self.n_samples) * \\\n",
    "            self.X.T @ (self.X @ self.params - self.y)\n",
    "\n",
    "        self.intercept_ = self.params[0]\n",
    "        self.coef_ = self.params[1:]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def score(self, X=None, y=None):\n",
    "\n",
    "        if X is None:\n",
    "            X = self.X\n",
    "        else:\n",
    "            n_samples = np.size(X, 0)\n",
    "            X = np.hstack((np.ones(\n",
    "                (n_samples, 1)), (X - np.mean(X, 0)) / np.std(X, 0)))\n",
    "\n",
    "        if y is None:\n",
    "            y = self.y\n",
    "        else:\n",
    "            y = y[:, np.newaxis]\n",
    "\n",
    "        y_pred = X @ self.params\n",
    "        score = 1 - (((y - y_pred)**2).sum() / ((y - y.mean())**2).sum())\n",
    "\n",
    "        return score\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_samples = np.size(X, 0)\n",
    "        y = np.hstack((np.ones((n_samples, 1)), (X-np.mean(X, 0)) \\\n",
    "                            / np.std(X, 0))) @ self.params\n",
    "        return y\n",
    "\n",
    "    def get_params(self):\n",
    "\n",
    "        return self.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 2.71\n",
    "b = -2\n",
    "N = 100\n",
    "x = np.linspace(0, 10, N)\n",
    "x = np.expand_dims(x,1)\n",
    "y = a *x + b + np.random.rand(N,1) / 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.LinearRegression at 0x191579044e0>"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression(x,y)\n",
    "lr.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variables(object):\n",
    "    \"\"\"\n",
    "    Variable that can used to train with mask\n",
    "    \"\"\"\n",
    "    __slots__ = ('train_mask','dim','trainable','val')\n",
    "    def __init__(self, val=None, dim=None, trainable=1):\n",
    "        self.dim = dim\n",
    "        if val is None:\n",
    "            self.val = np.random.rand(dim[0],dim[1])\n",
    "            #self.val = np.zeros((dim[0],dim[1]))\n",
    "        else:\n",
    "            assert val.shape == dim, \"input val not consistent to dim input\"\n",
    "            self.val = val\n",
    "        self.train_mask = np.ones_like(self.val)\n",
    "        self.trainable = trainable\n",
    "    def set_trainMask(self, mask):\n",
    "        assert mask.shape == self.dim\n",
    "        self.train_mask = mask\n",
    "    def update(self, dv):\n",
    "        if self.trainable:\n",
    "            self.val -= (dv * self.train_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression(object):\n",
    "    \"\"\"\n",
    "    Note. Training gradient may explode if lr is too large.\n",
    "    \"\"\"\n",
    "    __slots__ = ('var', 'iter', 'lr', 'err_th', 'j_hist',\n",
    "                'x_mean', 'y_mean', 'x_std', 'y_std',\n",
    "                'decay_rate', 'loss_weights','norm')\n",
    "    def __init__(self, var=None, iter=500, decay_rate=0.9, norm=1):\n",
    "        self.var = var\n",
    "        self.iter = iter\n",
    "        self.lr = 0.001\n",
    "        self.err_th = 1e-4\n",
    "        self.j_hist = np.zeros((self.iter,1))\n",
    "        self.decay_rate=0.9\n",
    "        self.loss_weights=None\n",
    "        self.norm = norm\n",
    "\n",
    "    def process(self, x, y):\n",
    "        #self.x_mean = np.mean(x)\n",
    "        self.x_std = np.std(x)\n",
    "        #self.y_mean = np.mean(y)\n",
    "        #self.y_std = np.std(y)\n",
    "        #_x = (x.copy() - self.x_mean)/self.x_std\n",
    "        #_y = (y.copy() - self.y_mean)/self.y_std\n",
    "        _x = x.copy()/self.x_std\n",
    "        _y = y.copy()/self.x_std\n",
    "        return _x, _y\n",
    "        #return x.copy(), y.copy()\n",
    "    \n",
    "    def pred(x,val):\n",
    "        return np.dot(x, val)\n",
    "    \n",
    "    def compute_cost(x, val, y):\n",
    "        m, n = y.shape\n",
    "        pred = Regression.pred(x, val)\n",
    "        dloss = pred - y\n",
    "        cost = np.sum(dloss * dloss) / (2 * m * n)\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        assert len(x.shape) == 2, 'shape of x is not support'\n",
    "        assert self.var.dim[0] == x.shape[1], 'shape of x %s is not consistent to var dim %s' % (self.var.dim, x.shape)\n",
    "        assert self.var.dim[1] == y.shape[1], 'shape of y %s is not consistent to var dim %s' % (self.var.dim, y.shape)\n",
    "        m, n = x.shape\n",
    "        print('processing data')\n",
    "        _x, _y = self.process(x, y)\n",
    "        dv_hist = []\n",
    "        lr= self.lr\n",
    "        method = 'L2norm'\n",
    "        for i in range(self.iter):\n",
    "            if i % 200 == 0:\n",
    "                lr *= self.decay_rate\n",
    "            pred = np.dot(_x, self.var.val)\n",
    "            # Loss L2norm\n",
    "            if self.loss_weights is not None:\n",
    "                res = (pred - _y) * self.loss_weights\n",
    "            else:\n",
    "                res = (pred - _y)\n",
    "            if method == 'L2norm':\n",
    "                invH = np.linalg.inv(self.var.val)\n",
    "                reproj = np.dot(pred, invH)\n",
    "                res_re = (reproj - _x) * self.loss_weights\n",
    "                loss = np.sum(res * res) / (2 * m * n) + np.sum(res_re * res_re) / (2 * m * n)\n",
    "            elif method == 'LMS':\n",
    "                loss = np.sum(np.median(res * res, axis=0)) / (2 * n)\n",
    "            if self.norm > 0:\n",
    "                loss += (np.sum(self.var.val ** 2) - 1) / 2 * self.norm\n",
    "            self.j_hist[i,0] = loss\n",
    "            if (loss < self.err_th):\n",
    "                return self.var.val\n",
    "            # Gradient\n",
    "            if method == 'L2norm':\n",
    "                # https://math.stackexchange.com/questions/190424/how-to-evaluate-the-derivatives-of-matrix-inverse\n",
    "                tV = np.dot(_x.T, res) / m\n",
    "                dH = -np.dot(np.dot(invH, self.var.val.T), invH)\n",
    "                dR = np.dot(pred, dH)\n",
    "                tV += (np.dot(dR.T, res_re) / m)\n",
    "                if self.norm > 0:\n",
    "                    tV += self.var.val * self.norm \n",
    "                dV = lr * tV\n",
    "                dv_hist.append(dV[0,0])\n",
    "            elif method == 'LMS':\n",
    "                dV = (lr) # working on\n",
    "            DEBUG('==============',i,'============', 'loss',loss, lr)\n",
    "            # Update variable\n",
    "            if self.var.trainable:\n",
    "                self.var.val -= (dV * self.var.train_mask)\n",
    "        print(\"iter Done! Final loss:\", loss)\n",
    "        return dv_hist\n",
    "            #self.var.update(dV, self.ir, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = np.random.rand(N,N)\n",
    "var  = Variables(val=rand,dim=(N,N))\n",
    "#var  = Variables(dim=(N,N))\n",
    "var.val[N-1,N-1] = 1.0\n",
    "mask = np.ones_like(t_m)\n",
    "mask[-1,:] = 1\n",
    "mask[N-1,N-1] = 0\n",
    "var.set_trainMask(mask)\n",
    "reg = Regression(var=var,iter=1000)\n",
    "reg.err_th = 1e-10\n",
    "#reg.lr = 0.000001\n",
    "#reg.lr = np.ones((N,N)) * 0.01\n",
    "#reg.lr[-1,:] = 0.0001\n",
    "reg.lr = 0.1\n",
    "reg.norm = 1e-8\n",
    "reg.loss_weights = np.array([[1,1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data\n",
      "============== 0 ============ loss 116014.94453889747 0.09000000000000001\n",
      "============== 1 ============ loss 22144.163619434694 0.09000000000000001\n",
      "============== 2 ============ loss 6089.0714430236585 0.09000000000000001\n",
      "============== 3 ============ loss 3273.3673188490893 0.09000000000000001\n",
      "============== 4 ============ loss 2712.7321334867115 0.09000000000000001\n",
      "============== 5 ============ loss 2538.658798665922 0.09000000000000001\n",
      "============== 6 ============ loss 2433.211939650282 0.09000000000000001\n",
      "============== 7 ============ loss 2342.2069540665484 0.09000000000000001\n",
      "============== 8 ============ loss 2256.326148113136 0.09000000000000001\n",
      "============== 9 ============ loss 2173.887221394699 0.09000000000000001\n",
      "============== 10 ============ loss 2094.5101826155715 0.09000000000000001\n",
      "============== 11 ============ loss 2018.0399845150655 0.09000000000000001\n",
      "============== 12 ============ loss 1944.3631428361227 0.09000000000000001\n",
      "============== 13 ============ loss 1873.3764233692837 0.09000000000000001\n",
      "============== 14 ============ loss 1804.9813996276673 0.09000000000000001\n",
      "============== 15 ============ loss 1739.0834152587179 0.09000000000000001\n",
      "============== 16 ============ loss 1675.5912997681091 0.09000000000000001\n",
      "============== 17 ============ loss 1614.4172163136573 0.09000000000000001\n",
      "============== 18 ============ loss 1555.4765357550073 0.09000000000000001\n",
      "============== 19 ============ loss 1498.6877188222875 0.09000000000000001\n",
      "============== 20 ============ loss 1443.9722031854537 0.09000000000000001\n",
      "============== 21 ============ loss 1391.2542947484305 0.09000000000000001\n",
      "============== 22 ============ loss 1340.4610629293138 0.09000000000000001\n",
      "============== 23 ============ loss 1291.5222397667142 0.09000000000000001\n",
      "============== 24 ============ loss 1244.3701227101292 0.09000000000000001\n",
      "============== 25 ============ loss 1198.9394809594166 0.09000000000000001\n",
      "============== 26 ============ loss 1155.1674652237355 0.09000000000000001\n",
      "============== 27 ============ loss 1112.9935207750968 0.09000000000000001\n",
      "============== 28 ============ loss 1072.3593036762354 0.09000000000000001\n",
      "============== 29 ============ loss 1033.2086000669217 0.09000000000000001\n",
      "============== 30 ============ loss 995.487248397045 0.09000000000000001\n",
      "============== 31 ============ loss 959.1430644988874 0.09000000000000001\n",
      "============== 32 ============ loss 924.125769394926 0.09000000000000001\n",
      "============== 33 ============ loss 890.3869197413065 0.09000000000000001\n",
      "============== 34 ============ loss 857.8798408107499 0.09000000000000001\n",
      "============== 35 ============ loss 826.5595619221809 0.09000000000000001\n",
      "============== 36 ============ loss 796.3827542277628 0.09000000000000001\n",
      "============== 37 ============ loss 767.3076707712511 0.09000000000000001\n",
      "============== 38 ============ loss 739.2940887347701 0.09000000000000001\n",
      "============== 39 ============ loss 712.3032537940833 0.09000000000000001\n",
      "============== 40 ============ loss 686.2978265054107 0.09000000000000001\n",
      "============== 41 ============ loss 661.2418306495974 0.09000000000000001\n",
      "============== 42 ============ loss 637.1006034621923 0.09000000000000001\n",
      "============== 43 ============ loss 613.8407476805628 0.09000000000000001\n",
      "============== 44 ============ loss 591.4300853417348 0.09000000000000001\n",
      "============== 45 ============ loss 569.8376132670159 0.09000000000000001\n",
      "============== 46 ============ loss 549.0334601718328 0.09000000000000001\n",
      "============== 47 ============ loss 528.9888453414416 0.09000000000000001\n",
      "============== 48 ============ loss 509.6760388153498 0.09000000000000001\n",
      "============== 49 ============ loss 491.06832302535815 0.09000000000000001\n",
      "============== 50 ============ loss 473.1399558341659 0.09000000000000001\n",
      "============== 51 ============ loss 455.8661349233893 0.09000000000000001\n",
      "============== 52 ============ loss 439.22296348174433 0.09000000000000001\n",
      "============== 53 ============ loss 423.187417145916 0.09000000000000001\n",
      "============== 54 ============ loss 407.73731214837875 0.09000000000000001\n",
      "============== 55 ============ loss 392.85127462811346 0.09000000000000001\n",
      "============== 56 ============ loss 378.50871106175356 0.09000000000000001\n",
      "============== 57 ============ loss 364.6897797742647 0.09000000000000001\n",
      "============== 58 ============ loss 351.3753634897319 0.09000000000000001\n",
      "============== 59 ============ loss 338.5470428842992 0.09000000000000001\n",
      "============== 60 ============ loss 326.1870711046544 0.09000000000000001\n",
      "============== 61 ============ loss 314.27834921682563 0.09000000000000001\n",
      "============== 62 ============ loss 302.80440255131595 0.09000000000000001\n",
      "============== 63 ============ loss 291.7493579118429 0.09000000000000001\n",
      "============== 64 ============ loss 281.09792161617906 0.09000000000000001\n",
      "============== 65 ============ loss 270.835358338688 0.09000000000000001\n",
      "============== 66 ============ loss 260.9474707252987 0.09000000000000001\n",
      "============== 67 ============ loss 251.42057975272087 0.09000000000000001\n",
      "============== 68 ============ loss 242.24150580472082 0.09000000000000001\n",
      "============== 69 ============ loss 233.39755043928437 0.09000000000000001\n",
      "============== 70 ============ loss 224.87647882144307 0.09000000000000001\n",
      "============== 71 ============ loss 216.66650279745534 0.09000000000000001\n",
      "============== 72 ============ loss 208.75626458693972 0.09000000000000001\n",
      "============== 73 ============ loss 201.13482107038269 0.09000000000000001\n",
      "============== 74 ============ loss 193.79162865030077 0.09000000000000001\n",
      "============== 75 ============ loss 186.71652866510053 0.09000000000000001\n",
      "============== 76 ============ loss 179.89973333546664 0.09000000000000001\n",
      "============== 77 ============ loss 173.3318122238327 0.09000000000000001\n",
      "============== 78 ============ loss 167.00367918820064 0.09000000000000001\n",
      "============== 79 ============ loss 160.90657981226545 0.09000000000000001\n",
      "============== 80 ============ loss 155.03207929445145 0.09000000000000001\n",
      "============== 81 ============ loss 149.37205077910747 0.09000000000000001\n",
      "============== 82 ============ loss 143.91866411371862 0.09000000000000001\n",
      "============== 83 ============ loss 138.6643750165803 0.09000000000000001\n",
      "============== 84 ============ loss 133.60191463994994 0.09000000000000001\n",
      "============== 85 ============ loss 128.72427951423577 0.09000000000000001\n",
      "============== 86 ============ loss 124.02472185931485 0.09000000000000001\n",
      "============== 87 ============ loss 119.49674024957316 0.09000000000000001\n",
      "============== 88 ============ loss 115.13407061975626 0.09000000000000001\n",
      "============== 89 ============ loss 110.93067759918829 0.09000000000000001\n",
      "============== 90 ============ loss 106.88074616236626 0.09000000000000001\n",
      "============== 91 ============ loss 102.97867358438678 0.09000000000000001\n",
      "============== 92 ============ loss 99.21906169006853 0.09000000000000001\n",
      "============== 93 ============ loss 95.59670938605385 0.09000000000000001\n",
      "============== 94 ============ loss 92.10660546555252 0.09000000000000001\n",
      "============== 95 ============ loss 88.74392167577942 0.09000000000000001\n",
      "============== 96 ============ loss 85.50400603849478 0.09000000000000001\n",
      "============== 97 ============ loss 82.38237641439676 0.09000000000000001\n",
      "============== 98 ============ loss 79.37471430248189 0.09000000000000001\n",
      "============== 99 ============ loss 76.47685886577638 0.09000000000000001\n",
      "============== 100 ============ loss 73.68480117518523 0.09000000000000001\n",
      "============== 101 ============ loss 70.99467866349327 0.09000000000000001\n",
      "============== 102 ============ loss 68.40276978184176 0.09000000000000001\n",
      "============== 103 ============ loss 65.90548885129466 0.09000000000000001\n",
      "============== 104 ============ loss 63.49938110236697 0.09000000000000001\n",
      "============== 105 ============ loss 61.18111789565656 0.09000000000000001\n",
      "============== 106 ============ loss 58.94749211696272 0.09000000000000001\n",
      "============== 107 ============ loss 56.795413740528446 0.09000000000000001\n",
      "============== 108 ============ loss 54.72190555426027 0.09000000000000001\n",
      "============== 109 ============ loss 52.72409904101766 0.09000000000000001\n",
      "============== 110 ============ loss 50.79923041027409 0.09000000000000001\n",
      "============== 111 ============ loss 48.94463677465343 0.09000000000000001\n",
      "============== 112 ============ loss 47.15775246606087 0.09000000000000001\n",
      "============== 113 ============ loss 45.43610548630829 0.09000000000000001\n",
      "============== 114 ============ loss 43.77731408732027 0.09000000000000001\n",
      "============== 115 ============ loss 42.17908347619787 0.09000000000000001\n",
      "============== 116 ============ loss 40.639202640571035 0.09000000000000001\n",
      "============== 117 ============ loss 39.1555412898618 0.09000000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== 118 ============ loss 37.726046908211856 0.09000000000000001\n",
      "============== 119 ============ loss 36.34874191500853 0.09000000000000001\n",
      "============== 120 ============ loss 35.021720929076224 0.09000000000000001\n",
      "============== 121 ============ loss 33.74314813274945 0.09000000000000001\n",
      "============== 122 ============ loss 32.511254732179815 0.09000000000000001\n",
      "============== 123 ============ loss 31.32433651036591 0.09000000000000001\n",
      "============== 124 ============ loss 30.180751469519826 0.09000000000000001\n",
      "============== 125 ============ loss 29.078917559506188 0.09000000000000001\n",
      "============== 126 ============ loss 28.01731048921618 0.09000000000000001\n",
      "============== 127 ============ loss 26.99446161784433 0.09000000000000001\n",
      "============== 128 ============ loss 26.008955923153707 0.09000000000000001\n",
      "============== 129 ============ loss 25.05943004391651 0.09000000000000001\n",
      "============== 130 ============ loss 24.144570393824324 0.09000000000000001\n",
      "============== 131 ============ loss 23.26311134425659 0.09000000000000001\n",
      "============== 132 ============ loss 22.41383347339563 0.09000000000000001\n",
      "============== 133 ============ loss 21.595561879262103 0.09000000000000001\n",
      "============== 134 ============ loss 20.807164554343412 0.09000000000000001\n",
      "============== 135 ============ loss 20.047550819559067 0.09000000000000001\n",
      "============== 136 ============ loss 19.315669815403954 0.09000000000000001\n",
      "============== 137 ============ loss 18.61050904817505 0.09000000000000001\n",
      "============== 138 ============ loss 17.931092989276014 0.09000000000000001\n",
      "============== 139 ============ loss 17.276481725659778 0.09000000000000001\n",
      "============== 140 ============ loss 16.64576965954016 0.09000000000000001\n",
      "============== 141 ============ loss 16.038084255578305 0.09000000000000001\n",
      "============== 142 ============ loss 15.452584833805371 0.09000000000000001\n",
      "============== 143 ============ loss 14.888461406617003 0.09000000000000001\n",
      "============== 144 ============ loss 14.344933558226053 0.09000000000000001\n",
      "============== 145 ============ loss 13.821249365025917 0.09000000000000001\n",
      "============== 146 ============ loss 13.316684355370464 0.09000000000000001\n",
      "============== 147 ============ loss 12.83054050733046 0.09000000000000001\n",
      "============== 148 ============ loss 12.362145283043288 0.09000000000000001\n",
      "============== 149 ============ loss 11.910850698313991 0.09000000000000001\n",
      "============== 150 ============ loss 11.47603242618733 0.09000000000000001\n",
      "============== 151 ============ loss 11.057088933245534 0.09000000000000001\n",
      "============== 152 ============ loss 10.653440647440112 0.09000000000000001\n",
      "============== 153 ============ loss 10.264529156304752 0.09000000000000001\n",
      "============== 154 ============ loss 9.889816434440611 0.09000000000000001\n",
      "============== 155 ============ loss 9.528784099205565 0.09000000000000001\n",
      "============== 156 ============ loss 9.180932693576898 0.09000000000000001\n",
      "============== 157 ============ loss 8.845780995197268 0.09000000000000001\n",
      "============== 158 ============ loss 8.52286535064487 0.09000000000000001\n",
      "============== 159 ============ loss 8.211739034010032 0.09000000000000001\n",
      "============== 160 ============ loss 7.911971628889451 0.09000000000000001\n",
      "============== 161 ============ loss 7.623148432942056 0.09000000000000001\n",
      "============== 162 ============ loss 7.344869884185814 0.09000000000000001\n",
      "============== 163 ============ loss 7.076751008237832 0.09000000000000001\n",
      "============== 164 ============ loss 6.818420885736276 0.09000000000000001\n",
      "============== 165 ============ loss 6.569522139206429 0.09000000000000001\n",
      "============== 166 ============ loss 6.329710438659926 0.09000000000000001\n",
      "============== 167 ============ loss 6.098654025244436 0.09000000000000001\n",
      "============== 168 ============ loss 5.876033252284413 0.09000000000000001\n",
      "============== 169 ============ loss 5.661540143078112 0.09000000000000001\n",
      "============== 170 ============ loss 5.454877964838641 0.09000000000000001\n",
      "============== 171 ============ loss 5.255760818190266 0.09000000000000001\n",
      "============== 172 ============ loss 5.063913241651471 0.09000000000000001\n",
      "============== 173 ============ loss 4.879069830558498 0.09000000000000001\n",
      "============== 174 ============ loss 4.700974869900749 0.09000000000000001\n",
      "============== 175 ============ loss 4.529381980562338 0.09000000000000001\n",
      "============== 176 ============ loss 4.364053778477031 0.09000000000000001\n",
      "============== 177 ============ loss 4.2047615462292836 0.09000000000000001\n",
      "============== 178 ============ loss 4.051284916642901 0.09000000000000001\n",
      "============== 179 ============ loss 3.903411567922472 0.09000000000000001\n",
      "============== 180 ============ loss 3.7609369299256605 0.09000000000000001\n",
      "============== 181 ============ loss 3.623663901157061 0.09000000000000001\n",
      "============== 182 ============ loss 3.491402576096139 0.09000000000000001\n",
      "============== 183 ============ loss 3.363969982479753 0.09000000000000001\n",
      "============== 184 ============ loss 3.2411898281754286 0.09000000000000001\n",
      "============== 185 ============ loss 3.1228922572972557 0.09000000000000001\n",
      "============== 186 ============ loss 3.0089136152250417 0.09000000000000001\n",
      "============== 187 ============ loss 2.8990962222017926 0.09000000000000001\n",
      "============== 188 ============ loss 2.7932881551985886 0.09000000000000001\n",
      "============== 189 ============ loss 2.69134303774127 0.09000000000000001\n",
      "============== 190 ============ loss 2.59311983741242 0.09000000000000001\n",
      "============== 191 ============ loss 2.4984826707445023 0.09000000000000001\n",
      "============== 192 ============ loss 2.407300615237534 0.09000000000000001\n",
      "============== 193 ============ loss 2.319447528238905 0.09000000000000001\n",
      "============== 194 ============ loss 2.2348018724360816 0.09000000000000001\n",
      "============== 195 ============ loss 2.1532465477206677 0.09000000000000001\n",
      "============== 196 ============ loss 2.0746687291901478 0.09000000000000001\n",
      "============== 197 ============ loss 1.998959711064848 0.09000000000000001\n",
      "============== 198 ============ loss 1.9260147563027652 0.09000000000000001\n",
      "============== 199 ============ loss 1.8557329517051764 0.09000000000000001\n",
      "============== 200 ============ loss 1.7880170683118983 0.08100000000000002\n",
      "============== 201 ============ loss 1.7292431950559328 0.08100000000000002\n",
      "============== 202 ============ loss 1.672402317437407 0.08100000000000002\n",
      "============== 203 ============ loss 1.617430861827634 0.08100000000000002\n",
      "============== 204 ============ loss 1.5642673454480314 0.08100000000000002\n",
      "============== 205 ============ loss 1.512852307604811 0.08100000000000002\n",
      "============== 206 ============ loss 1.4631282431857369 0.08100000000000002\n",
      "============== 207 ============ loss 1.4150395383434926 0.08100000000000002\n",
      "============== 208 ============ loss 1.3685324082945753 0.08100000000000002\n",
      "============== 209 ============ loss 1.3235548371639219 0.08100000000000002\n",
      "============== 210 ============ loss 1.2800565198079303 0.08100000000000002\n",
      "============== 211 ============ loss 1.2379888055507937 0.08100000000000002\n",
      "============== 212 ============ loss 1.1973046437715078 0.08100000000000002\n",
      "============== 213 ============ loss 1.1579585312798528 0.08100000000000002\n",
      "============== 214 ============ loss 1.1199064614240983 0.08100000000000002\n",
      "============== 215 ============ loss 1.0831058748712725 0.08100000000000002\n",
      "============== 216 ============ loss 1.0475156120072984 0.08100000000000002\n",
      "============== 217 ============ loss 1.013095866901989 0.08100000000000002\n",
      "============== 218 ============ loss 0.9798081427880013 0.08100000000000002\n",
      "============== 219 ============ loss 0.9476152090047713 0.08100000000000002\n",
      "============== 220 ============ loss 0.9164810593575574 0.08100000000000002\n",
      "============== 221 ============ loss 0.8863708718465242 0.08100000000000002\n",
      "============== 222 ============ loss 0.857250969720411 0.08100000000000002\n",
      "============== 223 ============ loss 0.8290887838105048 0.08100000000000002\n",
      "============== 224 ============ loss 0.8018528161041013 0.08100000000000002\n",
      "============== 225 ============ loss 0.7755126045155386 0.08100000000000002\n",
      "============== 226 ============ loss 0.7500386888156998 0.08100000000000002\n",
      "============== 227 ============ loss 0.725402577682796 0.08100000000000002\n",
      "============== 228 ============ loss 0.7015767168360363 0.08100000000000002\n",
      "============== 229 ============ loss 0.6785344582176775 0.08100000000000002\n",
      "============== 230 ============ loss 0.656250030188485 0.08100000000000002\n",
      "============== 231 ============ loss 0.6346985087037554 0.08100000000000002\n",
      "============== 232 ============ loss 0.6138557894368265 0.08100000000000002\n",
      "============== 233 ============ loss 0.5936985608199541 0.08100000000000002\n",
      "============== 234 ============ loss 0.5742042779713031 0.08100000000000002\n",
      "============== 235 ============ loss 0.5553511374798216 0.08100000000000002\n",
      "============== 236 ============ loss 0.5371180530191962 0.08100000000000002\n",
      "============== 237 ============ loss 0.5194846317640327 0.08100000000000002\n",
      "============== 238 ============ loss 0.5024311515813994 0.08100000000000002\n",
      "============== 239 ============ loss 0.4859385389727951 0.08100000000000002\n",
      "============== 240 ============ loss 0.46998834774142023 0.08100000000000002\n",
      "============== 241 ============ loss 0.4545627383610631 0.08100000000000002\n",
      "============== 242 ============ loss 0.4396444580234718 0.08100000000000002\n",
      "============== 243 ============ loss 0.4252168213420772 0.08100000000000002\n",
      "============== 244 ============ loss 0.41126369169028415 0.08100000000000002\n",
      "============== 245 ============ loss 0.3977694631533721 0.08100000000000002\n",
      "============== 246 ============ loss 0.3847190430743051 0.08100000000000002\n",
      "============== 247 ============ loss 0.3720978351731698 0.08100000000000002\n",
      "============== 248 ============ loss 0.359891723222193 0.08100000000000002\n",
      "============== 249 ============ loss 0.3480870552572425 0.08100000000000002\n",
      "============== 250 ============ loss 0.33667062830922634 0.08100000000000002\n",
      "============== 251 ============ loss 0.32562967363681705 0.08100000000000002\n",
      "============== 252 ============ loss 0.31495184244551516 0.08100000000000002\n",
      "============== 253 ============ loss 0.3046251920761873 0.08100000000000002\n",
      "============== 254 ============ loss 0.29463817264776043 0.08100000000000002\n",
      "============== 255 ============ loss 0.28497961413923734 0.08100000000000002\n",
      "============== 256 ============ loss 0.27563871389677574 0.08100000000000002\n",
      "============== 257 ============ loss 0.26660502455131435 0.08100000000000002\n",
      "============== 258 ============ loss 0.25786844233377737 0.08100000000000002\n",
      "============== 259 ============ loss 0.249419195774689 0.08100000000000002\n",
      "============== 260 ============ loss 0.24124783477501105 0.08100000000000002\n",
      "============== 261 ============ loss 0.23334522003697927 0.08100000000000002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== 262 ============ loss 0.22570251284208398 0.08100000000000002\n",
      "============== 263 ============ loss 0.21831116516551327 0.08100000000000002\n",
      "============== 264 ============ loss 0.21116291011560792 0.08100000000000002\n",
      "============== 265 ============ loss 0.20424975268781856 0.08100000000000002\n",
      "============== 266 ============ loss 0.19756396082259015 0.08100000000000002\n",
      "============== 267 ============ loss 0.19109805675757918 0.08100000000000002\n",
      "============== 268 ============ loss 0.18484480866408198 0.08100000000000002\n",
      "============== 269 ============ loss 0.17879722255867905 0.08100000000000002\n",
      "============== 270 ============ loss 0.17294853448075748 0.08100000000000002\n",
      "============== 271 ============ loss 0.16729220292741193 0.08100000000000002\n",
      "============== 272 ============ loss 0.16182190153712836 0.08100000000000002\n",
      "============== 273 ============ loss 0.15653151201398585 0.08100000000000002\n",
      "============== 274 ============ loss 0.1514151172847922 0.08100000000000002\n",
      "============== 275 ============ loss 0.14646699488102122 0.08100000000000002\n",
      "============== 276 ============ loss 0.14168161053856554 0.08100000000000002\n",
      "============== 277 ============ loss 0.13705361200803223 0.08100000000000002\n",
      "============== 278 ============ loss 0.13257782306836918 0.08100000000000002\n",
      "============== 279 ============ loss 0.1282492377376388 0.08100000000000002\n",
      "============== 280 ============ loss 0.12406301467407499 0.08100000000000002\n",
      "============== 281 ============ loss 0.12001447176126205 0.08100000000000002\n",
      "============== 282 ============ loss 0.11609908087141498 0.08100000000000002\n",
      "============== 283 ============ loss 0.11231246280093381 0.08100000000000002\n",
      "============== 284 ============ loss 0.1086503823724518 0.08100000000000002\n",
      "============== 285 ============ loss 0.10510874369809627 0.08100000000000002\n",
      "============== 286 ============ loss 0.10168358559830726 0.08100000000000002\n",
      "============== 287 ============ loss 0.09837107717155247 0.08100000000000002\n",
      "============== 288 ============ loss 0.09516751350963104 0.08100000000000002\n",
      "============== 289 ============ loss 0.0920693115539828 0.08100000000000002\n",
      "============== 290 ============ loss 0.08907300608809021 0.08100000000000002\n",
      "============== 291 ============ loss 0.08617524586191144 0.08100000000000002\n",
      "============== 292 ============ loss 0.08337278984367438 0.08100000000000002\n",
      "============== 293 ============ loss 0.08066250359486667 0.08100000000000002\n",
      "============== 294 ============ loss 0.07804135576464412 0.08100000000000002\n",
      "============== 295 ============ loss 0.07550641469928845 0.08100000000000002\n",
      "============== 296 ============ loss 0.0730548451634103 0.08100000000000002\n",
      "============== 297 ============ loss 0.07068390516879455 0.08100000000000002\n",
      "============== 298 ============ loss 0.06839094290770062 0.08100000000000002\n",
      "============== 299 ============ loss 0.06617339378691076 0.08100000000000002\n",
      "============== 300 ============ loss 0.06402877755931655 0.08100000000000002\n",
      "============== 301 ============ loss 0.061954695549984756 0.08100000000000002\n",
      "============== 302 ============ loss 0.05994882797325431 0.08100000000000002\n",
      "============== 303 ============ loss 0.05800893133828568 0.08100000000000002\n",
      "============== 304 ============ loss 0.05613283593970282 0.08100000000000002\n",
      "============== 305 ============ loss 0.054318443430998294 0.08100000000000002\n",
      "============== 306 ============ loss 0.05256372447755798 0.08100000000000002\n",
      "============== 307 ============ loss 0.05086671648698785 0.08100000000000002\n",
      "============== 308 ============ loss 0.049225521414027015 0.08100000000000002\n",
      "============== 309 ============ loss 0.04763830363772385 0.08100000000000002\n",
      "============== 310 ============ loss 0.04610328790833111 0.08100000000000002\n",
      "============== 311 ============ loss 0.04461875736180273 0.08100000000000002\n",
      "============== 312 ============ loss 0.04318305159958563 0.08100000000000002\n",
      "============== 313 ============ loss 0.04179456483147723 0.08100000000000002\n",
      "============== 314 ============ loss 0.04045174407971483 0.08100000000000002\n",
      "============== 315 ============ loss 0.039153087441977596 0.08100000000000002\n",
      "============== 316 ============ loss 0.03789714241162483 0.08100000000000002\n",
      "============== 317 ============ loss 0.036682504253103834 0.08100000000000002\n",
      "============== 318 ============ loss 0.03550781443085845 0.08100000000000002\n",
      "============== 319 ============ loss 0.03437175908983685 0.08100000000000002\n",
      "============== 320 ============ loss 0.03327306758601787 0.08100000000000002\n",
      "============== 321 ============ loss 0.03221051106528094 0.08100000000000002\n",
      "============== 322 ============ loss 0.031182901088972013 0.08100000000000002\n",
      "============== 323 ============ loss 0.030189088304689526 0.08100000000000002\n",
      "============== 324 ============ loss 0.029227961160790066 0.08100000000000002\n",
      "============== 325 ============ loss 0.02829844466319366 0.08100000000000002\n",
      "============== 326 ============ loss 0.02739949917298645 0.08100000000000002\n",
      "============== 327 ============ loss 0.026530119243743464 0.08100000000000002\n",
      "============== 328 ============ loss 0.025689332496882537 0.08100000000000002\n",
      "============== 329 ============ loss 0.024876198534159025 0.08100000000000002\n",
      "============== 330 ============ loss 0.024089807885863658 0.08100000000000002\n",
      "============== 331 ============ loss 0.023329280993621408 0.08100000000000002\n",
      "============== 332 ============ loss 0.022593767226637165 0.08100000000000002\n",
      "============== 333 ============ loss 0.021882443930333496 0.08100000000000002\n",
      "============== 334 ============ loss 0.021194515506208336 0.08100000000000002\n",
      "============== 335 ============ loss 0.02052921252204072 0.08100000000000002\n",
      "============== 336 ============ loss 0.019885790851266436 0.08100000000000002\n",
      "============== 337 ============ loss 0.019263530840762932 0.08100000000000002\n",
      "============== 338 ============ loss 0.01866173650590285 0.08100000000000002\n",
      "============== 339 ============ loss 0.01807973475216438 0.08100000000000002\n",
      "============== 340 ============ loss 0.01751687462226595 0.08100000000000002\n",
      "============== 341 ============ loss 0.01697252656813764 0.08100000000000002\n",
      "============== 342 ============ loss 0.016446081746784857 0.08100000000000002\n",
      "============== 343 ============ loss 0.015936951339317464 0.08100000000000002\n",
      "============== 344 ============ loss 0.015444565892394525 0.08100000000000002\n",
      "============== 345 ============ loss 0.014968374681317152 0.08100000000000002\n",
      "============== 346 ============ loss 0.014507845094047822 0.08100000000000002\n",
      "============== 347 ============ loss 0.01406246203554939 0.08100000000000002\n",
      "============== 348 ============ loss 0.013631727351618191 0.08100000000000002\n",
      "============== 349 ============ loss 0.01321515927178174 0.08100000000000002\n",
      "============== 350 ============ loss 0.01281229187040185 0.08100000000000002\n",
      "============== 351 ============ loss 0.012422674545596412 0.08100000000000002\n",
      "============== 352 ============ loss 0.012045871515238742 0.08100000000000002\n",
      "============== 353 ============ loss 0.011681461329586557 0.08100000000000002\n",
      "============== 354 ============ loss 0.011329036399869163 0.08100000000000002\n",
      "============== 355 ============ loss 0.01098820254243503 0.08100000000000002\n",
      "============== 356 ============ loss 0.010658578537873905 0.08100000000000002\n",
      "============== 357 ============ loss 0.010339795704653605 0.08100000000000002\n",
      "============== 358 ============ loss 0.010031497486732165 0.08100000000000002\n",
      "============== 359 ============ loss 0.009733339054803447 0.08100000000000002\n",
      "============== 360 ============ loss 0.009444986920588434 0.08100000000000002\n",
      "============== 361 ============ loss 0.009166118563850523 0.08100000000000002\n",
      "============== 362 ============ loss 0.008896422071701502 0.08100000000000002\n",
      "============== 363 ============ loss 0.008635595789679934 0.08100000000000002\n",
      "============== 364 ============ loss 0.008383347984431598 0.08100000000000002\n",
      "============== 365 ============ loss 0.008139396517352277 0.08100000000000002\n",
      "============== 366 ============ loss 0.007903468529079562 0.08100000000000002\n",
      "============== 367 ============ loss 0.007675300134283359 0.08100000000000002\n",
      "============== 368 ============ loss 0.007454636126526589 0.08100000000000002\n",
      "============== 369 ============ loss 0.0072412296928292125 0.08100000000000002\n",
      "============== 370 ============ loss 0.0070348421376179235 0.08100000000000002\n",
      "============== 371 ============ loss 0.006835242615751525 0.08100000000000002\n",
      "============== 372 ============ loss 0.0066422078743396335 0.08100000000000002\n",
      "============== 373 ============ loss 0.006455522003019587 0.08100000000000002\n",
      "============== 374 ============ loss 0.0062749761925029925 0.08100000000000002\n",
      "============== 375 ============ loss 0.006100368501003616 0.08100000000000002\n",
      "============== 376 ============ loss 0.005931503628380111 0.08100000000000002\n",
      "============== 377 ============ loss 0.005768192697712624 0.08100000000000002\n",
      "============== 378 ============ loss 0.005610253044034346 0.08100000000000002\n",
      "============== 379 ============ loss 0.005457508010040191 0.08100000000000002\n",
      "============== 380 ============ loss 0.005309786748486277 0.08100000000000002\n",
      "============== 381 ============ loss 0.005166924031127212 0.08100000000000002\n",
      "============== 382 ============ loss 0.00502876006389863 0.08100000000000002\n",
      "============== 383 ============ loss 0.004895140308191536 0.08100000000000002\n",
      "============== 384 ============ loss 0.00476591530801624 0.08100000000000002\n",
      "============== 385 ============ loss 0.004640940522834693 0.08100000000000002\n",
      "============== 386 ============ loss 0.0045200761658997935 0.08100000000000002\n",
      "============== 387 ============ loss 0.004403187047909675 0.08100000000000002\n",
      "============== 388 ============ loss 0.004290142425795828 0.08100000000000002\n",
      "============== 389 ============ loss 0.004180815856507827 0.08100000000000002\n",
      "============== 390 ============ loss 0.004075085055573547 0.08100000000000002\n",
      "============== 391 ============ loss 0.003972831760337577 0.08100000000000002\n",
      "============== 392 ============ loss 0.003873941597681735 0.08100000000000002\n",
      "============== 393 ============ loss 0.003778303956121077 0.08100000000000002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== 394 ============ loss 0.003685811862059613 0.08100000000000002\n",
      "============== 395 ============ loss 0.0035963618601692797 0.08100000000000002\n",
      "============== 396 ============ loss 0.003509853897663259 0.08100000000000002\n",
      "============== 397 ============ loss 0.0034261912124012367 0.08100000000000002\n",
      "============== 398 ============ loss 0.0033452802246455946 0.08100000000000002\n",
      "============== 399 ============ loss 0.003267030432427864 0.08100000000000002\n",
      "============== 400 ============ loss 0.0031913543102859085 0.07290000000000002\n",
      "============== 401 ============ loss 0.0031254308476047193 0.07290000000000002\n",
      "============== 402 ============ loss 0.003061460446536621 0.07290000000000002\n",
      "============== 403 ============ loss 0.0029993852469360326 0.07290000000000002\n",
      "============== 404 ============ loss 0.002939149102754309 0.07290000000000002\n",
      "============== 405 ============ loss 0.0028806975312610427 0.07290000000000002\n",
      "============== 406 ============ loss 0.0028239776637859373 0.07290000000000002\n",
      "============== 407 ============ loss 0.0027689381978812885 0.07290000000000002\n",
      "============== 408 ============ loss 0.0027155293509343904 0.07290000000000002\n",
      "============== 409 ============ loss 0.0026637028151443426 0.07290000000000002\n",
      "============== 410 ============ loss 0.002613411713824715 0.07290000000000002\n",
      "============== 411 ============ loss 0.0025646105590098884 0.07290000000000002\n",
      "============== 412 ============ loss 0.0025172552103191856 0.07290000000000002\n",
      "============== 413 ============ loss 0.0024713028350218363 0.07290000000000002\n",
      "============== 414 ============ loss 0.0024267118693170834 0.07290000000000002\n",
      "============== 415 ============ loss 0.002383441980724339 0.07290000000000002\n",
      "============== 416 ============ loss 0.0023414540316177854 0.07290000000000002\n",
      "============== 417 ============ loss 0.002300710043822036 0.07290000000000002\n",
      "============== 418 ============ loss 0.002261173164270997 0.07290000000000002\n",
      "============== 419 ============ loss 0.002222807631671469 0.07290000000000002\n",
      "============== 420 ============ loss 0.0021855787441548867 0.07290000000000002\n",
      "============== 421 ============ loss 0.0021494528279024034 0.07290000000000002\n",
      "============== 422 ============ loss 0.0021143972066830457 0.07290000000000002\n",
      "============== 423 ============ loss 0.0020803801723045297 0.07290000000000002\n",
      "============== 424 ============ loss 0.002047370955927541 0.07290000000000002\n",
      "============== 425 ============ loss 0.0020153397002442832 0.07290000000000002\n",
      "============== 426 ============ loss 0.001984257432477027 0.07290000000000002\n",
      "============== 427 ============ loss 0.001954096038165129 0.07290000000000002\n",
      "============== 428 ============ loss 0.00192482823574035 0.07290000000000002\n",
      "============== 429 ============ loss 0.0018964275518620465 0.07290000000000002\n",
      "============== 430 ============ loss 0.001868868297460967 0.07290000000000002\n",
      "============== 431 ============ loss 0.0018421255445108115 0.07290000000000002\n",
      "============== 432 ============ loss 0.0018161751034808811 0.07290000000000002\n",
      "============== 433 ============ loss 0.0017909935014643146 0.07290000000000002\n",
      "============== 434 ============ loss 0.0017665579609364573 0.07290000000000002\n",
      "============== 435 ============ loss 0.0017428463791674606 0.07290000000000002\n",
      "============== 436 ============ loss 0.0017198373082213496 0.07290000000000002\n",
      "============== 437 ============ loss 0.0016975099355628666 0.07290000000000002\n",
      "============== 438 ============ loss 0.0016758440652353147 0.07290000000000002\n",
      "============== 439 ============ loss 0.0016548200995870322 0.07290000000000002\n",
      "============== 440 ============ loss 0.0016344190215547313 0.07290000000000002\n",
      "============== 441 ============ loss 0.0016146223774606968 0.07290000000000002\n",
      "============== 442 ============ loss 0.001595412260320968 0.07290000000000002\n",
      "============== 443 ============ loss 0.0015767712936523362 0.07290000000000002\n",
      "============== 444 ============ loss 0.0015586826157514373 0.07290000000000002\n",
      "============== 445 ============ loss 0.0015411298644538995 0.07290000000000002\n",
      "============== 446 ============ loss 0.0015240971623244285 0.07290000000000002\n",
      "============== 447 ============ loss 0.0015075691023046863 0.07290000000000002\n",
      "============== 448 ============ loss 0.0014915307337743412 0.07290000000000002\n",
      "============== 449 ============ loss 0.0014759675490303506 0.07290000000000002\n",
      "============== 450 ============ loss 0.0014608654701683322 0.07290000000000002\n",
      "============== 451 ============ loss 0.0014462108363430958 0.07290000000000002\n",
      "============== 452 ============ loss 0.0014319903914208245 0.07290000000000002\n",
      "============== 453 ============ loss 0.0014181912719836752 0.07290000000000002\n",
      "============== 454 ============ loss 0.0014048009956979118 0.07290000000000002\n",
      "============== 455 ============ loss 0.001391807450026462 0.07290000000000002\n",
      "============== 456 ============ loss 0.0013791988812692786 0.07290000000000002\n",
      "============== 457 ============ loss 0.0013669638839364172 0.07290000000000002\n",
      "============== 458 ============ loss 0.0013550913904309026 0.07290000000000002\n",
      "============== 459 ============ loss 0.0013435706610368566 0.07290000000000002\n",
      "============== 460 ============ loss 0.0013323912742112549 0.07290000000000002\n",
      "============== 461 ============ loss 0.0013215431171528405 0.07290000000000002\n",
      "============== 462 ============ loss 0.0013110163766561294 0.07290000000000002\n",
      "============== 463 ============ loss 0.0013008015302401273 0.07290000000000002\n",
      "============== 464 ============ loss 0.001290889337528872 0.07290000000000002\n",
      "============== 465 ============ loss 0.0012812708319031958 0.07290000000000002\n",
      "============== 466 ============ loss 0.0012719373123801592 0.07290000000000002\n",
      "============== 467 ============ loss 0.0012628803357531235 0.07290000000000002\n",
      "============== 468 ============ loss 0.0012540917089462521 0.07290000000000002\n",
      "============== 469 ============ loss 0.0012455634816128374 0.07290000000000002\n",
      "============== 470 ============ loss 0.001237287938939238 0.07290000000000002\n",
      "============== 471 ============ loss 0.0012292575946677143 0.07290000000000002\n",
      "============== 472 ============ loss 0.0012214651843267164 0.07290000000000002\n",
      "============== 473 ============ loss 0.0012139036586582615 0.07290000000000002\n",
      "============== 474 ============ loss 0.0012065661772485841 0.07290000000000002\n",
      "============== 475 ============ loss 0.0011994461023305842 0.07290000000000002\n",
      "============== 476 ============ loss 0.0011925369927893313 0.07290000000000002\n",
      "============== 477 ============ loss 0.001185832598332756 0.07290000000000002\n",
      "============== 478 ============ loss 0.001179326853837669 0.07290000000000002\n",
      "============== 479 ============ loss 0.0011730138738665597 0.07290000000000002\n",
      "============== 480 ============ loss 0.0011668879473426723 0.07290000000000002\n",
      "============== 481 ============ loss 0.001160943532383416 0.07290000000000002\n",
      "============== 482 ============ loss 0.0011551752512918793 0.07290000000000002\n",
      "============== 483 ============ loss 0.0011495778856884151 0.07290000000000002\n",
      "============== 484 ============ loss 0.0011441463717941204 0.07290000000000002\n",
      "============== 485 ============ loss 0.0011388757958480829 0.07290000000000002\n",
      "============== 486 ============ loss 0.001133761389665183 0.07290000000000002\n",
      "============== 487 ============ loss 0.001128798526321779 0.07290000000000002\n",
      "============== 488 ============ loss 0.0011239827159735287 0.07290000000000002\n",
      "============== 489 ============ loss 0.001119309601790182 0.07290000000000002\n",
      "============== 490 ============ loss 0.0011147749560187921 0.07290000000000002\n",
      "============== 491 ============ loss 0.0011103746761564456 0.07290000000000002\n",
      "============== 492 ============ loss 0.0011061047812441693 0.07290000000000002\n",
      "============== 493 ============ loss 0.0011019614082607683 0.07290000000000002\n",
      "============== 494 ============ loss 0.001097940808632711 0.07290000000000002\n",
      "============== 495 ============ loss 0.0010940393448438375 0.07290000000000002\n",
      "============== 496 ============ loss 0.001090253487140233 0.07290000000000002\n",
      "============== 497 ============ loss 0.0010865798103453328 0.07290000000000002\n",
      "============== 498 ============ loss 0.0010830149907558083 0.07290000000000002\n",
      "============== 499 ============ loss 0.0010795558031366002 0.07290000000000002\n",
      "============== 500 ============ loss 0.0010761991178060242 0.07290000000000002\n",
      "============== 501 ============ loss 0.0010729418978030548 0.07290000000000002\n",
      "============== 502 ============ loss 0.0010697811961400848 0.07290000000000002\n",
      "============== 503 ============ loss 0.001066714153139256 0.07290000000000002\n",
      "============== 504 ============ loss 0.0010637379938439856 0.07290000000000002\n",
      "============== 505 ============ loss 0.0010608500255105606 0.07290000000000002\n",
      "============== 506 ============ loss 0.0010580476351706113 0.07290000000000002\n",
      "============== 507 ============ loss 0.0010553282872706976 0.07290000000000002\n",
      "============== 508 ============ loss 0.00105268952137512 0.07290000000000002\n",
      "============== 509 ============ loss 0.0010501289499445137 0.07290000000000002\n",
      "============== 510 ============ loss 0.0010476442561727977 0.07290000000000002\n",
      "============== 511 ============ loss 0.0010452331918935666 0.07290000000000002\n",
      "============== 512 ============ loss 0.00104289357554575 0.07290000000000002\n",
      "============== 513 ============ loss 0.00104062329020172 0.07290000000000002\n",
      "============== 514 ============ loss 0.001038420281650568 0.07290000000000002\n",
      "============== 515 ============ loss 0.0010362825565416389 0.07290000000000002\n",
      "============== 516 ============ loss 0.0010342081805806951 0.07290000000000002\n",
      "============== 517 ============ loss 0.0010321952767801982 0.07290000000000002\n",
      "============== 518 ============ loss 0.0010302420237629196 0.07290000000000002\n",
      "============== 519 ============ loss 0.0010283466541125723 0.07290000000000002\n",
      "============== 520 ============ loss 0.0010265074527757625 0.07290000000000002\n",
      "============== 521 ============ loss 0.0010247227555123892 0.07290000000000002\n",
      "============== 522 ============ loss 0.0010229909473875142 0.07290000000000002\n",
      "============== 523 ============ loss 0.0010213104613126527 0.07290000000000002\n",
      "============== 524 ============ loss 0.0010196797766270605 0.07290000000000002\n",
      "============== 525 ============ loss 0.0010180974177233656 0.07290000000000002\n",
      "============== 526 ============ loss 0.0010165619527125979 0.07290000000000002\n",
      "============== 527 ============ loss 0.0010150719921280446 0.07290000000000002\n",
      "============== 528 ============ loss 0.0010136261876691958 0.07290000000000002\n",
      "============== 529 ============ loss 0.001012223230982304 0.07290000000000002\n",
      "============== 530 ============ loss 0.0010108618524763528 0.07290000000000002\n",
      "============== 531 ============ loss 0.001009540820175465 0.07290000000000002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== 532 ============ loss 0.0010082589386043977 0.07290000000000002\n",
      "============== 533 ============ loss 0.0010070150477054201 0.07290000000000002\n",
      "============== 534 ============ loss 0.0010058080217918597 0.07290000000000002\n",
      "============== 535 ============ loss 0.0010046367685281061 0.07290000000000002\n",
      "============== 536 ============ loss 0.0010035002279409756 0.07290000000000002\n",
      "============== 537 ============ loss 0.001002397371463358 0.07290000000000002\n",
      "============== 538 ============ loss 0.0010013272010006076 0.07290000000000002\n",
      "============== 539 ============ loss 0.0010002887480309516 0.07290000000000002\n",
      "============== 540 ============ loss 0.0009992810727265818 0.07290000000000002\n",
      "============== 541 ============ loss 0.0009983032631053898 0.07290000000000002\n",
      "============== 542 ============ loss 0.0009973544342055227 0.07290000000000002\n",
      "============== 543 ============ loss 0.0009964337272843676 0.07290000000000002\n",
      "============== 544 ============ loss 0.000995540309042378 0.07290000000000002\n",
      "============== 545 ============ loss 0.0009946733708689106 0.07290000000000002\n",
      "============== 546 ============ loss 0.0009938321281114965 0.07290000000000002\n",
      "============== 547 ============ loss 0.0009930158193644655 0.07290000000000002\n",
      "============== 548 ============ loss 0.0009922237057824284 0.07290000000000002\n",
      "============== 549 ============ loss 0.0009914550704099353 0.07290000000000002\n",
      "============== 550 ============ loss 0.0009907092175340227 0.07290000000000002\n",
      "============== 551 ============ loss 0.0009899854720548556 0.07290000000000002\n",
      "============== 552 ============ loss 0.000989283178874007 0.07290000000000002\n",
      "============== 553 ============ loss 0.0009886017023038565 0.07290000000000002\n",
      "============== 554 ============ loss 0.0009879404254898052 0.07290000000000002\n",
      "============== 555 ============ loss 0.0009872987498552182 0.07290000000000002\n",
      "============== 556 ============ loss 0.0009866760945582652 0.07290000000000002\n",
      "============== 557 ============ loss 0.0009860718959657977 0.07290000000000002\n",
      "============== 558 ============ loss 0.0009854856071455191 0.07290000000000002\n",
      "============== 559 ============ loss 0.0009849166973695377 0.07290000000000002\n",
      "============== 560 ============ loss 0.000984364651635453 0.07290000000000002\n",
      "============== 561 ============ loss 0.0009838289701994745 0.07290000000000002\n",
      "============== 562 ============ loss 0.0009833091681247573 0.07290000000000002\n",
      "============== 563 ============ loss 0.0009828047748427137 0.07290000000000002\n",
      "============== 564 ============ loss 0.0009823153337280236 0.07290000000000002\n",
      "============== 565 ============ loss 0.000981840401684058 0.07290000000000002\n",
      "============== 566 ============ loss 0.0009813795487433141 0.07290000000000002\n",
      "============== 567 ============ loss 0.0009809323576777768 0.07290000000000002\n",
      "============== 568 ============ loss 0.0009804984236218848 0.07290000000000002\n",
      "============== 569 ============ loss 0.0009800773537061715 0.07290000000000002\n",
      "============== 570 ============ loss 0.0009796687667014416 0.07290000000000002\n",
      "============== 571 ============ loss 0.0009792722926744335 0.07290000000000002\n",
      "============== 572 ============ loss 0.0009788875726534033 0.07290000000000002\n",
      "============== 573 ============ loss 0.0009785142583020475 0.07290000000000002\n",
      "============== 574 ============ loss 0.0009781520116057274 0.07290000000000002\n",
      "============== 575 ============ loss 0.0009778005045655601 0.07290000000000002\n",
      "============== 576 ============ loss 0.0009774594189009378 0.07290000000000002\n",
      "============== 577 ============ loss 0.0009771284457617342 0.07290000000000002\n",
      "============== 578 ============ loss 0.0009768072854500394 0.07290000000000002\n",
      "============== 579 ============ loss 0.0009764956471473306 0.07290000000000002\n",
      "============== 580 ============ loss 0.0009761932486524586 0.07290000000000002\n",
      "============== 581 ============ loss 0.0009758998161262607 0.07290000000000002\n",
      "============== 582 ============ loss 0.0009756150838432982 0.07290000000000002\n",
      "============== 583 ============ loss 0.000975338793952122 0.07290000000000002\n",
      "============== 584 ============ loss 0.0009750706962414992 0.07290000000000002\n",
      "============== 585 ============ loss 0.0009748105479140251 0.07290000000000002\n",
      "============== 586 ============ loss 0.0009745581133672153 0.07290000000000002\n",
      "============== 587 ============ loss 0.000974313163979192 0.07290000000000002\n",
      "============== 588 ============ loss 0.0009740754779029194 0.07290000000000002\n",
      "============== 589 ============ loss 0.0009738448398644096 0.07290000000000002\n",
      "============== 590 ============ loss 0.0009736210409694106 0.07290000000000002\n",
      "============== 591 ============ loss 0.000973403878512691 0.07290000000000002\n",
      "============== 592 ============ loss 0.0009731931557962837 0.07290000000000002\n",
      "============== 593 ============ loss 0.0009729886819501796 0.07290000000000002\n",
      "============== 594 ============ loss 0.0009727902717605343 0.07290000000000002\n",
      "============== 595 ============ loss 0.0009725977455016167 0.07290000000000002\n",
      "============== 596 ============ loss 0.0009724109287735215 0.07290000000000002\n",
      "============== 597 ============ loss 0.0009722296523441458 0.07290000000000002\n",
      "============== 598 ============ loss 0.0009720537519959958 0.07290000000000002\n",
      "============== 599 ============ loss 0.0009718830683781753 0.07290000000000002\n",
      "============== 600 ============ loss 0.000971717446861346 0.06561000000000002\n",
      "============== 601 ============ loss 0.0009715726993531213 0.06561000000000002\n",
      "============== 602 ============ loss 0.0009714318184860747 0.06561000000000002\n",
      "============== 603 ============ loss 0.0009712947010300429 0.06561000000000002\n",
      "============== 604 ============ loss 0.000971161246509975 0.06561000000000002\n",
      "============== 605 ============ loss 0.0009710313571327023 0.06561000000000002\n",
      "============== 606 ============ loss 0.0009709049377149525 0.06561000000000002\n",
      "============== 607 ============ loss 0.0009707818956137627 0.06561000000000002\n",
      "============== 608 ============ loss 0.0009706621406594921 0.06561000000000002\n",
      "============== 609 ============ loss 0.000970545585088384 0.06561000000000002\n",
      "============== 610 ============ loss 0.0009704321434797939 0.06561000000000002\n",
      "============== 611 ============ loss 0.0009703217326925391 0.06561000000000002\n",
      "============== 612 ============ loss 0.0009702142718047905 0.06561000000000002\n",
      "============== 613 ============ loss 0.0009701096820546257 0.06561000000000002\n",
      "============== 614 ============ loss 0.0009700078867822323 0.06561000000000002\n",
      "============== 615 ============ loss 0.0009699088113738544 0.06561000000000002\n",
      "============== 616 ============ loss 0.0009698123832075242 0.06561000000000002\n",
      "============== 617 ============ loss 0.0009697185315996325 0.06561000000000002\n",
      "============== 618 ============ loss 0.0009696271877529476 0.06561000000000002\n",
      "============== 619 ============ loss 0.0009695382847068806 0.06561000000000002\n",
      "============== 620 ============ loss 0.0009694517572879111 0.06561000000000002\n",
      "============== 621 ============ loss 0.0009693675420622543 0.06561000000000002\n",
      "============== 622 ============ loss 0.0009692855772891493 0.06561000000000002\n",
      "============== 623 ============ loss 0.0009692058028761908 0.06561000000000002\n",
      "============== 624 ============ loss 0.000969128160334432 0.06561000000000002\n",
      "============== 625 ============ loss 0.0009690525927366547 0.06561000000000002\n",
      "============== 626 ============ loss 0.0009689790446749498 0.06561000000000002\n",
      "============== 627 ============ loss 0.0009689074622202893 0.06561000000000002\n",
      "============== 628 ============ loss 0.0009688377928834886 0.06561000000000002\n",
      "============== 629 ============ loss 0.0009687699855760599 0.06561000000000002\n",
      "============== 630 ============ loss 0.000968703990573544 0.06561000000000002\n",
      "============== 631 ============ loss 0.0009686397594787567 0.06561000000000002\n",
      "============== 632 ============ loss 0.0009685772451862817 0.06561000000000002\n",
      "============== 633 ============ loss 0.0009685164018480855 0.06561000000000002\n",
      "============== 634 ============ loss 0.0009684571848401709 0.06561000000000002\n",
      "============== 635 ============ loss 0.0009683995507296304 0.06561000000000002\n",
      "============== 636 ============ loss 0.0009683434572430624 0.06561000000000002\n",
      "============== 637 ============ loss 0.0009682888632353215 0.06561000000000002\n",
      "============== 638 ============ loss 0.0009682357286597594 0.06561000000000002\n",
      "============== 639 ============ loss 0.0009681840145387285 0.06561000000000002\n",
      "============== 640 ============ loss 0.0009681336829351846 0.06561000000000002\n",
      "============== 641 ============ loss 0.0009680846969245318 0.06561000000000002\n",
      "============== 642 ============ loss 0.0009680370205681919 0.06561000000000002\n",
      "============== 643 ============ loss 0.0009679906188868059 0.06561000000000002\n",
      "============== 644 ============ loss 0.000967945457834907 0.06561000000000002\n",
      "============== 645 ============ loss 0.0009679015042758348 0.06561000000000002\n",
      "============== 646 ============ loss 0.000967858725957488 0.06561000000000002\n",
      "============== 647 ============ loss 0.0009678170914887938 0.06561000000000002\n",
      "============== 648 ============ loss 0.0009677765703168535 0.06561000000000002\n",
      "============== 649 ============ loss 0.0009677371327042963 0.06561000000000002\n",
      "============== 650 ============ loss 0.0009676987497076157 0.06561000000000002\n",
      "============== 651 ============ loss 0.0009676613931560403 0.06561000000000002\n",
      "============== 652 ============ loss 0.0009676250356309135 0.06561000000000002\n",
      "============== 653 ============ loss 0.0009675896504455402 0.06561000000000002\n",
      "============== 654 ============ loss 0.0009675552116256631 0.06561000000000002\n",
      "============== 655 ============ loss 0.0009675216938906303 0.06561000000000002\n",
      "============== 656 ============ loss 0.00096748907263443 0.06561000000000002\n",
      "============== 657 ============ loss 0.0009674573239083573 0.06561000000000002\n",
      "============== 658 ============ loss 0.0009674264244028693 0.06561000000000002\n",
      "============== 659 ============ loss 0.0009673963514307422 0.06561000000000002\n",
      "============== 660 ============ loss 0.0009673670829105219 0.06561000000000002\n",
      "============== 661 ============ loss 0.0009673385973504365 0.06561000000000002\n",
      "============== 662 ============ loss 0.0009673108738322381 0.06561000000000002\n",
      "============== 663 ============ loss 0.00096728389199649 0.06561000000000002\n",
      "============== 664 ============ loss 0.0009672576320270981 0.06561000000000002\n",
      "============== 665 ============ loss 0.000967232074637253 0.06561000000000002\n",
      "============== 666 ============ loss 0.0009672072010549833 0.06561000000000002\n",
      "============== 667 ============ loss 0.0009671829930096021 0.06561000000000002\n",
      "============== 668 ============ loss 0.0009671594327181975 0.06561000000000002\n",
      "============== 669 ============ loss 0.000967136502872775 0.06561000000000002\n",
      "============== 670 ============ loss 0.0009671141866273593 0.06561000000000002\n",
      "============== 671 ============ loss 0.0009670924675859362 0.06561000000000002\n",
      "============== 672 ============ loss 0.0009670713297901169 0.06561000000000002\n",
      "============== 673 ============ loss 0.0009670507577077851 0.06561000000000002\n",
      "============== 674 ============ loss 0.0009670307362215316 0.06561000000000002\n",
      "============== 675 ============ loss 0.0009670112506175841 0.06561000000000002\n",
      "============== 676 ============ loss 0.0009669922865750412 0.06561000000000002\n",
      "============== 677 ============ loss 0.0009669738301554779 0.06561000000000002\n",
      "============== 678 ============ loss 0.0009669558677926941 0.06561000000000002\n",
      "============== 679 ============ loss 0.0009669383862826807 0.06561000000000002\n",
      "============== 680 ============ loss 0.000966921372774052 0.06561000000000002\n",
      "============== 681 ============ loss 0.00096690481475861 0.06561000000000002\n",
      "============== 682 ============ loss 0.0009668887000622848 0.06561000000000002\n",
      "============== 683 ============ loss 0.0009668730168359048 0.06561000000000002\n",
      "============== 684 ============ loss 0.0009668577535469389 0.06561000000000002\n",
      "============== 685 ============ loss 0.0009668428989706856 0.06561000000000002\n",
      "============== 686 ============ loss 0.0009668284421822918 0.06561000000000002\n",
      "============== 687 ============ loss 0.0009668143725486124 0.06561000000000002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== 688 ============ loss 0.0009668006797206002 0.06561000000000002\n",
      "============== 689 ============ loss 0.0009667873536254029 0.06561000000000002\n",
      "============== 690 ============ loss 0.0009667743844593516 0.06561000000000002\n",
      "============== 691 ============ loss 0.0009667617626805677 0.06561000000000002\n",
      "============== 692 ============ loss 0.0009667494790019137 0.06561000000000002\n",
      "============== 693 ============ loss 0.0009667375243844198 0.06561000000000002\n",
      "============== 694 ============ loss 0.0009667258900304918 0.06561000000000002\n",
      "============== 695 ============ loss 0.0009667145673774964 0.06561000000000002\n",
      "============== 696 ============ loss 0.0009667035480915269 0.06561000000000002\n",
      "============== 697 ============ loss 0.000966692824061232 0.06561000000000002\n",
      "============== 698 ============ loss 0.0009666823873919451 0.06561000000000002\n",
      "============== 699 ============ loss 0.0009666722303998806 0.06561000000000002\n",
      "============== 700 ============ loss 0.0009666623456064753 0.06561000000000002\n",
      "============== 701 ============ loss 0.0009666527257329363 0.06561000000000002\n",
      "============== 702 ============ loss 0.0009666433636948161 0.06561000000000002\n",
      "============== 703 ============ loss 0.0009666342525970872 0.06561000000000002\n",
      "============== 704 ============ loss 0.0009666253857287255 0.06561000000000002\n",
      "============== 705 ============ loss 0.0009666167565581036 0.06561000000000002\n",
      "============== 706 ============ loss 0.0009666083587279857 0.06561000000000002\n",
      "============== 707 ============ loss 0.0009666001860509987 0.06561000000000002\n",
      "============== 708 ============ loss 0.0009665922325050758 0.06561000000000002\n",
      "============== 709 ============ loss 0.000966584492228991 0.06561000000000002\n",
      "============== 710 ============ loss 0.0009665769595180946 0.06561000000000002\n",
      "============== 711 ============ loss 0.0009665696288202209 0.06561000000000002\n",
      "============== 712 ============ loss 0.0009665624947314373 0.06561000000000002\n",
      "============== 713 ============ loss 0.0009665555519922487 0.06561000000000002\n",
      "============== 714 ============ loss 0.0009665487954836157 0.06561000000000002\n",
      "============== 715 ============ loss 0.0009665422202233282 0.06561000000000002\n",
      "============== 716 ============ loss 0.0009665358213622366 0.06561000000000002\n",
      "============== 717 ============ loss 0.0009665295941808019 0.06561000000000002\n",
      "============== 718 ============ loss 0.0009665235340855619 0.06561000000000002\n",
      "============== 719 ============ loss 0.0009665176366057712 0.06561000000000002\n",
      "============== 720 ============ loss 0.0009665118973901802 0.06561000000000002\n",
      "============== 721 ============ loss 0.0009665063122037608 0.06561000000000002\n",
      "============== 722 ============ loss 0.000966500876924675 0.06561000000000002\n",
      "============== 723 ============ loss 0.0009664955875411923 0.06561000000000002\n",
      "============== 724 ============ loss 0.0009664904401488764 0.06561000000000002\n",
      "============== 725 ============ loss 0.0009664854309475058 0.06561000000000002\n",
      "============== 726 ============ loss 0.000966480556238508 0.06561000000000002\n",
      "============== 727 ============ loss 0.0009664758124220673 0.06561000000000002\n",
      "============== 728 ============ loss 0.0009664711959946349 0.06561000000000002\n",
      "============== 729 ============ loss 0.0009664667035462601 0.06561000000000002\n",
      "============== 730 ============ loss 0.0009664623317581448 0.06561000000000002\n",
      "============== 731 ============ loss 0.0009664580774001581 0.06561000000000002\n",
      "============== 732 ============ loss 0.0009664539373285397 0.06561000000000002\n",
      "============== 733 ============ loss 0.0009664499084834882 0.06561000000000002\n",
      "============== 734 ============ loss 0.0009664459878871037 0.06561000000000002\n",
      "============== 735 ============ loss 0.0009664421726408975 0.06561000000000002\n",
      "============== 736 ============ loss 0.0009664384599240381 0.06561000000000002\n",
      "============== 737 ============ loss 0.000966434846990972 0.06561000000000002\n",
      "============== 738 ============ loss 0.0009664313311695952 0.06561000000000002\n",
      "============== 739 ============ loss 0.0009664279098591948 0.06561000000000002\n",
      "============== 740 ============ loss 0.0009664245805286148 0.06561000000000002\n",
      "============== 741 ============ loss 0.0009664213407143234 0.06561000000000002\n",
      "============== 742 ============ loss 0.0009664181880186934 0.06561000000000002\n",
      "============== 743 ============ loss 0.0009664151201081211 0.06561000000000002\n",
      "============== 744 ============ loss 0.0009664121347114786 0.06561000000000002\n",
      "============== 745 ============ loss 0.000966409229618272 0.06561000000000002\n",
      "============== 746 ============ loss 0.0009664064026771834 0.06561000000000002\n",
      "============== 747 ============ loss 0.0009664036517943562 0.06561000000000002\n",
      "============== 748 ============ loss 0.0009664009749319516 0.06561000000000002\n",
      "============== 749 ============ loss 0.0009663983701066162 0.06561000000000002\n",
      "============== 750 ============ loss 0.0009663958353879781 0.06561000000000002\n",
      "============== 751 ============ loss 0.0009663933688973513 0.06561000000000002\n",
      "============== 752 ============ loss 0.0009663909688062402 0.06561000000000002\n",
      "============== 753 ============ loss 0.0009663886333350882 0.06561000000000002\n",
      "============== 754 ============ loss 0.0009663863607518603 0.06561000000000002\n",
      "============== 755 ============ loss 0.0009663841493709123 0.06561000000000002\n",
      "============== 756 ============ loss 0.0009663819975516511 0.06561000000000002\n",
      "============== 757 ============ loss 0.0009663799036973095 0.06561000000000002\n",
      "============== 758 ============ loss 0.0009663778662539475 0.06561000000000002\n",
      "============== 759 ============ loss 0.0009663758837090402 0.06561000000000002\n",
      "============== 760 ============ loss 0.000966373954590635 0.06561000000000002\n",
      "============== 761 ============ loss 0.0009663720774661009 0.06561000000000002\n",
      "============== 762 ============ loss 0.0009663702509411239 0.06561000000000002\n",
      "============== 763 ============ loss 0.0009663684736586993 0.06561000000000002\n",
      "============== 764 ============ loss 0.0009663667442981127 0.06561000000000002\n",
      "============== 765 ============ loss 0.0009663650615740041 0.06561000000000002\n",
      "============== 766 ============ loss 0.0009663634242353678 0.06561000000000002\n",
      "============== 767 ============ loss 0.0009663618310646919 0.06561000000000002\n",
      "============== 768 ============ loss 0.0009663602808770283 0.06561000000000002\n",
      "============== 769 ============ loss 0.0009663587725191195 0.06561000000000002\n",
      "============== 770 ============ loss 0.0009663573048685979 0.06561000000000002\n",
      "============== 771 ============ loss 0.000966355876833089 0.06561000000000002\n",
      "============== 772 ============ loss 0.0009663544873494822 0.06561000000000002\n",
      "============== 773 ============ loss 0.000966353135383103 0.06561000000000002\n",
      "============== 774 ============ loss 0.0009663518199269396 0.06561000000000002\n",
      "============== 775 ============ loss 0.0009663505400009752 0.06561000000000002\n",
      "============== 776 ============ loss 0.0009663492946513834 0.06561000000000002\n",
      "============== 777 ============ loss 0.0009663480829498831 0.06561000000000002\n",
      "============== 778 ============ loss 0.0009663469039930408 0.06561000000000002\n",
      "============== 779 ============ loss 0.0009663457569015905 0.06561000000000002\n",
      "============== 780 ============ loss 0.0009663446408198202 0.06561000000000002\n",
      "============== 781 ============ loss 0.0009663435549148875 0.06561000000000002\n",
      "============== 782 ============ loss 0.0009663424983763092 0.06561000000000002\n",
      "============== 783 ============ loss 0.0009663414704152282 0.06561000000000002\n",
      "============== 784 ============ loss 0.0009663404702639478 0.06561000000000002\n",
      "============== 785 ============ loss 0.0009663394971753318 0.06561000000000002\n",
      "============== 786 ============ loss 0.0009663385504222111 0.06561000000000002\n",
      "============== 787 ============ loss 0.0009663376292969096 0.06561000000000002\n",
      "============== 788 ============ loss 0.0009663367331107119 0.06561000000000002\n",
      "============== 789 ============ loss 0.0009663358611933291 0.06561000000000002\n",
      "============== 790 ============ loss 0.0009663350128924079 0.06561000000000002\n",
      "============== 791 ============ loss 0.000966334187573098 0.06561000000000002\n",
      "============== 792 ============ loss 0.0009663333846175376 0.06561000000000002\n",
      "============== 793 ============ loss 0.0009663326034244097 0.06561000000000002\n",
      "============== 794 ============ loss 0.0009663318434085076 0.06561000000000002\n",
      "============== 795 ============ loss 0.0009663311040002888 0.06561000000000002\n",
      "============== 796 ============ loss 0.0009663303846454922 0.06561000000000002\n",
      "============== 797 ============ loss 0.000966329684804673 0.06561000000000002\n",
      "============== 798 ============ loss 0.0009663290039528777 0.06561000000000002\n",
      "============== 799 ============ loss 0.0009663283415791722 0.06561000000000002\n",
      "============== 800 ============ loss 0.000966327697186353 0.05904900000000002\n",
      "============== 801 ============ loss 0.0009663271325848871 0.05904900000000002\n",
      "============== 802 ============ loss 0.0009663265817935232 0.05904900000000002\n",
      "============== 803 ============ loss 0.000966326044477444 0.05904900000000002\n",
      "============== 804 ============ loss 0.0009663255203099504 0.05904900000000002\n",
      "============== 805 ============ loss 0.0009663250089722084 0.05904900000000002\n",
      "============== 806 ============ loss 0.0009663245101530978 0.05904900000000002\n",
      "============== 807 ============ loss 0.0009663240235489906 0.05904900000000002\n",
      "============== 808 ============ loss 0.0009663235488635997 0.05904900000000002\n",
      "============== 809 ============ loss 0.000966323085807783 0.05904900000000002\n",
      "============== 810 ============ loss 0.0009663226340993974 0.05904900000000002\n",
      "============== 811 ============ loss 0.0009663221934630901 0.05904900000000002\n",
      "============== 812 ============ loss 0.0009663217636301695 0.05904900000000002\n",
      "============== 813 ============ loss 0.0009663213443384331 0.05904900000000002\n",
      "============== 814 ============ loss 0.000966320935331995 0.05904900000000002\n",
      "============== 815 ============ loss 0.0009663205363611757 0.05904900000000002\n",
      "============== 816 ============ loss 0.0009663201471823077 0.05904900000000002\n",
      "============== 817 ============ loss 0.0009663197675576174 0.05904900000000002\n",
      "============== 818 ============ loss 0.0009663193972550635 0.05904900000000002\n",
      "============== 819 ============ loss 0.0009663190360482233 0.05904900000000002\n",
      "============== 820 ============ loss 0.0009663186837161416 0.05904900000000002\n",
      "============== 821 ============ loss 0.000966318340043193 0.05904900000000002\n",
      "============== 822 ============ loss 0.0009663180048189745 0.05904900000000002\n",
      "============== 823 ============ loss 0.0009663176778381533 0.05904900000000002\n",
      "============== 824 ============ loss 0.0009663173589003646 0.05904900000000002\n",
      "============== 825 ============ loss 0.0009663170478100926 0.05904900000000002\n",
      "============== 826 ============ loss 0.0009663167443765472 0.05904900000000002\n",
      "============== 827 ============ loss 0.0009663164484135418 0.05904900000000002\n",
      "============== 828 ============ loss 0.0009663161597393878 0.05904900000000002\n",
      "============== 829 ============ loss 0.000966315878176793 0.05904900000000002\n",
      "============== 830 ============ loss 0.0009663156035527572 0.05904900000000002\n",
      "============== 831 ============ loss 0.0009663153356984531 0.05904900000000002\n",
      "============== 832 ============ loss 0.0009663150744491458 0.05904900000000002\n",
      "============== 833 ============ loss 0.0009663148196440794 0.05904900000000002\n",
      "============== 834 ============ loss 0.0009663145711263787 0.05904900000000002\n",
      "============== 835 ============ loss 0.0009663143287429733 0.05904900000000002\n",
      "============== 836 ============ loss 0.0009663140923444892 0.05904900000000002\n",
      "============== 837 ============ loss 0.0009663138617851674 0.05904900000000002\n",
      "============== 838 ============ loss 0.0009663136369227713 0.05904900000000002\n",
      "============== 839 ============ loss 0.0009663134176185188 0.05904900000000002\n",
      "============== 840 ============ loss 0.0009663132037369668 0.05904900000000002\n",
      "============== 841 ============ loss 0.0009663129951459628 0.05904900000000002\n",
      "============== 842 ============ loss 0.0009663127917165412 0.05904900000000002\n",
      "============== 843 ============ loss 0.0009663125933228688 0.05904900000000002\n",
      "============== 844 ============ loss 0.0009663123998421552 0.05904900000000002\n",
      "============== 845 ============ loss 0.000966312211154576 0.05904900000000002\n",
      "============== 846 ============ loss 0.0009663120271432178 0.05904900000000002\n",
      "============== 847 ============ loss 0.0009663118476939851 0.05904900000000002\n",
      "============== 848 ============ loss 0.0009663116726955531 0.05904900000000002\n",
      "============== 849 ============ loss 0.0009663115020392965 0.05904900000000002\n",
      "============== 850 ============ loss 0.00096631133561921 0.05904900000000002\n",
      "============== 851 ============ loss 0.0009663111733318628 0.05904900000000002\n",
      "============== 852 ============ loss 0.0009663110150763268 0.05904900000000002\n",
      "============== 853 ============ loss 0.0009663108607541201 0.05904900000000002\n",
      "============== 854 ============ loss 0.0009663107102691408 0.05904900000000002\n",
      "============== 855 ============ loss 0.0009663105635276307 0.05904900000000002\n",
      "============== 856 ============ loss 0.0009663104204380844 0.05904900000000002\n",
      "============== 857 ============ loss 0.0009663102809112279 0.05904900000000002\n",
      "============== 858 ============ loss 0.0009663101448599402 0.05904900000000002\n",
      "============== 859 ============ loss 0.000966310012199222 0.05904900000000002\n",
      "============== 860 ============ loss 0.0009663098828461336 0.05904900000000002\n",
      "============== 861 ============ loss 0.0009663097567197361 0.05904900000000002\n",
      "============== 862 ============ loss 0.0009663096337410627 0.05904900000000002\n",
      "============== 863 ============ loss 0.0009663095138330582 0.05904900000000002\n",
      "============== 864 ============ loss 0.0009663093969205338 0.05904900000000002\n",
      "============== 865 ============ loss 0.0009663092829301299 0.05904900000000002\n",
      "============== 866 ============ loss 0.000966309171790262 0.05904900000000002\n",
      "============== 867 ============ loss 0.0009663090634310884 0.05904900000000002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== 868 ============ loss 0.0009663089577844541 0.05904900000000002\n",
      "============== 869 ============ loss 0.0009663088547838579 0.05904900000000002\n",
      "============== 870 ============ loss 0.0009663087543644181 0.05904900000000002\n",
      "============== 871 ============ loss 0.0009663086564628259 0.05904900000000002\n",
      "============== 872 ============ loss 0.0009663085610173104 0.05904900000000002\n",
      "============== 873 ============ loss 0.0009663084679675972 0.05904900000000002\n",
      "============== 874 ============ loss 0.0009663083772548809 0.05904900000000002\n",
      "============== 875 ============ loss 0.0009663082888217737 0.05904900000000002\n",
      "============== 876 ============ loss 0.0009663082026122944 0.05904900000000002\n",
      "============== 877 ============ loss 0.0009663081185718121 0.05904900000000002\n",
      "============== 878 ============ loss 0.0009663080366470291 0.05904900000000002\n",
      "============== 879 ============ loss 0.0009663079567859404 0.05904900000000002\n",
      "============== 880 ============ loss 0.0009663078789378028 0.05904900000000002\n",
      "============== 881 ============ loss 0.00096630780305311 0.05904900000000002\n",
      "============== 882 ============ loss 0.0009663077290835546 0.05904900000000002\n",
      "============== 883 ============ loss 0.0009663076569820122 0.05904900000000002\n",
      "============== 884 ============ loss 0.0009663075867024875 0.05904900000000002\n",
      "============== 885 ============ loss 0.0009663075182001246 0.05904900000000002\n",
      "============== 886 ============ loss 0.000966307451431145 0.05904900000000002\n",
      "============== 887 ============ loss 0.0009663073863528396 0.05904900000000002\n",
      "============== 888 ============ loss 0.0009663073229235369 0.05904900000000002\n",
      "============== 889 ============ loss 0.0009663072611025782 0.05904900000000002\n",
      "============== 890 ============ loss 0.0009663072008502977 0.05904900000000002\n",
      "============== 891 ============ loss 0.0009663071421279945 0.05904900000000002\n",
      "============== 892 ============ loss 0.0009663070848979064 0.05904900000000002\n",
      "============== 893 ============ loss 0.0009663070291231945 0.05904900000000002\n",
      "============== 894 ============ loss 0.0009663069747679151 0.05904900000000002\n",
      "============== 895 ============ loss 0.0009663069217970017 0.05904900000000002\n",
      "============== 896 ============ loss 0.0009663068701762373 0.05904900000000002\n",
      "============== 897 ============ loss 0.0009663068198722454 0.05904900000000002\n",
      "============== 898 ============ loss 0.0009663067708524596 0.05904900000000002\n",
      "============== 899 ============ loss 0.0009663067230851043 0.05904900000000002\n",
      "============== 900 ============ loss 0.0009663066765391834 0.05904900000000002\n",
      "============== 901 ============ loss 0.0009663066311844546 0.05904900000000002\n",
      "============== 902 ============ loss 0.0009663065869914098 0.05904900000000002\n",
      "============== 903 ============ loss 0.0009663065439312691 0.05904900000000002\n",
      "============== 904 ============ loss 0.0009663065019759452 0.05904900000000002\n",
      "============== 905 ============ loss 0.0009663064610980414 0.05904900000000002\n",
      "============== 906 ============ loss 0.0009663064212708297 0.05904900000000002\n",
      "============== 907 ============ loss 0.0009663063824682316 0.05904900000000002\n",
      "============== 908 ============ loss 0.0009663063446648099 0.05904900000000002\n",
      "============== 909 ============ loss 0.0009663063078357415 0.05904900000000002\n",
      "============== 910 ============ loss 0.0009663062719568183 0.05904900000000002\n",
      "============== 911 ============ loss 0.0009663062370044167 0.05904900000000002\n",
      "============== 912 ============ loss 0.0009663062029554919 0.05904900000000002\n",
      "============== 913 ============ loss 0.0009663061697875632 0.05904900000000002\n",
      "============== 914 ============ loss 0.0009663061374786987 0.05904900000000002\n",
      "============== 915 ============ loss 0.0009663061060075069 0.05904900000000002\n",
      "============== 916 ============ loss 0.000966306075353115 0.05904900000000002\n",
      "============== 917 ============ loss 0.0009663060454951576 0.05904900000000002\n",
      "============== 918 ============ loss 0.0009663060164137744 0.05904900000000002\n",
      "============== 919 ============ loss 0.000966305988089589 0.05904900000000002\n",
      "============== 920 ============ loss 0.0009663059605036939 0.05904900000000002\n",
      "============== 921 ============ loss 0.0009663059336376573 0.05904900000000002\n",
      "============== 922 ============ loss 0.000966305907473485 0.05904900000000002\n",
      "============== 923 ============ loss 0.0009663058819936277 0.05904900000000002\n",
      "============== 924 ============ loss 0.0009663058571809716 0.05904900000000002\n",
      "============== 925 ============ loss 0.0009663058330188167 0.05904900000000002\n",
      "============== 926 ============ loss 0.0009663058094908734 0.05904900000000002\n",
      "============== 927 ============ loss 0.0009663057865812531 0.05904900000000002\n",
      "============== 928 ============ loss 0.0009663057642744507 0.05904900000000002\n",
      "============== 929 ============ loss 0.0009663057425553531 0.05904900000000002\n",
      "============== 930 ============ loss 0.0009663057214092074 0.05904900000000002\n",
      "============== 931 ============ loss 0.0009663057008216299 0.05904900000000002\n",
      "============== 932 ============ loss 0.0009663056807785828 0.05904900000000002\n",
      "============== 933 ============ loss 0.0009663056612663792 0.05904900000000002\n",
      "============== 934 ============ loss 0.0009663056422716667 0.05904900000000002\n",
      "============== 935 ============ loss 0.0009663056237814215 0.05904900000000002\n",
      "============== 936 ============ loss 0.0009663056057829357 0.05904900000000002\n",
      "============== 937 ============ loss 0.0009663055882638192 0.05904900000000002\n",
      "============== 938 ============ loss 0.0009663055712119847 0.05904900000000002\n",
      "============== 939 ============ loss 0.0009663055546156406 0.05904900000000002\n",
      "============== 940 ============ loss 0.0009663055384632873 0.05904900000000002\n",
      "============== 941 ============ loss 0.000966305522743708 0.05904900000000002\n",
      "============== 942 ============ loss 0.0009663055074459628 0.05904900000000002\n",
      "============== 943 ============ loss 0.0009663054925593794 0.05904900000000002\n",
      "============== 944 ============ loss 0.0009663054780735482 0.05904900000000002\n",
      "============== 945 ============ loss 0.0009663054639783183 0.05904900000000002\n",
      "============== 946 ============ loss 0.0009663054502637875 0.05904900000000002\n",
      "============== 947 ============ loss 0.0009663054369202994 0.05904900000000002\n",
      "============== 948 ============ loss 0.000966305423938433 0.05904900000000002\n",
      "============== 949 ============ loss 0.0009663054113090016 0.05904900000000002\n",
      "============== 950 ============ loss 0.000966305399023046 0.05904900000000002\n",
      "============== 951 ============ loss 0.0009663053870718266 0.05904900000000002\n",
      "============== 952 ============ loss 0.0009663053754468204 0.05904900000000002\n",
      "============== 953 ============ loss 0.0009663053641397153 0.05904900000000002\n",
      "============== 954 ============ loss 0.0009663053531424038 0.05904900000000002\n",
      "============== 955 ============ loss 0.0009663053424469798 0.05904900000000002\n",
      "============== 956 ============ loss 0.000966305332045731 0.05904900000000002\n",
      "============== 957 ============ loss 0.0009663053219311407 0.05904900000000002\n",
      "============== 958 ============ loss 0.0009663053120958725 0.05904900000000002\n",
      "============== 959 ============ loss 0.0009663053025327725 0.05904900000000002\n",
      "============== 960 ============ loss 0.0009663052932348679 0.05904900000000002\n",
      "============== 961 ============ loss 0.0009663052841953565 0.05904900000000002\n",
      "============== 962 ============ loss 0.0009663052754076046 0.05904900000000002\n",
      "============== 963 ============ loss 0.0009663052668651413 0.05904900000000002\n",
      "============== 964 ============ loss 0.0009663052585616605 0.05904900000000002\n",
      "============== 965 ============ loss 0.0009663052504910095 0.05904900000000002\n",
      "============== 966 ============ loss 0.000966305242647186 0.05904900000000002\n",
      "============== 967 ============ loss 0.0009663052350243461 0.05904900000000002\n",
      "============== 968 ============ loss 0.0009663052276167787 0.05904900000000002\n",
      "============== 969 ============ loss 0.0009663052204189249 0.05904900000000002\n",
      "============== 970 ============ loss 0.0009663052134253592 0.05904900000000002\n",
      "============== 971 ============ loss 0.0009663052066307926 0.05904900000000002\n",
      "============== 972 ============ loss 0.0009663052000300664 0.05904900000000002\n",
      "============== 973 ============ loss 0.0009663051936181518 0.05904900000000002\n",
      "============== 974 ============ loss 0.0009663051873901444 0.05904900000000002\n",
      "============== 975 ============ loss 0.0009663051813412638 0.05904900000000002\n",
      "============== 976 ============ loss 0.0009663051754668455 0.05904900000000002\n",
      "============== 977 ============ loss 0.0009663051697623449 0.05904900000000002\n",
      "============== 978 ============ loss 0.0009663051642233295 0.05904900000000002\n",
      "============== 979 ============ loss 0.0009663051588454782 0.05904900000000002\n",
      "============== 980 ============ loss 0.0009663051536245747 0.05904900000000002\n",
      "============== 981 ============ loss 0.0009663051485565129 0.05904900000000002\n",
      "============== 982 ============ loss 0.0009663051436372869 0.05904900000000002\n",
      "============== 983 ============ loss 0.0009663051388629917 0.05904900000000002\n",
      "============== 984 ============ loss 0.0009663051342298192 0.05904900000000002\n",
      "============== 985 ============ loss 0.0009663051297340568 0.05904900000000002\n",
      "============== 986 ============ loss 0.000966305125372086 0.05904900000000002\n",
      "============== 987 ============ loss 0.0009663051211403791 0.05904900000000002\n",
      "============== 988 ============ loss 0.0009663051170354963 0.05904900000000002\n",
      "============== 989 ============ loss 0.0009663051130540847 0.05904900000000002\n",
      "============== 990 ============ loss 0.0009663051091928752 0.05904900000000002\n",
      "============== 991 ============ loss 0.0009663051054486805 0.05904900000000002\n",
      "============== 992 ============ loss 0.0009663051018183937 0.05904900000000002\n",
      "============== 993 ============ loss 0.0009663050982989871 0.05904900000000002\n",
      "============== 994 ============ loss 0.0009663050948875085 0.05904900000000002\n",
      "============== 995 ============ loss 0.0009663050915810819 0.05904900000000002\n",
      "============== 996 ============ loss 0.0009663050883768993 0.05904900000000002\n",
      "============== 997 ============ loss 0.0009663050852722285 0.05904900000000002\n",
      "============== 998 ============ loss 0.0009663050822644032 0.05904900000000002\n",
      "============== 999 ============ loss 0.000966305079350827 0.05904900000000002\n",
      "iter Done! Final loss: 0.000966305079350827\n"
     ]
    }
   ],
   "source": [
    "dv_hist = reg.fit(data_x, data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.92272362e-01,  8.22117652e-02,  4.34671442e+02],\n",
       "       [-1.33501977e-01,  9.58413827e-01,  6.55962640e+01],\n",
       "       [-2.09248580e-04,  1.60937718e-05,  1.00000000e+00]])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.90900075e-01,  8.06359397e-02,  4.34671372e+02],\n",
       "       [-1.33281047e-01,  9.58667338e-01,  6.55963190e+01],\n",
       "       [ 6.01986564e-01,  6.91658939e-01,  1.00000000e+00]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.var.val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[366 632   1] [   205.59822578    635.80706082 200547.58662   ] [   205.83779247    636.08217029 200547.59589302]\n",
      "[691 937   1] [   422.36864044    954.84210175 361822.66579   ] [   422.22959761    954.68238868 361822.66920104]\n",
      "[386  74   1] [2.95937776e+02 1.02656381e+02 1.72638300e+05] [2.96026618e+02 1.02758515e+02 1.72638277e+05]\n",
      "[570 609   1] [   370.2923331    630.5347429 287711.846716 ] [   370.24687188    630.48255322 287711.84051069]\n"
     ]
    }
   ],
   "source": [
    "for idx in range(4):\n",
    "    test_x = data_x[idx]\n",
    "    test_y = data_y[idx]\n",
    "    pred_y = np.dot(data_x[idx], reg.var.val)\n",
    "    print (data_x[idx], test_y, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[366, 632,   1],\n",
       "       [691, 937,   1],\n",
       "       [386,  74,   1],\n",
       "       [570, 609,   1]])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression(object):\n",
    "    \"\"\"\n",
    "    Note. Training gradient may explode if lr is too large.\n",
    "    \"\"\"\n",
    "    __slots__ = ('var', 'iter', 'lr', 'err_th', 'j_hist',\n",
    "                'x_mean', 'y_mean', 'x_std', 'y_std',\n",
    "                'decay_rate', 'loss_weights','norm')\n",
    "    def __init__(self, var=None, iter=500, decay_rate=0.9, norm=1):\n",
    "        self.var = var\n",
    "        self.iter = iter\n",
    "        self.lr = 0.001\n",
    "        self.err_th = 1e-4\n",
    "        self.j_hist = np.zeros((self.iter,1))\n",
    "        self.decay_rate=0.9\n",
    "        self.loss_weights=None\n",
    "        self.norm = norm\n",
    "\n",
    "    def process(self, x, y):\n",
    "        #self.x_mean = np.mean(x)\n",
    "        self.x_std = np.std(x)\n",
    "        #self.y_mean = np.mean(y)\n",
    "        #self.y_std = np.std(y)\n",
    "        #_x = (x.copy() - self.x_mean)/self.x_std\n",
    "        #_y = (y.copy() - self.y_mean)/self.y_std\n",
    "        _x = x.copy()/self.x_std\n",
    "        _y = y.copy()/self.x_std\n",
    "        return _x, _y\n",
    "        #return x.copy(), y.copy()\n",
    "    \n",
    "    def pred(x,val):\n",
    "        return np.dot(x, val)\n",
    "    \n",
    "    def compute_cost(x, val, y):\n",
    "        m, n = y.shape\n",
    "        pred = Regression.pred(x, val)\n",
    "        dloss = pred - y\n",
    "        cost = np.sum(dloss * dloss) / (2 * m * n)\n",
    "    \n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        assert len(x.shape) == 2, 'shape of x is not support'\n",
    "        assert self.var.dim[0] == x.shape[1], 'shape of x %s is not consistent to var dim %s' % (self.var.dim, x.shape)\n",
    "        assert self.var.dim[1] == y.shape[1], 'shape of y %s is not consistent to var dim %s' % (self.var.dim, y.shape)\n",
    "        m, n = x.shape\n",
    "        print('processing data')\n",
    "        _x, _y = self.process(x, y)\n",
    "        dv_hist = []\n",
    "        lr= self.lr\n",
    "        method = 'L2norm'\n",
    "        for i in range(self.iter):\n",
    "            if i % 200 == 0:\n",
    "                lr *= self.decay_rate\n",
    "            pred = np.dot(_x, self.var.val)\n",
    "            # Loss L2norm\n",
    "            if self.loss_weights is not None:\n",
    "                res = (pred - _y) * self.loss_weights\n",
    "            else:\n",
    "                res = (pred - _y)\n",
    "            if method == 'L2norm':\n",
    "                loss = np.sum(res * res) / (2 * m * n)\n",
    "            if self.norm > 0:\n",
    "                loss += (np.sum(self.var.val ** 2) - 1) / 2 * self.norm\n",
    "            self.j_hist[i,0] = loss\n",
    "            if (loss < self.err_th):\n",
    "                return self.var.val\n",
    "            # Gradient\n",
    "            if method == 'L2norm':\n",
    "                tV = np.dot(_x.T, res) / m\n",
    "                if self.norm > 0:\n",
    "                    tV += self.var.val * self.norm \n",
    "                dV = lr * tV\n",
    "                dv_hist.append(dV[0,0])\n",
    "            DEBUG('==============',i,'============', 'loss',loss, lr)\n",
    "            # Update variable\n",
    "            if self.var.trainable:\n",
    "                self.var.val -= (dV * self.var.train_mask)\n",
    "        print(\"iter Done! Final loss:\", loss)\n",
    "        return dv_hist\n",
    "            #self.var.update(dV, self.ir, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 990,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.array([[188.51387797, 188.25674871, 190.24676645, 185.97885747],\n",
    " [194.12135314, 191.07547677, 189.01739573, 194.65284295],\n",
    " [ -1.82874661,   0.91050942,  -1.23402567,   2.45709586]]) \n",
    "xT = np.array([[-2.50696386, -5.9552604,   1.        ],\n",
    " [ 0.96999609,  2.84768042,  1.        ],\n",
    " [-3.60295508, -0.58000537,  1.        ],\n",
    " [ 5.13992285,  3.68758535,  1.        ]] )\n",
    "tv = np.array([[-1.95227435e+01, -1.10854111e+01,  7.52996251e+02],\n",
    " [ 1.81666577e+01, -3.75344571e+00,  7.68867069e+02],\n",
    " [ 2.25432145e+01,  2.32599944e+01,  3.04832992e-01]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 994,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-19.52274352208258"
      ]
     },
     "execution_count": 994,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(188.51387797 * -2.50696386)  + (188.25674871 * 0.96999609) +  (190.24676645*-3.60295508) + (185.97885747 * 5.13992285)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 993,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-19.52274352208258"
      ]
     },
     "execution_count": 993,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(res[0,:],xT[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 991,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.95227435e+01, -1.10854110e+01,  7.52996251e+02],\n",
       "       [ 1.81666577e+01, -3.75344576e+00,  7.68867069e+02],\n",
       "       [ 2.25432144e+01,  2.32599943e+01,  3.04833000e-01]])"
      ]
     },
     "execution_count": 991,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res @ xT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 978,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.95227435e+01, -1.10854111e+01,  7.52996251e+02],\n",
       "       [ 1.81666577e+01, -3.75344571e+00,  7.68867069e+02],\n",
       "       [ 2.25432145e+01,  2.32599944e+01,  3.04832992e-01]])"
      ]
     },
     "execution_count": 978,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv [[-2.04379374e-02  1.77674701e-02  1.24713524e+02]\n",
    " [ 2.77237708e-02 -2.41013200e-02  1.27342097e+02]\n",
    " [ 4.63450171e-03 -4.02894719e-03  3.04832992e-01]]\n",
    "dv [-1.02189687e-04  8.88373504e-05  6.23567619e-01] val [ 0.79079653  0.08349476 34.54371102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 998,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6235676200000001"
      ]
     },
     "execution_count": 998,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.24713524e+02 * 0.02 / 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
